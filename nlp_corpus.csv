Author,Dataset,Description,Instances,Language,Task,Paper,Download
Ramos et al.,B5 Corpus,"Dataset is a collection of Facebook posts, including information about brazilian authors, like gender, age, personality score (Based in B5 test), education level, politic position, religious, and others.",1012,Portuguese,Text Classification,Paper,Link
Weller et al.,Zero Shot Learning from Task Descriptions (ZEST),"Dataset used for zero-shot prediction that is formatted similarly to reading comprehension datasets, where the authors formulate task descriptions as questions and pair them with paragraphs of text.","25,026",English,Zero Shot Prediction,Paper,Link
Farahani,WikiSummary,A summarization dataset extracted from Wikipedia.,"56,363",Persian,Summarization,Paper,Link
Upadhayay et al.,Sentimental LIAR,Sentimental LIAR dataset is a modified and further extended version of the original LIAR dataset. It was modified to be a binary-label dataset that was then extended by adding sentiments derived using the Google NLP API.,n/a,English,"Classification, Fake News Detection",Paper,Link
Malo et al.,FinancialPhraseBank,Dataset contains the sentiments for financial news headlines from the perspective of a retail investor.,"4,837",English,Sentiment Analysis,Paper,Link
Tandon et al.,WebChild,"Dataset contains triples that connect nouns with adjectives via fine-grained relations like hasShape, hasTaste, evokesEmotion, etc. The arguments of these assertions, nouns and adjectives, are disambiguated by mapping them onto their proper WordNet senses.",4M triples,English,"Commonsense, Knowledge Base",Paper,Link
Ilmania et al.,CASA (IndoNLU),"An aspect-based sentiment analysis dataset consisting of around a thousand car reviews collected from multiple Indonesian online automobile platforms. Task is defined as a multi-label classification task, where each label represents a sentiment for a single aspect with three possible values: positive, negative, and neutral.","1,08",Indonesian,"Classification, Sentiment Analysis",Paper,Link
Azhar et al.,HoASA (IndoNLU),"An aspect-based sentiment analysis dataset consisting of hotel reviews collected from the hotel aggregator platform, AiryRooms. The dataset covers ten different aspects of hotel quality. There are four possible sentiment classes for each sentiment label: positive, negative, neutral, and positive-negative.","2,854",Indonesian,"Classification, Sentiment Analysis",Paper,Link
Setya and Mahendra et al.,The Wiki Revision Edits Textual Entailment (WReTE) (IndoNLU),"Dataset consists of 450 sentence pairs constructed from Wikipedia revision history. It contains pairs of sentences and binary semantic relations between the pairs. The data are labeled as entailed when the meaning of the second sentence can be derived from the first one, and not entailed otherwise.",450,Indonesian,Natural Language Inference (NLI),Paper,Link
Hoesen and Purwarianti et al.,POSP (IndoNLU),"Dataset is collected from Indonesian news websites. The dataset consists of around 8,000 sentences with 26 POS tags.","8,4",Indonesian,Part-of-Speech (POS),Paper,Link
"Jiang, Bordia et al.",HOVER,"Dataset is an open-domain, many-hop fact extraction and claim verification dataset built upon the Wikipedia corpus. The original 2-hop claims are adapted from question-answer pairs from HotpotQA.","26,171",English,Information Extraction,Paper,Link
Yao et al.,Stack Overflow Question-Code Pairs (StaQC),"Dataset contains 148K Python and 120K SQL domain question-code pairs, which were mined from Stack Overflow.","267,065",English,Language-to-Code,Paper,Link
Saputri et al.,EmoT (IndoNLU),"Dataset used for emotion classification of tweets with 5 categories: anger, fear, happiness, love and sadness.","4,403",Indonesian,"Classification, Sentiment Analysis",Paper,Link
Purwarianti and Crisdayanti et al.,SmSA (IndoNLU),"Dataset is a collection of comments and reviews in Indonesian obtained from multiple online platforms. The text was crawled and then annotated by several Indonesian linguists to construct this dataset. There are three possible sentiments: positive, negative, and neutral.","12,76",Indonesian,"Classification, Sentiment Analysis",Paper,Link
Dinakaramani et al.,BaPOS (IndoNLU),"Dataset contains about 1,000 sentences, collected from the PAN Localization Project. In this dataset, each word is tagged by one of 23 POS tag classes.","10,029",Indonesian,Part-of-Speech (POS),Paper,Link
Septiandri and Sutiono & Fernando et al.,TermA (IndoNLU),"Dataset consists of thousands of hotel reviews, which each contain a span label for aspect and sentiment words representing the opinion of the reviewer on the corresponding aspect. The labels use Inside-Outside-Beginning (IOB) tagging representation with two kinds of tags, aspect and sentiment.",5,Indonesian,Sentiment Extraction,Paper,Link
Mahfuzh et al.,KEPS (IndoNLU),"Dataset consists of text from Twitter discussing banking products and services. A phrase containing important information is considered a keyphrase. Text may contain one or more keyphrases since important phrases can be located at different positions. The dataset follows the IOB chunking format, which represents the position of the keyphrase.","1,247",Indonesian,Keyphrase Extraction,Paper,Link
Grit ID,NERGrit (IndoNLU),"Dataset consists of three kinds of named entity tags, PERSON (name of person), PLACE (name of location), and ORGANIZATION (name of organization).","2,09",Indonesian,Named Entity Recognition (NER),Paper,Link
Hoesen and Purwarianti et al.,NERP (IndoNLU),"Dataset contains contains texts collected from several Indonesian news websites. There are five labels available in this dataset, PER (name of person), LOC (name of location), IND (name of product or brand), EVT (name of the event), and FNB (name of food and beverage).","8,4",Indonesian,Named Entity Recognition (NER),Paper,Link
Purwarianti et al.,FacQA (IndoNLU),"Dataset is to find the answer to a question from a provided short passage from a news article (Purwarianti et al., 2007). Each row in the FacQA dataset consists of a question, a short passage, and a label phrase, which can be found inside the corresponding short passage. There are six categories of questions: date, location, name, organization, person, and quantitative.","3,117",Indonesian,Span Extraction,Paper,Link
Ferguson et al.,IIRC,"Dataset contains more than 13K questions over paragraphs from English Wikipedia that provide only partial information to answer them, with the missing information occurring in one or more linked documents.","5,698",English,Reading Comprehension,Paper,Link
Nothman et al.,WikiNER,"Dataset contains 7,200 manually-labelled Wikipedia articles across nine languages: English, German, French, Polish, Italian, Spanish,Dutch, Portuguese and Russian.","7,2",Multi-Lingual,Named Entity Recognition (NER),Paper,Link
Bastianelli et al.,Spoken Language Understanding Resource Package (SLURP),"Dataset is a collection of ~72K audio recordings of single turn user interactions with a home assistant, annotated with three levels of semantics: Scenario, Action and Entities, including over 18 different scenarios, with 46 defined actions and 55 different entity types.","~72,000",English,Speech Language Understanding,Paper,Link
Roy et al.,XQuaD-R,"Dataset is the retrieval version of the normal XQuAD dataset. Like XQuAD, XQUAD-R is an 11-way parallel dataset, where each question appears in 11 different languages and has 11 parallel correct answers across the languages.",n/a,Multi-Lingual,Question Answering,Paper,Link
Raganato et al.,XL-WiC,"Dataset extends the WiC dataset containing 80K instances to 12 new languages: Bulgarian, Chinese, Croatian, Danish, Dutch, Estonian, Farsi, French, German, Italian, Japanese and Korean.",n/a,Multi-Lingual,Word Sense Disambiguation,Paper,Link
Zeng et al.,MedDialog,Dataset contains conversations (in Chinese) between doctors and patients. It has 1.1 million dialogues and 4 million utterances.,1.1M,Chinese,Dialogue,Paper,Link
Bjerva et al.,SubjQA,"Dataset is a question answering dataset that focuses on subjective (as opposed to factual) questions and answers. The dataset consists of roughly 10,000 questions over reviews from 6 different domains: books, movies, grocery, electronics, TripAdvisor (i.e. hotels), and restaurants. Each question is paired with a review and a span is highlighted as the answer to the question (with some questions having no answer).","10,098",English,Question Answering,Paper,Link
Wang et al.,MAVEN,"Dataset contains 4,480 Wikipedia documents, 118,732 event mention instances, and 168 event types.","4,48",English,Event Detection,Paper,Link
Gugliotta et al.,Tunisian Arabish Corpus (TArC),"Dataset has been extracted from social media for an amount of 43,313 tokens. The classification task consists in categorizing the text at the token level into three classes: arabizi, foreign and emotag.","4,79",Tunisian,"Classification, Part-of-Speech (POS)",Paper,Link
Mudalige et al.,SigmaLaw-ABSA,"Dataset contains legal data consisting of 39,155 legal cases including 22,776 taken from the United States Supreme Court. For the data collection process, about 2,000 sentences were gathered to annotate and court cases were selected without targeting any specific category. Party based sentiment polarity values are annotated: negative, positive, & neutral.","39,155",English,"Text Corpora, Sentiment Analysis",Paper,Link
Bastan et al.,PerSenT,Dataset that captures the sentiment of an author towards the main entity in a news article. This dataset contains annotation for 5.3K documents and 38K paragraphs covering 3.2K unique entities.,"5,339",English,Sentiment Analysis,Paper,Link
Hardalov et al.,EXAMS,"A benchmark dataset for cross-lingual and multi-lingual question answering for high school examinations. We collected more than 24,000 high quality high school exam questions in 16 languages, covering 8 language families and 24 school subjects from Natural Sciences and Social Sciences. Langs: Albanian, Arabic, Bulgarian, Croatian, French, German, Hungarian, Italian, Lithuanian, Macedonian, Polish, Portuguese, Serbian, Spanish, Turkish, and Vietnamese.","24,143",Multi-Lingual,Question Answering,Paper,Link
Kurniawan et al.,IndoSum,Dataset for text summarization in Indonesian that is compiled from online news articles and publicly available.,19,Indonesian,Summarization,Paper,Link
Spiegel et al.,MK-SQuIT,"Dataset contains 110,000 English question and SPARQL query pairs across four WikiData domains.",110,English,"Semantic Parsing, Text-To-SPARQL",Paper,Link
Ohman et al.,XED,"Dataset consists of emotion annotated movie subtitles from OPUS. Plutchik's 8 core emotions to annotate were used. The data is multilabel. The original annotations have been sourced for mainly English and Finnish, with the rest created using annotation projection to aligned subtitles in 41 additional languages, with 31 languages included in the final dataset (more than 950 lines of annotated subtitle lines).",n/a,Multi-Lingual,Sentiment Analysis,Paper,Link
Fujii et al.,PheMT,"Dataset is based on the MTNT dataset, with additional annotations of four linguistic phenomena: Proper Noun, Abbreviated Noun, Colloquial Expression, and Variant.","1,566","Japanese, English",Machine Translation,Paper,Link
Schulz et al.,EHR-Rel,"A benchmark dataset for biomedical concept relatedness, consisting of 3,630 concept pairs sampled from electronic health records (EHRs).","3,63",English,Relatedness Textual Similarity,Paper,Link
Ho et al.,2WikiMultihopQA,"A multihop QA dataset, which uses structured and unstructured data. It includes the evidence information containing a reasoning path for multi-hop questions.","192,606",English,Question Answering,Paper,Link
Akallouch et al.,ASAYAR,"Dataset is used for extraction of text information from traffic panels. It consists of 3 sub-datasets: Arabic-Latin scene text localization, traffic sign detection, and directional symbol detection. The dataset contains 1,763 images collected on different Moroccan highways, and annotated manually, using 16 object categories. The fully annotated ASAYAR images contains more than 20,000 bounding box objects. [requires form completion]","1,763","Arabic, French",Scene Text Recognition,Paper,Link
Sun et al.,CLIRMatrix,"Dataset is a collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval extracted automatically from Wikipedia. It comprises of (1) BI-139, a bilingual dataset of queries in one language matched with relevant documents in another language for 19,182 language pairs, and (2) MULTI-8, a multilingual dataset of queries and documents jointly aligned in 8 different languages. In total, 49 million unique queries and 34 billion (query, document, label) triplets were mined.",n/a,Multi-Lingual,Information Retrieval,Paper,Link
Wang et al.,CoNLL 2003 ++,"Similar to the original CoNLL except test set has been corrected for label mistakes. The dataset is split into training, development, and test sets, with 14,041, 3,250, and 3,453 instances respectively.","20,744",English,Named Entity Recognition (NER),Paper,Link
Qu et al.,Open-Retrieval Conversational Question Answering (ORConvQA),"Dataset enhances QuAC by adapting it to an open retrieval setting. It is an aggregation of 3 existing datasets: (1) the QuAC dataset that offers information-seeking conversations, (2) the CANARD dataset that consists of context-independent rewrites of QuAC questions, and (3) the Wikipedia corpus that serves as the knowledge source of answering questions.","5,644",English,Question Answering,Paper,Link
Tawalbeh et al.,Arabic Dataset for Commonsense Validation,"Dataset was translated from the original English dataset for commonsense validation (Wang et al., 2019). Each example in the provided dataset is composed of 2 sentences: {s1, s2} and a label indicating which one is invalid.",12,Arabic,Commonsense Validation,Paper,Link
Wang et al.,KB-Ref,"Dataset is a referring expression comprehension dataset containing 43K expressions on 16K images. Different with other referring expression dataset, it requires that each referring expression must use at least one external knowledge (the information can not be got from the image).",16,English,"Referring Expression Comprehension, Multi-Modal",Paper,Link
Neea Rusch,FI News Corpus,"Dataset is a collection of news headlines and short summaries of text, organized by date. The news articles were published between 2012-2020.","93,957",Finnish,Text Corpora,Paper,Link
Dong & Shafer,ACL Citation Coreference Corpus,Dataset was constructed from papers from proceedings of the ACL conference in 2007 and 2008. Text was annotated for the coreference resolution task.,"1,768",English,Coreference Resolution,Paper,Link
Basaldella et al.,COMETA,"Dataset is an entity linking dataset of layman medical terminology. It consists of 20K English biomedical entity mentions from Reddit expert-annotated with links to SNOMED CT, a widely-used medical knowledge graph.",20,English,Entity Linking,Paper,Link
Botha et al.,Mewsli-9,"Dataset consists of entity mentions linked to WikiData, extracted from WikiNews articles. It covers 9 diverse languages, 5 language families and 6 writing systems. It features many WikiData entities that do not appear in English Wikipedia, thereby incentivizing research into multilingual entity linking against WikiData at-large. Langs: Japanese, German, Spanish, Arabic, Serbian, Turkish, Persian, Tamil & English.","289,087",Multi-Lingual,Entity Linking,Paper,Link
Strauss et al.,WNUT 2016,"Dataset is annotated with 10 fine-grained NER categories: person, geo-location, company, facility, product,music artist, movie, sports team, tv show and other. Dataset was extracted from tweets and is structured in CoNLL format.","5,63",English,Named Entity Recognition (NER),Paper,Link
Gu et al.,GrailQA,"Dataset contains 64,331 crowdsourced questions involving up to 4 relations and functions like counting, comparatives, and superlatives. The dataset covers all the 86 domains in Freebase Commons.","64,331",English,"Question Answering, Knowledge Base",Paper,Link
Cheng et al.,ENT-DESC,"Dataset was extracted from Wikipedia and Wikidata, which contains over 110k instances. Each sample is a triplet, containing a set of entities, the explored knowledge from a KG, and the description.",110,English,Data-To-Text Generation,Paper,Link
Sap et al.,Social Bias Inference Corpus (SBIC),"Dataset contains 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups.","44,671",English,"Classification, Text Generation",Paper,Link
Wambsganss et al.,Argumentation Annotated Student Peer Reviews Corpus (AASPRC),"Dataset contains 1,000 persuasive student peer reviews about business model feedbacks annotated for their argumentative components and argumentative relations.",1,German,"Argument Component Classification, Argument Relation Classification",Paper,Link
You et al.,Social Narrative Tree,"Dataset contains 1,250 stories documenting a variety of daily social interactions.","1,25",English,Commonsense,Paper,Link
Lu et al.,Multi-Xscience,A multi-document summarization dataset created from scientific articles. MultiXScience introduces a challenging multidocument summarization task: writing the related-work section of a paper based on its abstract and the articles it references.,"40,528",English,Summarization,Paper,Link
Agarwal et al.,Corpus for Knowledge-Enhanced Language Model Pre-training (KELM),"Dataset consists of ∼18M sentences spanning ∼45M triples with ∼1,500 distinct relations from English Wikidata.",18M,English,Data-To-Text Generation,Paper,Link
Zhang et al.,TriageSQL,Dataset is a cross-domain text-to-SQL question intention classification benchmark. It contains 34K databases and 390K questions from 20 existing datasets.,390,English,Text-to-SQL,Paper,Link
Barbieri et al.,TweetEval,"TweetEval consists of seven tasks in Twitter, all framed as multi-class tweet classification. Emotion Recognition, Emoji Prediction, Irony Detection, Hate Speech Detection, Offensive Language Identification, Sentiment Analysis, & Stance Detection.",n/a,English,Classification,Paper,Link
Cheng et al.,Tree-Based Dialog State Tracking (TreeDST),"Dataset is a multi-turn, multi-domain task-oriented dialog dataset annotated with tree-based user dialog states and system dialog acts. The goal is to provide a novel solution for end-to-end dialog state tracking as a conversational semantic parsing task. In total, it contains 27,280 conversations covering 10 domains with shared types of person, time and location.","27,28",English,Dialogue,Paper,Link
Niyongabo et al.,KINNEWS and KIRNEWS,"There are 2 news classification datasets (KINNEWS and KIRNEWS), which were both collected from Rwanda and Burundi news websites and newspapers. In total, there are 21,268 and 4,612 news articles which are distributed across 14 and 12 categories for KINNEWS and KIRNEWS respectively.","25,88",Kinyarwanda & Kirundi,Classification,Paper,Link
Veyseh et al.,Acronym Detection Dataset,"Dataset contains 62,441 samples where each sample involves a sentence, an ambiguous acronym, and its correct meaning. Samples came from scientific papers from arXiv.","62,441",English,Acronym Disambiguation,Paper,Link
Veyseh et al.,Acronym Identification,Task is to to find the acronyms and the phrases that have been abbreviated by the acronyms in the document.,"17,506",English,Acronym Identification,Paper,Link
Ant,Ant Financial Question Matching Corpus (AFQMC) (CLUE Benchmark),Dataset is a binary classification task that aims to predict whether two sentences are semantically similar.,n/a,Chinese,Semantic Textual Similarity,Paper,Link
https://github.com/skdjfla,TouTiao Text Classification for News Titles (TNEWS) (CLUE Benchmark),"Dataset consists of Chinese news published by TouTiao before May 2018, with a total of 73,360 titles. Each title is labeled with one of 15 news categories (finance, technology, sports, etc.) and the task is to predict which category the title belongs to.","73,36",Chinese,Classification,Paper,Link
IFLYTEK,IFLYTEK (CLUE Benchmark),"Dataset contains 17,332 long text annotation data about app application descriptions, including various application topics related to daily life. The task is to classify the descriptions from 119 categories.","17,332",Chinese,Classification,Paper,Link
n/a,The Chinese Winograd Schema Challenge (CLUEWSC2020) (CLUE Benchmark),Dataset is an anaphora/coreference resolution task where the model is asked to decide whether a pronoun or noun (phrase) in a sentence co-refer. Data comes from 36 contemporary literary works in Chinese.,"1,838",Chinese,Coreference Resolution,Paper,Link
Li Yudong et al.,The Chinese Science and Technology Literature Data Set (CSL) (CLUE Benchmark),Dataset is taken from the abstracts of Chinese papers and their keywords. The papers are selected from some core journals of Chinese social sciences and natural sciences. Use tf-idf to generate a mixture of fake keywords and real keywords in the paper to construct abstract-keyword pairs. The task goal is to judge whether the keywords are all real keywords based on the abstract.,n/a,Chinese,Keyword Recognition,Paper,Link
Zheng et al.,ChID (CLUE Benchmark),"Dataset is a Chinese idiom cloze test dataset which contains 498,611 passages with 623,377 blanks covered from news, novels and essays.","498,611",Chinese,Idiom Comprehension,Paper,Link
Shavrina et al.,Russian Commitment Bank (RCB) (SuperGlue),"Dataset is a corpus of naturally occurring discourses whose final sentence contains a clause-embedding predicate under an entailment canceling operator (question, modal, negation, antecedent of conditional).","1,006",Russian,Natural Language Inference (NLI),Paper,Link
Shavrina et al.,Choice of Plausible Alternatives for Russian language (PARus) (SuperGlue),"Dataset is composed of a premise and two alternatives, where the task is to select the alternative that more plausibly has a causal relation with the premise.",1,Russian,Commonsense,Paper,Link
Shavrina et al.,Russian Multi-Sentence Reading Comprehension (MuSeRC) (SuperGlue),"Dataset for question answering in which questions can only be answered by taking into account information from multiple sentences. It contains approximately 6,000 questions for more than 800 paragraphs across 5 different domains, namely: 1) elementary school texts, 2) news, 3) fiction stories, 4) fairy tales, 5) brief annotations of TV series and books.",800,Russian,Question Answering,Paper,Link
Shavrina et al.,Textual Entailment Recognition for Russian (TERRa) (SuperGlue),"This task requires to recognize, given two text fragments, whether the meaning of one text is entailed (can be inferred) from the other text.","6,121",Russian,Natural Language Inference (NLI),Paper,Link
Panchenko et al.,Words in Context (RUSSe) (SuperGlue),"Given two sentences and a polysemous word, which occurs in both sentences, the task is to determine, whether the word is used in the same sense in both sentences, or not.","40,504",Russian,Word Sense Disambiguation,Paper,Link
Shavrina et al.,The Winograd Schema Challenge Russian (RWSD) (SuperGlue),Dataset is constructed as translation of the English Winograd Schema Challenge. The task consists of a pair of sentences that differ in only one or two words and that contain an ambiguity that is resolved in opposite ways in the two sentences and requires the use of world knowledge and reasoning for its resolution.,964,Russian,Coreference Resolution,Paper,Link
Shavrina et al.,DaNetQA (SuperGlue),Dataset is a question-answering corpus comprising of natural yes/no questions.,982,Russian,Binary Question Answering,Paper,Link
Shavrina et al.,Russian Reading Comprehension with Commonsense reasoning (RuCoS) (SuperGlue),"Dataset consists of passages and cloze-style queries automatically generated from Russian news articles, namely Lenta4 and Deutsche Welle5. Dataset is used for commonsense reasoning modeled after the ReCord dataset.","80,71",Russian,Commonsense,Paper,Link
Conneau & Wenzek et al.,CC100-Afrikaans,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 305M.,n/a,Afrikaans,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Amharic,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 133M.,n/a,Amharic,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Arabic,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 5.4G.,n/a,Arabic,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Assamese,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 7.6M.,n/a,Assamese,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Azerbaijani,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 1.3G.,n/a,Azerbaijani,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Belarusian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 692M.,n/a,Belarusian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Bulgarian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 9.3G.,n/a,Bulgarian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Bengali,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 860M.,n/a,Bengali,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Bengali Romanized,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 164M.,n/a,Bengali Romanized,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Breton,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 21M.,n/a,Breton,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Bosnian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 18M.,n/a,Bosnian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Catalan,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 2.4G.,n/a,Catalan,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Czech,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 4.4G.,n/a,Czech,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Welsh,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 179M.,n/a,Welsh,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Danish,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 12G.,n/a,Danish,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-German,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 18G.,n/a,German,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Greek,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 7.4G.,n/a,Greek,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-English,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 82G.,n/a,English,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Esperanto,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 250M.,n/a,Esperanto,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Spanish,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 14G.,n/a,Spanish,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Estonian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 1.7G.,n/a,Estonian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Basque,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 488M.,n/a,Basque,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Persian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 20G.,n/a,Persian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Fulah,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 3.1M.,n/a,Fulah,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Finnish,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 15G.,n/a,Finnish,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-French,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 14G.,n/a,French,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Frisian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 38M.,n/a,Frisian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Irish,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 108M.,n/a,Irish,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Scottish Gaelic,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 22M.,n/a,Scottish Gaelic,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Galician,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 708M.,n/a,Galician,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Guarani,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 1.5M.,n/a,Guarani,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Gujarati,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 242M.,n/a,Gujarati,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Hausa,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 61M.,n/a,Hausa,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Hebrew,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 6.1G.,n/a,Hebrew,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Hindi,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 2.5G.,n/a,Hindi,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Hindi Romanized,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 129M.,n/a,Hindi Romanized,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Croatian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 5.7G.,n/a,Croatian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Haitian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 9.1M.,n/a,Haitian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Hungarian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 15G.,n/a,Hungarian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Armenian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 776M.,n/a,Armenian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Indonesian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 36G.,n/a,Indonesian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Igbo,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 6.6M.,n/a,Igbo,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Icelandic,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 779M.,n/a,Icelandic,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Italian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 7.8G.,n/a,Italian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Japanese,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 15G.,n/a,Japanese,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Javanese,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 37M.,n/a,Javanese,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Georgian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 1.1G.,n/a,Georgian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Kazakh,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 889M.,n/a,Kazakh,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Khmer,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 153M.,n/a,Khmer,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Kannada,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 360M.,n/a,Kannada,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Korean,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 14G.,n/a,Korean,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Kurdish,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 90M.,n/a,Kurdish,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Kyrgyz,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 173M.,n/a,Kyrgyz,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Latin,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 609M.,n/a,Latin,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Ganda,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 7.3M.,n/a,Ganda,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Limburgish,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 2.2M.,n/a,Limburgish,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Lingala,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 2.3M.,n/a,Lingala,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Lao,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 63M.,n/a,Lao,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Lithuanian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 3.4G.,n/a,Lithuanian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Latvian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 2.1G.,n/a,Latvian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Malagasy,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 29M.,n/a,Malagasy,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Macedonian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 706M.,n/a,Macedonian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Malayalam,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 831M.,n/a,Malayalam,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Mongolian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 397M.,n/a,Mongolian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Marathi,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 334M.,n/a,Marathi,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Malay,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 2.1G.,n/a,Malay,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Burmese,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 46M.,n/a,Burmese,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Burmese (Zawgyi),This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 178M.,n/a,Burmese (Zawgyi),Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Nepali,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 393M.,n/a,Nepali,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Dutch,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 7.9G.,n/a,Dutch,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Norwegian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 13G.,n/a,Norwegian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Northern Sotho,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 1.8M.,n/a,Northern Sotho,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Oromo,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 11M.,n/a,Oromo,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Oriya,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 56M.,n/a,Oriya,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Punjabi,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 90M.,n/a,Punjabi,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Polish,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 12G.,n/a,Polish,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Pashto,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 107M.,n/a,Pashto,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Portuguese,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 13G.,n/a,Portuguese,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Quechua,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 1.5M.,n/a,Quechua,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Romansh,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 4.8M.,n/a,Romansh,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Romanian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 16G.,n/a,Romanian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Russian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 46G.,n/a,Russian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Sanskrit,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 44M.,n/a,Sanskrit,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Sinhala,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 452M.,n/a,Sinhala,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Sardinian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 143K.,n/a,Sardinian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Sindhi,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 67M.,n/a,Sindhi,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Slovak,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 6.1G.,n/a,Slovak,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Slovenian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 2.8G.,n/a,Slovenian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Somali,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 78M.,n/a,Somali,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Albanian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 1.3G.,n/a,Albanian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Serbian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 1.5G.,n/a,Serbian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Swati,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 86K.,n/a,Swati,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Sundanese,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 15M.,n/a,Sundanese,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Swedish,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 21G.,n/a,Swedish,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Swahili,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 332M.,n/a,Swahili,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Tamil,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 1.3G.,n/a,Tamil,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Tamil Romanized,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 68M.,n/a,Tamil Romanized,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Telugu,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 536M.,n/a,Telugu,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Telugu Romanized,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 79M.,n/a,Telugu Romanized,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Thai,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 8.7G.,n/a,Thai,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Tagalog,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 701M.,n/a,Tagalog,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Tswana,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 8.0M.,n/a,Tswana,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Turkish,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 5.4G.,n/a,Turkish,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Uyghur,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 46M.,n/a,Uyghur,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Ukrainian,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 14G.,n/a,Ukrainian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Urdu,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 884M.,n/a,Urdu,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Urdu Romanized,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 141M.,n/a,Urdu Romanized,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Uzbek,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 155M.,n/a,Uzbek,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Vietnamese,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 28G.,n/a,Vietnamese,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Wolof,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 3.6M.,n/a,Wolof,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Xhosa,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 25M.,n/a,Xhosa,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Yiddish,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 51M.,n/a,Yiddish,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Yoruba,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 1.1M.,n/a,Yoruba,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Chinese (Simplified),This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 14G.,n/a,Chinese (Simplified),Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Chinese (Traditional),This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 5.3G.,n/a,Chinese (Traditional),Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100-Zulu,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 4.3M.,n/a,Zulu,Text Corpora,Paper,Link
Vilares et al.,HeadQA,"Dataset is a multichoice testbed of graduate-level questions about medicine, nursing, biology, chemistry, psychology, and pharmacology.","6,765","Spanish, English",Question Answering,Paper,Link
Leino et al.,FinChat,"Dataset contains conversations with message timestamps, sender’s id, and metadata information. It contains 86 conversations with 3,630 messages, 22,210 words with the average word length of 5.6, and on the average 14 turns per each conversation.","3,63",Finnish,Dialogue,Paper,Link
Chen et al.,Open Table-and-Text Question Answering (OTT-QA),Dataset contains open questions which require retrieving tables and text from the web to answer. The dataset is built on the HybridQA dataset.,45,English,Open Domain Question Answering,Paper,Link
Byrne et al.,Taskmaster-3,"Dataset consists of 23,757 movie ticketing dialogs. ""Movie ticketing"" is defined as conversations where the customer's goal is to purchase tickets after deciding on theater, time, movie name, number of tickets, and date, or opt out of the transaction.","23,757",English,Dialogue,Paper,Link
Asai et al.,XOR-TyDi QA,"Dataset is a multi-lingual open-retrieval QA dataset that enables cross-lingual answer retrieval. It consists of questions written by information-seeking native speakers in 7 typologically diverse languages and answer annotations that are retrieved from multilingual document collections. There are three sub-tasks: XOR-Retrieve, XOR-EnglishSpan, and XOR-Full.",40,Multi-Lingual,Question Answering,Paper,Link
Mosig et al.,STAR,"A schema-guided task oriented dialog dataset consisting of 127,833 utterances and knowledge base queries across 5,820 task-oriented dialogs in 13 domains that is especially designed to facilitate task and domain transfer learning in task-oriented dialog.","5,82",English,Dialogue,Paper,Link
Quan et al.,RiSAWOZ,"Dataset contains 11.2K human-to-human (H2H) multiturn semantically annotated dialogues, with more than 150K utterances spanning over 12 domains.","11,2",Chinese,Dialogue,Paper,Link
Koksal et al.,RELX & RELX-Distant,Two datasets for cross-lingual relation classification are included: RELX and RELXDistant. RELX contains 502 parallel sentences per language (total of 5 languages) with 18 relations with direction and no_relation in total of 37 categories. RELX-Distant was extracted from Wikipedia & Wikidata.,n/a,Multi-Lingual,Relation Classification,Paper,Link
Lourie et al.,Scruples,"Scruples contains 2 datasets: Anecdotes and Dilemmas. Anecdotes contains 32,000 real-life anecdotes about complex ethical situations, with 625,000 ethical judgments extracted from reddit. Dilemmas contains 10,000 ethical dilemmas in the form of paired actions, where the model must identify which one was considered less ethical by crowd workers on Mechanical Turk.","32,766",English,Commonsense,Paper,Link
Li et al.,MTOP,"Dataset contains 100k annotated utterances in 6 languages (English Germany French Spanish Hindi Thai) across 11 domains. Dataset contains a mix of both simple as well as compositional nested queries across 11 domains, 117 intents and 78 slots.","100,997",Multi-Lingual,Semantic Parsing,Paper,Link
Kershaw et al.,Elsevier OA CC-BY,"Dataset contains 40, 091 open access (OA) CC-BY articles from across Elsevier’s journals.","40, 091",English,Text Corpora,Paper,Link
Karim et al.,Bengali Hate Speech,"Dataset contains Bengali text classified into 5 categories: personal hate, political hate, religious hate, geopolitical hate, & gender abusive hate.","3,418",Bengali,"Classification, Hate Speech Detection",Paper,Link
Wu et al.,Microsoft News Dataset (MIND),Dataset contains ~160k English news articles and more than 15 million impression logs generated by 1 million users.,160,English,News Recommendation,Paper,Link
Moon et al.,Situated and Interactive Multimodal Conversations (SIMMC),"There are 2 datasets totalling ∼13K human-human dialogs (∼169K utterances) using a multimodal Wizard-of-Oz (WoZ) setup, on two shopping domains: (a) furniture (grounded in a shared virtual environment) and, (b) fashion (grounded in an evolving set of images).",13,English,"Dialogue, Multi-Modal",Paper,Link
Arora et al.,HINT3,"In total there are three datasets: SOFMattress, Curekart and Powerplay11 with each containing diverse set of intents in a single domain - mattress products retail, fitness supplements retail and online gaming respectively. Each datasets spans multiple coarse and fine grain intents, with the test sets being drawn entirely from actual user queries on live systems at scale instead of being crowdsourced.",n/a,English,Intent Detection,Paper,Link
Gu et al.,NewSHead,"Dataset contains 369,940 English stories with 932,571 unique URLs, among which we have 359,940 stories for training, 5,000 for validation, and 5,000 for testing, respectively. Each news story contains at least three (and up to five) articles.","369,94",English,"Summarization, Headline Generation",Paper,Link
Malajyan et al.,Armenian Paraphrase Detection Corpus (ARPA),Dataset used for paraphrase detection in Armenian was collected from news texts consisting of articles written in the last 10 years from Hetq and Panarmenian news websites.,"2,36",Armenian,Paraphrase Identification,Paper,Link
Doostmohammadi et al.,PerKey,"Dataset contains 553K news articles from six Persian news websites and agencies with author extracted keyphrases, which is then filtered and cleaned to achieve higher quality keyphrases.","553,111",Persian,"Keyphrase Extraction, Information Extraction",Paper,Link
Liu et al.,LiveQA,"Dataset was constructed from play-by-play live broadcast. It contains 117k multiple-choice questions written by human commentators for over 1,670 NBA games, which are collected from the Chinese Hupu1 website.",117,Chinese,Question Answering,Paper,Link
Nguyen et al.,Vietnamese Question Answering Dataset (ViQuAD),"Dataset comprises over 23,000 human-generated question-answer pairs based on 5,109 passages of 174 Vietnamese articles from Wikipedia. [REQUIRES GETTING AUTHOR PERMISSION]",23,Vietnamese,Question Answering,Paper,Link
Chu et al.,NatCat,"Dataset contains naturally annotated category-text pairs for training text classifiers derived from 3 sources: Wikipedia, Reddit, and Stack Exchange.",12M+,English,Text Classification,Paper,Link
Mehri et al.,DialoGLUE,"Benchmark for task oriented dialogue containing 7 datasets: Banking77 containing online banking queries, HWU64 containing popular personal assistant queries, CLINC150 containing popular personal assistant queries, Restaurant8k containing restaurant booking domain queries, DSTC8 SGD containing multi-domain, task-oriented conversations between a human and a virtual assistant, and TOP containing compositional queries for hierachical semantic representations, MultiWOZ 2.1 12K multi-domain dialogues with multiple turns.",100,English,Dialogue,Paper,Link
Zampieri et al.,Offensive Language Identification Dataset (OLID),"Dataset contains a collection of 14,200 annotated English tweets using an annotation model that encompasses three levels: offensive language detection, categorization of offensive language, and offensive language target identification.","14,2",English,"Text Classification, Hate Speech Detection",Paper,Link
Rikters et al.,Business Scene Dialogue (BSD),"Dataset contains 955 scenarios, 30,000 parallel sentences in English-Japanese.",30,"Japanese, English",Machine Translation,Paper,Link
Saxena et al.,English Possible Idiomatic Expressions (EPIE),"Dataset containing 25,206 sentences labelled with lexical instances of 717 idiomatic expressions.","25,506",English,"Text Classification, Idiomatic Expressions",Paper,Link
Gudkov et al.,ParaPhraser Plus,"Dataset contains 7,227 pairs of sentences, which are classified by humans into three classes: 2,582 non-paraphrases, 2,957 near-paraphrases,and 1,688 precise-paraphrases.","7,227",Russian,Paraphrase Generation,Paper,Link
Liu et al.,MedDG,"Dataset contains more than 17K conversations collected from the online health consultation community relating to 12 types of common Gastrointestinal diseases. Five different categories of entities, including diseases, symptoms, attributes, tests, and medicines, are annotated in each conversation of MedDG as additional labels.",17,Chinese,Dialogue,Paper,Link
Wenzek et al.,CC Net,Dataset of the common crawl corpus that has been cleaned and deduplicated. This pipeline preserves the structure of documents and filter the data based on their distance to Wikipedia.,A LOT!,Multi-Lingual,Text Corpora,Paper,Link
Kolhatkar et al.,SFU Opinion and Comments Corpus (SOCC),"Dataset contains 10,339 opinion articles (editorials, columns, and op-eds) together with their 663,173 comments from 303,665 comment threads, from the main Canadian daily in English, The Globe and Mail, from January 2012 to December 2016. In addition there's a subset annotated corpus measuring toxicity, negation and its scope, and appraisal containing 1,043 annotated comments in responses to 10 different articles covering a variety of subjects: technology, immigration, terrorism, politics, budget, social issues, religion, property, and refugees.","663,173",English,"Text Corpora, Text Classification",Paper,Link
Ko et al.,Inquisitive,"Dataset contains ∼19K questions that are elicited while a person is reading through a document. Compared to existing datasets, INQUISITIVE questions target more towards high-level (semantic and discourse) comprehension of text.",19,English,Question Generation,Paper,Link
Kolhatkar et al.,Constructive Comments Corpus (C3),"Dataset is a subset of comments from the SFU Opinion and Comments Corpus. This subset, the Constructive Comments Corpus (C3) consists of 12,000 comments annotated by crowdworkers.",12,English,Text Classification,Paper,Link
ACL 2016,News Commentary Parallel Corpus,Dataset consists of parallel corpora consisting of political and economic commentary crawled from the web site Project Syndicate.,n/a,Multi-Lingual,Machine Translation,Paper,Link
Wang et al.,CoVoST,"Dataset is a multilingual speech-to-text translation corpus covering translations from 21 languages into English and from English into 15 languages. The overall speech duration is 2,880 hours. The total number of speakers is 78K.","2,880 Hours",Multi-Lingual,"Machine Translation, Speech-To-Text",Paper,Link
Svajlenko & Wang et al.,CodeXGLUE: BigCloneBench Dataset,"Given two codes as the input, the task is to do binary classification (0/1), where 1 stands for semantic equivalence and 0 for others. Models are evaluated by F1 score.","1,731,860",Coding Lang: Java,Clone Detection,Paper,Link
Mou et al.,CodeXGLUE: POJ-104,"Given a code and a collection of candidates as the input, the task is to return Top K codes with the same semantic. Models are evaluated by MAP score.",52,Coding Lang: C/C++,Clone Detection,Paper,Link
Zhou et al.,CodeXGLUE: Defect Detection Dataset,"Given a source code, the task is to identify whether it is an insecure code that may attack software systems, such as resource leaks, use-after-free vulnerabilities and DoS attack. We treat the task as binary classification (0/1), where 1 stands for insecure code and 0 for secure code.","27,318",Coding Lang: C,Defect Detection,Paper,Link
CodeXGLUE & Husain et al.,CodeXGLUE: CT-All,"The task is aimed to predict the answers for the blank with the context of the blank, which can be formulated as a multi-choice classification problem. Each instance in the dataset contains a masked code function, its docstring and the target word.","176,115","Coding Lang: Python, Java, PHP, Javascript, Ruby, Go",Cloze-Test,Paper,Link
CodeXGLUE & Feing & Husain et al.,CodeXGLUE: CT-Max/Min,"The difference between this dataset and CT-All is that this dataset only contains two words. The task is aimed to predict the answers for the blank with the context of the blank, which can be formulated as a multi-choice classification problem. Each instance in the dataset contains a masked code function, its docstring and the target word.","2,615","Coding Lang: Python, Java, PHP, Javascript, Ruby, Go",Cloze-Test,Paper,Link
Raychev & Allamanis et al.,CodeXGLUE: PY 150/Java Corpus token,Datasets used for code completion on the token level for Python and Java.,155,"Coding Lang: Python, Java",Code Token Completion,Paper,Link
Raychev & Allamanis et al.,CodeXGLUE: PY 150/Java Corpus line,Datasets used for code completion on the line level for Python and Java.,13,"Coding Lang: Python, Java",Code Line Completion,Paper,Link
Tufano et al.,CodeXGLUE: Bugs2Fix,"Given a piece of Java code with bugs, the task is to remove the bugs to output the refined code. Models are evaluated by BLEU scores, accuracy (exactly match) and CodeBLEU.","123,804",Coding Lang: Java,Code Refinement,Paper,Link
CodeXGLUE,CodeXGLUE: CodeTrans,"Given a piece of Java (C#) code, the task is to translate the code into C# (Java) version. Models are evaluated by BLEU scores, accuracy (exactly match), and CodeBLEU scores.","11,8","Coding Lang: Java, C#",Code Translation,Paper,Link
Husain et al.,"CodeXGLUE: CodeSearchNet, AdvTest","Given a natural language prompt, the task is to search source code that matches the natural language. To test the generalization ability of a model, function names and variables in test sets are replaced by special tokens.","280,634",Coding Lang: Python,Code Search,Paper,Link
CodeXGLUE & Yao et al.,CodeXGLUE: NL Code Search WebQuery,Code Search is aimed to find a code snippet which best matches the demand of the query. This task is formulated in text-code classification.,"5,93",Coding Lang: Python,Code Search,Paper,Link
Iyer et al.,CodeXGLUE: CONCODE,Dataset is used for when a model is given the task to generate a code given natural language description.,104,Coding Lang: Java,Text-to-Code Generation,Paper,Link
Elazar & Goldberg,Numeric Fused-Heads,"Dataset contains annotated sentences of numeric-fused-heads, along with their ""missing head"". A number refers to an implicit (and not explicitly provided) reference. For example, in the sentence ""I miss being 10"", the number 10 refers to the age of 10, but is not explicitly said.","9,412",English,"Numeric Fused-Head, Missing Elements, Commonsense",Paper,Link
Tuggener et al.,LEDGAR,"LEDGAR is a multilabel corpus of legal provisions in contracts suited for text classification in the legal domain (legaltech). It features over 1.8M+ provisions and a set of 180K+ labels. A smaller, cleaned version of the corpus is also available.",1.8M,English,Classification,Paper,Link
Sun et al.,C3,"Dataset is first free-form multipleChoice Chinese machine reading Comprehension dataset (C3), containing 13,369 documents (dialogues or more formally written mixed-genre texts) and their associated 19,577 multiple-choice free-form questions collected from Chinese-as-a-second language examinations.","13,369",Chinese,"Dialogue, Question Answering",Paper,Link
Broscheit et al.,Olpbench,"Dataset contains 30M open triples, 1M distinct open relations and 2.5M distinct mentions of approximately 800K entities. Dataset is used for open link prediction task.",30M,English,"Open Link Prediction, Knowledge Base",Paper,Link
Han et al.,FewRel 1.0,"Dataset is a few-shot relation extraction dataset, which contains more than one hundred relations and tens of thousands of annotated instances cross different domains.",70,English,Relation Extraction,Paper,Link
Hendrickx et al.,SemEval2010 Task 8,"Dataset consists of 8,000 sentences annotated for Cause-Effect , Instrument-Agency, Product-Producer, Content-Container, Entity-Origin, Entity-Destination, Component-Whole, Member-Collection, and Message-Topic.",8,English,Relation Extraction,Paper,Link
Mesquita et al.,KnowledgeNet,KnowledgeNet is a benchmark dataset for the task of automatically populating a knowledge base (Wikidata) with facts expressed in natural language text on the web.,9,English,"Relation Extraction, Knowledge Base Population",Paper,Link
Lehman et al.,Evidence Inference,"Dataset contains 10,137 annotated prompts for 2,419 unique article with the task of inferring whether a given clinical treatment is effective with respect to a specified outcome. The dataset provides a prompt that specifies an intervention, a comparator, and an outcome, along with a fulltext article. The model is then used to infer the reported findings with respect to this prompt.","10,137",English,Information Extraction,Paper,Link
Sai et al.,DailyDialog++,"DailyDialog++ is an open-domain dialogue evaluation dataset consisting of 19k contexts with five relevant responses for each context. Additionally for 11k contexts, it includes five adversarial irrelevant responses which are specifically crafted to have lexical or semantic overlap with the context but are still unacceptable as valid responses.",19,English,"Dialogue, Adversarial",Paper,Link
Liu et al.,LogiQA,"Dataset consists of 8,678 QA instances, covering multiple types of deductive reasoning. Multiple-choice.","8,678",English,"Question Answering, Commonsense, Reading Comprehension",Paper,Link
Kong et al.,SCDE,"Dataset of sentence-level cloze questions sourced from public school examinations. Each instance consists of a passage with multiple sentence-level blanks and a shared set of candidates. Besides the right answer to each cloze in the passage, the candidate set also contains ones which don’t answer any cloze, called distractors. [requires contacting authors for data]","14,062",English,Sentence Level Cloze Completion,Paper,Link
Longpre et al.,Multilingual Knowledge Questions & Answers (MKQA),Dataset is an open-domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total).,10,Multi-Lingual,Question Answering,Paper,Link
Safavi et al.,CoDEx,"Three graph datasets containing positive and hard negative triples, entity types, entity and relation descriptions, and Wikipedia page extracts for entities.","1,156,222",English,"Link Prediction, Triple Classification",Paper,Link
Lamm et al.,QED,"Given a question and a passage, QED represents an explanation of the answer as a combination of discrete, human-interpretable steps: sentence selection, referential equality, and predicate entailment. Dataset was built as a subset of the Natural Questions dataset.","8,993",English,"Question Answering, Explainability",Paper,Link
Andreas et al.,SMCalFlow,"Dataset contains natural conversations about tasks involving calendars, weather, places, and people. Each turn is annotated with an executable dataflow program featuring API calls, function composition, and complex constraints built from strings, numbers, dates and times.","36,296",English,Dialogue,Paper,Link
Rameshkumar et al.,Critical Role Dungeons and Dragons Dataset (CRD3),"Dataset is collected from 159 Critical Role episodes transcribed to text dialogues, consisting of 398,682 turns. It also includes corresponding abstractive summaries collected from the Fandom wiki. Critical Role is an unscripted, live-streamed show where a fixed group of people play Dungeons and Dragons.","398,682",English,Dialogue,Paper,Link
Lo et al.,The Semantic Scholar Open Research Corpus (S2ORC),Dataset contains 136M+ paper nodes with 12.7M+ full text papers and connected by 467M+ citation edges.,"467M edges, 136M nodes",English,"Text Corpora, Knowledge Base",Paper,Link
Mernyei et al.,Wiki-CS,"Dataset consists of nodes corresponding to Computer Science articles, with edges based on hyperlinks and 10 classes representing different branches of the field.","216,123 Edges, 11,701 Nodes",English,"Node Classification, Relation Link Prediction",Paper,Link
Elgohary et al.,Semantic Parsing with Language Assistance from Humans (SPLASH),"Dataset enables text-to-SQL systems to seek and leverage human feedback to further improve the overall performance and user experience. Dataset contains 9,314 question-feedback pairs, 8,352 of which, correspond to questions in the Spider training split and 962 from the spider development split.","9,314",English,Semantic Parse Correction,Paper,Link
Aliannejadi et al.,ClariQ,"Dataset consists of single-turn conversations (initial_request, followed by clarifying question and answer). In addition, it comes with synthetic multi-turn conversations (up to three turns). ClariQ features approximately 18K single-turn conversations, as well as 1.8 million multi-turn conversations.",1.8M,English,"Question Clarification, Dialogue",Paper,Link
Amirkhani et al.,FarsTail,"Dataset contains 10,367 instances generated from a collection of 3,539 multiple choice questions in the Farsi language. The train, validation, and test portions include 7,266, 1,537, and 1,564 instances.","10,367",Persian (Farsi),Natural Language Inference (NLI),Paper,Link
Yu et al.,DialogRE,"Dataset contains human-annotated dialogue-based relation extraction containing 1,788 dialogues originating from the complete transcripts of a famous American television situation comedy ""Friends"". There are 36 possible relation types that exist between an argument pair in a dialogue.","1,788","Chinese, English","Dialogue, Relation Extraction",Paper,Link
Krishna et al.,Visual Genome,"Dataset contains over 100K images where each image has an average of 21 objects, 18 attributes, and 18 pairwise relationships between objects.","100,000+",English,"Visual Question Answering, Knowkedge Base",Paper,Link
He et al.,CraigsListBargain,"Dataset contains 6,682 human-human dialogues where 2 agents negotiate the sale/purchase of an item.","6,682",English,Dialogue,Paper,Link
Eric et al.,"A Multi-Turn, Multi-Domain Dialogue Dataset (KVRET)","Dataset contains 3,031 multi-turn dialogues in three distinct domains appropriate for an in-car assistant: calendar scheduling, weather information retrieval, and point-of-interest navigation.","3,301",English,Dialogue,Paper,Link
CMU,CMU_ARCTIC,"Dataset contains 1,150 utterances carefully selected from out-of-copyright texts from Project Gutenberg. The databases include US English male (bdl) and female (slt) speakers (both experinced voice talent) as well as other accented speakers.","1,15",English,Speech Recognition,Paper,Link
Zhao et al.,L2-ARTIC,"Dataset includes recordings from twenty-four (24) non-native speakers of English whose first languages (L1s) are Hindi, Korean, Mandarin, Spanish, Arabic and Vietnamese, each L1 containing recordings from two male and two female speakers. Each speaker recorded approximately one hour of read speech from CMU’s ARCTIC prompts.",n/a,English (Non-Native),Speech Recognition,Paper,Link
Lahiri et al.,ACL Anthology Reference Corpus (ACL ARC),"Dataset contains 10,921 articles from the February 2007 snapshot of the Anthology; text and metadata for the articles were extracted, consisting of BibTeX records derived either from the headers of each paper or from metadata taken from the Anthology website.","10,921",English,Text Corpora,Paper,Link
Zhu et al.,NCLS-Corpora,"Contains two datasets for cross-lingual summarization: ZH2ENSUM and EN2ZHSUM. There exists 370,759 English-to-Chinese cross-lingual summarization (CLS) pairs from ENSUM and 1,699,713 Chinese-to-English CLS pairs.",2M+,"Chinese, English",Cross-Lingual Summarization,Paper,Link
Wang et al.,TalkDown,Dataset used for classifying condescending acts in context. Dataset was extracted from Reddit COMMENT and REPLY pairs in which the REPLY targets a specific quoted span (QUOTED) in the COMMENT as being condescending.,n/a,English,Hate Speech Detection,Paper,Link
Jeretic et al.,Implicature and Presupposition Diagnostic dataset (IMPPRES),"Dataset contains semiautomatically generated sentence pairs illustrating well-studied pragmatic inference types. IMPPRES follows the format of SNLI, MultiNLI and XNLI, which was created to evaluate how well trained NLI models recognize several classes of presuppositions and scalar implicatures.","25,000+",English,Natural Language Inference (NLI),Paper,Link
Alva-Manchego et al.,Abstractive Sentence Simplification Evaluation and Tuning (ASSET),"Dataset consists of 23,590 human simplifications associated with the 2,359 original sentences from TurkCorpus (10 simplifications per original sentence).","23,59",English,Sentence Simplification,Paper,Link
Park et al.,Visual Commonsense Graphs,"Dataset consists of over 1.4 million textual descriptions of visual commonsense inferences carefully annotated over a diverse set of 59,000 images, each paired with short video summaries of before and after.",59,English,"Visual Question Answering, Commonsense",Paper,Link
Lee et al.,Open-Domain Spoken Question Ansswering Dataset (ODSQA),"Dataset contains questions in both text and spoken forms, a multi-sentence spoken-form document and a word span answer based from the document.","3,000+",Chinese,Speech Question Answering,Paper,Link
Nguyen et al.,Vietnamese Multiple-choice Machine Reading Comprehension Corpus (ViMMRC),"Dataset contains 2,783 multiple-choice questions and answers based on a set of 417 Vietnamese texts used for teaching reading comprehension for 1st to 5th graders. [requires contacting author for corpus]",417,Vietnamese,"Question Answering, Reading Comprehension",Paper,Link
Nguyen et al.,Vietnamese Students’ Feedback Corpus (UIT-VSFC),"Dataset contains over 16,000 sentences which are human-annotated with two different tasks: sentiment-based and topic-based classifications.","16,000+",Vietnamese,"Text Classification, Sentiment Analysis",Paper,Link
Rashkin et al.,Story Commonsense,"Dataset contains a total of 300k low-level annotations for motivation and emotion across15,000 stories (randomly selected from the ROC story training set). It covers over 150,000 character-line pairs, in which 56k character-line pairs have an annotated motivation and 105k have an annotated change in emotion (i.e. a label other than none).",15,English,Commonsense,Paper,Link
Thin et al.,UIT-SPC,"Dataset contains 1,565 papers of top NLP/CL conferences such as ACL, CoNLL , EACL NAACL and EMNLP. They are pre-processed by removing unnecessary information (e.g formula, table, etc). Then, they were formatted to .xml that includes the title paper, sections, and sub-sections according to the paper's structure. [requires contacting author for corpus]","1,565",Vietnamese,Text Corpora,Paper,Link
Cho et al.,Intonation-Aided Intention Identification for Korean (3i4K),Dataset contains seven class annotated corpus of single text utterances/intents in conversation.,61,Korean,Text Classification,Paper,Link
Venugopal et al.,Aesthetics Text Corpus,"Dataset consists of novels and short stories written in Hindi language. Novels and stories were scraped from http://hindisamay.com, http://premchand.co.in, a website dedicated to the popular novelist Premchand’s stories, and Bhandarkar Oriental Research Institute’s Digital Library (http://borilib.com). As a preprocessing step, the text was split into sentences and special characters, English tokens and Latin numbers were deleted.",978,Hindi,Text Corpora,Paper,Link
Nadeem et al.,StereoSet,"Dataset that measures stereotype bias in language models. StereoSet consists of 17,000 sentences that measures model preferences across gender, race, religion, and profession.",17,English,Language Model Evaluation,Paper,Link
Ho et al.,Vietnamese Social Media Emotion Corpus (UIT-VSMEC),"Dataset contains 6,927 human-annotated sentences with six emotion labels, contributing to emotion recognition research in Vietnamese.","6,927",Vietnamese,Emotion Classification,Paper,Link
Lam et al.,Vietnamese Image Captioning Dataset (UIT-ViIC),"Dataset consists of 19,250 captions for 3,850 images on sport-ball. [requires contacting author for corpus]","3,85",Vietnamese,Automatic Image Captioning,Paper,Link
Sap et al.,Hippocorpus,"Dataset of 6,854 English diary-like short stories about recalled and imagined events.","6,854",English,Text Corpora,Paper,Link
Srivastava et al.,Web Demonstration and Explanation Dataset (Web-D-E),Dataset consists of 520 explanations and corresponding demonstrations of web-based tasks from the Mini Word-of-Bits.,520,English,Semantic Parsing,Paper,Link
El-Nouby,GeNeVA,Data contains the CoDraw and i-CLEVR datasets used for the Generative Neural Visual Artist (GeNeVA) task.,n/a,English,Text-to-Image,Paper,Link
Kahou et al.,FigureQA,"Dataset is a visual reasoning corpus of over one million question answer pairs grounded in over 100,000 images. The images are synthetic, scientific-style figures from five classes: line plots, dot-line plots, vertical and horizontal bar graphs, and pie charts.",~1.3M,English,Visual Question Answering,Paper,Link
Smith et al.,BioCreative II Gene Mention Recognition (BC2GM),"Dataset contains data where participants are asked to identify a gene mention in a sentence by giving its start and end characters. The training set consists of a set of sentences, and for each sentence a set of gene mentions (GENE annotations). [registration required for access]",20,English,"Information Extraction, Named Entity Recognition (NER)",Paper,Link
Li et al.,BC5CDR Drug/Chemical (BC5-Chem),Dataset consists of three separate sets of articles with chemicals and their relations annotated. [registration required for access],"1,5",English,"Information Extraction, Named Entity Recognition (NER)",Paper,Link
Li et al.,BC5CDR Disease (BC5-Disease),Dataset consists of three separate sets of articles with chemicals and their relations annotated. [registration required for access],"1,5",English,"Information Extraction, Named Entity Recognition (NER)",Paper,Link
Kim et al.,JNLPBA,The BioNLP / JNLPBA Shared Task 2004 involves the identification and classification of technical terms referring to concepts of interest to biologists in the domain of molecular biology.,"~2,000",English,"Information Extraction, Named Entity Recognition (NER)",Paper,Link
Dogan et al.,NCBI Disease Corpus,"Dataset contains 6,892 disease mentions, which are mapped to 790 unique disease concepts. Of these, 88% link to a MeSH identifier, while the rest contain an OMIM identifier.","6,892",English,"Information Extraction, Named Entity Recognition (NER)",Paper,Link
Nye et al.,EBM PICO,"Dataset contains ~5,000 medical abstracts describing clinical trials, annotated in detail with respect to characteristics of the underlying trial Populations (e.g., diabetics), Interventions (insulin), Comparators (placebo) and Outcomes (blood glucose levels).","~5,000",English,Text Corpora,Paper,Link
Krallinger et al.,ChemProt,"ChemProt [is] a disease chemical biology database, which is based on a compilation of multiple chemical–protein annotation resources, as well as disease-associated protein–protein interactions (PPIs). [registration required for access]",n/a,English,Relation Extraction,Paper,Link
Herroro-Zazo et al.,Drug-Disease Interaction (DDI),"Dataset contains 792 texts selected from the DrugBank database and other 233 Medline abstracts. This fined-grained corpus has been annotated with a total of 18,502 pharmacological substances and 5028 DDIs, including both PK as well as PD interactions.",792,English,Relation Extraction,Paper,Link
Becker et al.,Gene-Disease Associations (GAD),"Dataset is an archive of published genetic association studies that provides a comprehensive, public, web-based repository of molecular, clinical and study parameters for >5,000 human genetic association studies at this time.",5,English,Relation Extraction,Paper,Link
Sogancıoglu et al.,BIOSSES,"Dataset comprises 100 sentence pairs, in which each sentence was selected from the TAC (Text Analysis Conference) Biomedical Summarization Track Training Dataset containing articles from the biomedical domain. TAC dataset consists of 20 articles (reference articles) and citing articles that vary from 12 to 20 for each of the reference articles.",100,English,Semantic Textual Similarity,Paper,Link
Baker et al.,HoC (Hallmarks of Cancer),"Dataset consists of 1,852 PubMed publication abstracts manually annotated by experts according to the Hallmarks of Cancer taxonomy. The taxonomy consists of 37 classes in a hierarchy.","1,852",English,Document Classification,Paper,Link
Jin et al.,PubmedQA,A biomedical question answering (QA) dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes/no/maybe.,"~273,000",English,Question Answering,Paper,Link
Reddy et al.,TRACT: Tweets Reporting Abuse Classification Task Corpus,"Dataset used for multi-class classification task involving three classes of tweets that mention abuse reportings: ""report"" (annotated as 1); ""empathy"" (annotated as 2); and ""general"" (annotated as 3).","4,5",English,Text Classification,Paper,Link
Kiela et al.,Hateful Memes,"Dataset is used to detect hateful memes. In total, the datset contains 10,000 memes comprising of five different types: multimodal hate, where benign confounders were found for both modalities, unimodal hate, where one or both modalities were already hateful on their own, benign image, benign text confounders and finally random not-hateful examples.",10,English,"Multi-Modal, Hate Speech Detection",Paper,Link
Gurulingappa et al.,Adverse Drug Effect (ADE) Corpus,"There's 3 different datasets: DRUG-AE.rel provides relations between drugs and adverse effects, DRUG-DOSE.rel provides relations between drugs and dosages and ADE-NEG.txt provides all sentences in the ADE corpus that DO NOT contain any drug-related adverse effects.","2,972",English,Information Extraction,Paper,Link
Eyal et al.,Hebrew Parallel Movie Subtitles,Dataset derived from subtitles of movies and television shows for the purpose of semantic role labeling in Hebrew. It includes both FrameNet and PropBank annotations.,"30,789",Hebrew,Semantic Role Labeling,Paper,Link
Savery et al.,MEDIQA-Answer Summarization,Dataset containing question-driven summaries of answers to consumer health questions.,156,English,Summarization,Paper,Link
Liu et al.,NEJM-enzh,"Dataset is an English-Chinese parallel corpus, consisting of about 100,000 sentence pairs and 3,000,000 tokens on each side, from the New England Journal of Medicine (NEJM).",100,"Chinese, English",Machine Translation,Paper,Link
Ghalandari et al.,Wikipedia Current Events Portal (WCEP) Dataset,"Dataset is used for multi-document summarization (MDS) and consists of short, human-written summaries about news events, obtained from the Wikipedia Current Events Portal (WCEP), each paired with a cluster of news articles associated with an event.","10,2",English,Summarization,Paper,Link
Xie et al.,Worldtree Corpus,"Dataset contains multi-hop question answering/explanations where questions require combining between 1 and 16 facts (average 6) to generate detailed explanations for question answering inference. Each explanation is represented as a lexically-connected “explanation graph” that combines an average of 6 facts drawn from a semi-structured knowledge base of 9,216 facts across 66 tables.","5,114",English,"Question Answering, Knowledge Base",Paper,Link
Smith et al.,ScienceExamCER,"Dataset contains 133k mentions in the science exam domain where nearly all (96%) of content words have been annotated with one or more fine-grained semantic class labels including taxonomic groups, meronym groups, verb/action groups, properties and values, and synonyms.",133,English,Named Entity Recognition (NER),Paper,Link
Korablinov et al.,RuBQ,"Dataset consists of 1,500 Russian questions of varying complexity, their English machine translations, SPARQL queries to Wikidata, reference answers, as well as a Wikidata sample of triples containing entities with Russian labels.","1,5",Russian,"Question Answering, Knowledge Base",Paper,Link
Cosentino et al.,LibriMix,Dataset is used for speech source separation in noisy environments. It is derived from LibriSpeech signals (clean subset) and WHAM noise. It offers a free alternative to the WHAM dataset and complements it.,400+ Hours,English,Speech Seperation,Paper,Link
Lei et al.,TVQA,"Dataset is used for video question answering and consists of 152,545 QA pairs from 21,793 clips, spanning over 460 hours of video.",460+ Hours,English,"Multi-Modal Learning, Video Question Answering",Paper,Link
Tapaswi et al.,MovieQA,"Dataset used to evaluate automatic story comprehension from both video and text. The data set consists of almost 15,000 multiple choice question answers obtained from over 400 movies.","14,944",English,"Multi-Modal Learning, Video Question Answering",Paper,Link
Jang et al.,TGIF-QA,Dataset consists of 165K QA pairs from 72K animated GIFs. Used for video question answering.,165,English,"Multi-Modal Learning, Video Question Answering",Paper,Link
Li et al.,Tumblr GIF (TGIF),Dataset contains 100K animated GIFs and 120K sentences describing visual content of the animated GIFs.,100,English,Image Description Generation,Paper,Link
Moon et al.,Korean Hate Speech Dataset,"Dataset contains ~9,4K manually labeled entertainment news comments for identifying Korean toxic speech.","9,381",Korean,"Classification, Hate Speech Detection",Paper,Link
Kirkedal et al.,FT Speech,"Dataset contains recorded meetings of the Danish Parliament, otherwise known as the Folketing (FT). The corpus contains over 1,800 hours of transcribed speech by a total of 434 speakers.",1800 Hours,Danish,Speech Corpora,Paper,Link
Salesky et al.,VoxClamantis,"Dataset contains phoneme-level alignments for more than 600 languages, high-resource alignments for ~50 languages, and phonetic measures for all vowels and sibilants. Consists of 690 audio readings of the New Testament of the Bible.",690,Multi-Lingual,Phonetic Typology,Paper,Link
Paperswithcode,ArxivPapers,"Dataset is a corpus of over 100,000 scientific papers related to machine learning.","104,723",English,Text Corpora,Paper,Link
Paperswithcode,SegmentedTables & LinkedResults,"Dataset mentions in captions, the type of table (leaderboard, ablation, irrelevant) and ground truth cell annotations into classes: dataset, metric, paper model, cited model, meta and task.","~2,000",English,"Table Segmentation, Table Type Classification",Paper,Link
Husain et al.,CodeSearchNet Corpus,"Dataset contains functions with associated documentation written in Go, Java, JavaScript, PHP, Python, and Ruby from open source projects on GitHub.",6M,English,Text Corpora,Paper,Link
Suglia et al.,CompGuessWhat?!,"Dataset contains 65,700 dialogues based on GuessWhat?! dataset dialogues and enhanced by including object attributes coming from resources such as VISA attributes, VisualGenome and ImSitu.","65,7",English,"Grounded Language Learning, Visual",Paper,Link
Rush et al.,Gigaword,Dataset contains headline-generation on a corpus of article pairs from Gigaword consisting of around 4 million articles.,4M,English,Summarization,Paper,Link
Ganesan et al.,Opinosis,"Dataset contains sentences extracted from reviews for 51 topics. Topics and opinions are obtained from Tripadvisor, Edmunds.com and Amazon.com.",n/a,English,Summarization,Paper,Link
Kornilova et al.,BillSum,Dataset contains a summarization of US Congressional and California state bills.,"22,218",English,Summarization,Paper,Link
Gliwa et al.,SAMSum,Dataset contains over 16K chat dialogues with manually annotated summaries.,16,English,Summarization,Paper,Link
Zhang et al.,Annotated Enron Subject Line Corpus (AESLC),Dataset contains email messages of employees in the Enron Corporation.,"18,302",English,Summarization,Paper,Link
Fabbri et al.,Multi-News,Dataset consists of news articles and human-written summaries of these articles from the site newser.com. Each summary is professionally written by editors and includes links to the original articles cited.,"56,216",English,Summarization,Paper,Link
Rishabh Misra,News Category Dataset,Dataset contains around 200k news headlines from the year 2012 to 2018 obtained from HuffPost.,"~200,000",English,Classification,Paper,Link
Salamon et al.,UrbanSound & UrbanSound8K,"UrbanSound: Dataset contains 1,302 labeled sound recordings. Each recording is labeled with the start and end times of sound events from 10 classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, enginge_idling, gun_shot, jackhammer, siren, and street_music. UrbanSound8K: Dataset contains 8,732 labeled sound excerpts (<=4s) of urban sounds from 10 classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, enginge_idling, gun_shot, jackhammer, siren, and street_music.","10,034",🤷,Acoustic Classification,Paper,Link
Ben Hamner,NIPS Papers,"Dataset contains the title, authors, abstracts, and extracted text for all NIPS papers between 1987-2016.","~3,000",English,Text Corpora,Paper,Link
Veaux et al.,CSTR VCTK Corpus,"Dataset contains speech data uttered by 109 native speakers of English with various accents. Each speaker reads out about 400 sentences, most of which were selected from a newspaper plus the Rainbow Passage and an elicitation paragraph intended to identify the speaker's accent.",n/a,English,Text-to-Speech,Paper,Link
Agic et al.,JW300,Dataset is parallel corpus of over 300 languages with around 100 thousand parallel sentences per language pair on average.,105.11M,Multi-Lingual,Machine Translation,Paper,Link
Tiedemann et al.,Tanzil,Dataset is a collection of Quran translations in 42 languages.,1.01M,Multi-Lingual,Machine Translation,Paper,Link
Tatoeba,Tatoeba,Dataset is a collection of sentences and translations.,8.5M,Multi-Lingual,Machine Translation,Paper,Link
Craswell et al.,Open Resource for Click Analysis in Search (ORCAS),"ORCAS is a click-based dataset associated with the TREC Deep Learning Track. It covers 1.4 million of the TREC DL documents, providing 18 million connections to 10 million distinct queries.","10,405,342",English,Document Ranking,Paper,Link
Kumar et al.,ClarQ,Dataset consists of ∼2M question/post tuples distributed across 173 domains of stackexchange.,~2M,English,Clarification Question Generation,Paper,Link
Hannan et al.,ManyModalQA,"Dataset contains 10,190 questions, 2,873 images, 3,789 text, and 3,528 tables scraped from Wikipedia.","10,19",English,"Question Answering, Multi-Modal",Paper,Link
Gebhard et al.,Polusa,Dataset contains 0.9M articles covering policy topics published between Jan. 2017 and Aug. 2019 by 18 news outlets representing the political spectrum.,0.9M,English,Classification,Paper,Link
Chakravarthi et al.,MalayalamMixSentiment,"Dataset contains 6,739 comments and 7,743 distinct sentences. There are 5 classes: Positive, Negative, Mixed feelings, Neutral, and Non-Malayalam. Requires to email author for dataset download.","6,739",Malayalam,Sentiment Analysis,Paper,Link
Li et al.,DocBank,"Dataset contains fine-grained token-level annotations for document layout analysis. It includes 5,053 documents and both the validation set and the test set include 100 documents.","5,053",English,Document Layout Analysis,Paper,Link
Jaidka et al.,Get it #OffMyChest,Dataset is used for affective understanding of conversations focusing on the problem of how speakers use emotions to react to a situation and to each other. Posts were taken from the 2018 top reddit posts from /r/CasualConversations and /r/OffMyChest.,"437,86",English,Dialogue,Paper,Link
Wei et al.,AirDialogue,"Dataset contains 402,038 goal-oriented conversations.","402,038",English,Dialogue,Paper,Link
Wichern et al.,WSJ0 Hipster Ambient Mixtures (WHAM!),"Dataset consists of two speaker mixtures from the wsj0-2mix dataset combined with real ambient noise samples. The samples were collected in coffee shops, restaurants, and bars in the San Francisco Bay Area.",81 Hours,English,Speech Seperation,Paper,Link
Cao et al.,Crema-D,"Dataset consists of facial and vocal emotional expressions in sentences spoken in a range of basic emotional states (happy, sad, anger, fear, disgust, and neutral). 7,442 clips of 91 actors with diverse ethnic backgrounds were collected.","7,438",English,"Emotion Recognition, Multi-Modal",Paper,Link
Gillick et al.,Groove MIDI Dataset (GMD),"Dataset is composed of 13.6 hours of aligned MIDI and (synthesized) audio of human-performed, tempo-aligned expressive drumming.",13.6 Hours,🤷,Audio Generation,Paper,Link
Zen et al.,LibriTTS,Dataset is a multi-speaker English corpus of approximately 585 hours of read English speech at 24kHz sampling rate.,585 Hours,English,Text-to-Speech,Paper,Link
Keith Ito,Ljspeech,"Dataset consisting of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. A transcription is provided for each clip. Clips vary in length from 1 to 10 seconds and have a total length of approximately 24 hours.",~24 Hours,English,Speech Corpora,Paper,Link
Engel et al.,NSynth Dataset,"Dataset contains ~300K musical notes, each with a unique pitch, timbre, and envelope.",n/a,🤷,Audio Synthesis,Paper,Link
Vlasenko et al.,Surrey Audio-Visual Expressed Emotion (SAVEE),"Dataset consists of recordings from 4 male actors in 7 different emotions, 480 British English utterances in total. The sentences were chosen from the standard TIMIT corpus and phonetically-balanced for each emotion.",480,English,"Emotion Recognition, Audio",Paper,Link
VoxForge,VoxForge,Dataset consisting of speech audio clips submitted by the community involving several different languages. Dataset is constantly updated.,n/a,Multi-Lingual,Speech Corpora,Paper,Link
Jigsaw/Conversation AI,Civil Comments,Dataset contains the archive of the Civil Comments platform. Dataset was annotated for toxicity.,n/a,English,Classification,Paper,Link
Rajani et al.,Common Sense Explanations (CoS-E),Dataset used to train language models to automatically generate explanations that can be used during training and inference in a novel Commonsense Auto-Generated Explanation (CAGE) framework.,"19,522",English,Commonsense,Paper,Link
Camburu et al.,e-SNLI,Dataset contains human-annotated natural language explanations of the entailment relations.,n/a,English,Natural Language Inference (NLI),Paper,Link
Chelba et al.,1 Billion Word Language Model Benchmark (lm1b),Dataset used for measuring progress in statistical language modeling.,1.1B,English,Language Modeling,Paper,Link
Saxton et al.,Math Dataset,"Dataset contains mathematical question and answer pairs, from a range of question types at roughly school-level difficulty.",n/a,English,Mathematical Reasoning,Paper,Link
Cohan et al.,SciCite,Dataset used for classifying citation intents in academic papers. The main citation intent label for each JSON object is specified with the label key while the citation context is specified in with a context key.,"11,02",English,Classification,Paper,Link
George Miller,WordNet,"Dataset is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations.",n/a,English,Knowledge Base,Paper,Link
Zhang et al.,Yelp Polarity Reviews,"Dataset contains 1,569,264 samples from the Yelp Dataset Challenge 2015. This subset has 280,000 training samples and 19,000 test samples in each polarity. Dataset from FastAI's website.","1,569,264",English,Sentiment Analysis,Paper,Link
Translatefx,"Hong Kong Stock Exchange, the Securities and Futures Commission of Hong Kong","Dataset contains aligned sentence pairs from bilingual texts, covering the financial and legal domains in Hong Kong. The sources include government legislations and regulations, stock exchange announcements, financial offering documents, regulatory filings, regulatory guidelines, corporate constitutional documents and others.","350,000+","Chinese, English",Text Corpora,Paper,Link
Banda et al.,COVID-19 Twitter Chatter Dataset,"Dataset contains over 152 million tweets, growing daily, related to COVID-19 chatter generated from January 1st, 2020 to present.",152M+,Multi-Lingual,Text Corpora,Paper,Link
Saha et al.,MMD,Dataset contains over 150K conversation sessions between shoppers and sales agents.,"150,000+",English,Dialogue,Paper,Link
Hu et al.,ParaBank,Dataset contains paraphrases with 79.5 million references and on average 4 paraphrases per reference.,79.5M references,English,Semantic Textual Similarity,Paper,Link
Hossain et al.,Humicroedit,"Dataset contains 15,095 edited news headlines and their numerically assessed humor.","15,095",English,Classification,Paper,Link
Selvaraju et al.,VQA-Introspect,Dataset consists of 238K new perception questions from the VQA dataset which serve as sub questions corresponding to the set of perceptual tasks needed to answer complex reasoning questions.,238,English,"Question Answering, Visual",Paper,Link
Vries et al.,Talk the Walk,Dataset consists of over 10k crowd-sourced dialogues in which two human annotators collaborate to navigate to target locations in the virtual streets of NYC.,"10,000+",English,"Dialogue, Grounded Language Learning",Paper,Link
Toutanova et al.,FB15K-237 Knowledge Base Completion Dataset,Dataset contains knowledge base relation triples and textual mentions of Freebase entity pairs.,"237 relations, 14,451 entities",English,Relation Prediction,Paper,Link
Dettmers,WN18RR,Dataset contains knowledge base relation triples from WordNet.,"11 relations, 40,943 entities",English,Relation Prediction,Paper,Link
Min et al.,AmbigNQ,"Dataset covering 14,042 questions from NQ-open, an existing open-domain QA benchmark.","14,042",English,"Question Answering, Reading Comprehension",Paper,Link
Xiong et al,OpenKeyPhrase (OpenKP),"Open domain keyphrase extraction dataset containing 148,124 real world web documents along with a human annotation indicating the 1-3 most relevant keyphrases.","148,124",English,"Question Answering, Reading Comprehension",Paper,Link
Plummer et al.,Flickr30K Entities,"Dataset contains 244k coreference chains and 276k manually annotated bounding boxes for each of the 31,783 images and 158,915 English captions (five per image) in the original dataset.","31,783",English,Automatic Image Captioning,Paper,Link
Wang et al.,Street View Text (SVT),Dataset contains images with textual content used for scene text recognition.,n/a,English,"Multi-Modal Learning, Scene Text Recognition",Paper,Link
Lebret et al.,WikiBio,"Dataset contains 728,321 biographies from wikipedia. For each article, it provides the first paragraph and the infobox (both tokenized).","728,321",English,Text Generation,Paper,Link
Wiseman et al.,Rotowire and SBNation Datasets,Dataset consists of (human-written) NBA basketball game summaries aligned with their corresponding box and line scores.,"~15,000",English,Data-to-Text,Paper,Link
Novikova et al.,E2E,Dataset contains 50k combinations of a dialogue-act-based meaning representation and 8.1 references on average in the restaurant domain.,50,English,Text Generation,Paper,Link
Chen et al.,LogicNLG,Dataset is a table-based factchecking dataset with rich logical inferences in the annotated statements.,37,English,Data-to-Text,Paper,Link
Shu et al.,FakeNewsNet,"Repo contains two datasets with news content, social context, and spatiotemporal information from Politifact and Gossipcop.",n/a,English,"Classification, Fake News Detection",Paper,Link
Wang et al.,LIAR Dataset,"Dataset contains 12.8K manually labeled short statements in various contexts from POLITIFACT.COM, which provides detailed analysis report and links to source documents for each case.","12,8",English,"Classification, Fake News Detection",Paper,Link
Sun et al.,Dialogue-Based Reading Comprehension Examination (DREAM),"Dataset contains 10,197 multiple choice questions for 6,444 dialogues, collected from English-as-a-foreign-language examinations designed by human experts. DREAM is likely to present significant challenges for existing reading comprehension systems: 84% of answers are non-extractive, 85% of questions require reasoning beyond a single sentence, and 34% of questions also involve commonsense knowledge.","6,444",English,"Question Answering, Reading Comprehension, Dialogue",Paper,Link
Sandhaus et al.,The New York Times Annotated Corpus,"Dataset contains over 1.8 million articles written and published by the New York Times between January 1, 1987 and June 19, 2007 with article metadata provided by the New York Times Newsroom.",1.8M,English,"Summarization, Information Extraction",Paper,Link
Sharma et al.,BigPatent,Dataset consists of 1.3 million records of U.S. patent documents along with human written abstractive summaries.,1.3M,English,Summarization,Paper,Link
Hu et al.,LCSTS,Dataset constructed from the Chinese microblogging website Sina Weibo. It consists of over 2 million real Chinese short texts with short summaries given by the author of each text. Requires application.,2M+,Chinese,Summarization,Paper,Link
El-Haj,Essex Arabic Summaries Corpus (EASC),Dataset contains 153 Arabic articles and 765 human-generated extractive summaries of those articles. These summaries were generated using Mechanical Turk.,153,Arabic,Summarization,Paper,Link
El-Haj et al.,KALIMAT Multipurpose Arabic Corpus,"Dataset contains 20,291 Arabic articles collected from the Omani newspaper Alwatan. Extractive Single-document and multi-document system summaries. Named Entity Recognised articles. The data has 6 categories: culture, economy, local-news, international-news, religion, and sports.","20,291",Arabic,"Summarization, Named Entity Recognition (NER), Part-of-Speech (POS)",Paper,Link
Khan et al.,Libri-Light,"Dataset contains 60K hours of unlabelled speech from audiobooks in English and a small labelled data set (10h, 1h, and 10 min).","60,000 Hours",English,Speech Recognition,Paper,Link
Sap et al.,Atlas of Machine Commonsense (ATOMIC),Dataset is a knowledge graph of 877K textual description triples of inferential knowledge.,877,English,"Commonsense, Knowledge Graph",Paper,Link
Speer et al.,ConceptNet,"A knowledge graph that connects words and phrases of natural language (terms) with labeled, weighted edges (assertions).",21M+ edges and 8M+ nodes,Multi-Lingual,"Commonsense, Knowledge Graph",Paper,Link
Kim et al.,Genia,"Dataset contains 1,999 Medline abstracts, selected using a PubMed query for the three MeSH terms ""human"", ""blood cells"", and ""transcription factors"". The corpus has been annotated for part-of-speech, contituency syntactic, terms, events, relations, and coreference.","1,999",English,"Part of Speech (POS), Constituency, Coreference, Event, Relation",Paper,Link
Ohta et al.,DNA Methylation Corpus,"Dataset contains 200 abstracts including a representative sample of all PubMed citations relevant to DNA methylation, and introduce manual annotation for nearly 3,000 gene/protein mentions and 1,500 DNA methylation and demethylation events.",200,English,"Information Extraction, Entity Extraction, Event Extraction",Paper,Link
Pyysalo et al.,Exhaustive PTM Corpus,"Dataset contains 360 abstracts manually annotated in the BioNLP Shared Task event representation for over 4,500 mentions of proteins and 1,000 statements of modification events of nearly 40 different types.",360,English,"Information Extraction, Event Extraction",Paper,Link
Ohta et al.,mTOR Pathway Corpus,"Dataset contains 1,300 annotated event instances of protein associations and dissociation reactions.","1,3",English,"Information Extraction, Entity Extraction, Event Extraction",Paper,Link
Ohta et al.,PTM Event Corpus,"Dataset contains 157 PubMed abstracts annotated for over 1,000 proteins and 400 post-translational modification events identifying the modified proteins and sites.",157,English,"Information Extraction, Event Extraction",Paper,Link
Pyysalo et al.,T4SS Event Corpus,"Dataset contains 27 full text publications totaling 15,143 pseudo-sentences (text sentences plus table rows, references, etc.) and 244,942 tokens covering 4 classes: Bacteria, Cellular components, Biological Processes, and Molecular functions.",27,English,"Information Extraction, Event Extraction",Paper,Link
Knight et al.,Abstract Meaning Respresentation (AMR) Bank,"Dataset contains a sembank (semantic treebank) of over 59,255 English natural language sentences from broadcast conversations, newswire, weblogs, web discussion forums, fiction and web text.","59,255",English,"Information Extraction, Semantic Role Labeling",Paper,Link
Mota et al.,HAREM,Dataset used for Named-Entity Recognition (NER) in Portuguese.,n/a,Portuguese,Named Entity Recognition (NER),Paper,Link
Nie et al.,Adversarial NLI (ANLI),Dataset is an NLI benchmark created via human-and-model-in-the-loop enabled training (HAMLET). Human was tasked to provide a hypothesis that fools the model into misclassifying the label.,"169,265",English,Natural Language Inference (NLI),Paper,Link
Cachola et al.,SCITLDR,Dataset of a combination of TLDRs written by human experts and author written TLDRs of computer science papers from OpenReview.,"3,9",English,Summarization,Paper,Link
Khodak et al.,Self-Annotated Reddit Corpus (SARC),"Dataset contains 1.3 million sarcastic comments from the Internet commentary website Reddit. It contains statements, along with their responses as well as many non-sarcastic comments from the same source.",1.3M,English,"Text Corpora, Sarcasm Detection",Paper,Link
Thorne et al.,Fact Extraction and Verfication (FEVER),"Dataset contains 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as supported, rufted or notenoughinfo.","185,445",English,"Classification, Fake News Detection",Paper,Link
Saha et al.,DuoRC,"Dataset contains 186,089 unique question-answer pairs created from a collection of 7,680 pairs of movie plots where each pair in the collection reflects two versions of the same movie.","186,089",English,Paraphrasing Identification,Paper,Link
Annamoradnejad et al.,ColBERT,"Dataset contains 200k short texts (100k positive, 100k negative). Used for humor detection.",200,English,"Classification, Humor Detection",Paper,Link
Wieting et al.,PARANMT-50M,Dataset containing more than 50 million English-English sentential paraphrase pairs.,50M,English,Paraphrasing Generation,Paper,Link
Ruoho Ruotsi,Yoruba Text,Multiple datasets scraped together for the Yoruba language.,n/a,Yoruba,"Text Corpora, Machine Translation",Paper,Link
Ruoho Ruotsi,Igbo Text,Dataset is a parallel dataset for the Urhobo language.,10.3M,"Igbo, English","Text Corpora, Machine Translation",Paper,Link
Ruoho Ruotsi,Urhobo Text,Dataset is a parallel dataset containing 10.3M tokens.,n/a,"Urhobo, English","Text Corpora, Machine Translation",Paper,Link
Chen et al.,Logic2Text,"Dataset contains 5,600 tables and 10,753 descriptions involving common logic types paired with the underlying logical forms.","10,753",English,Data-to-Text,Paper,Link
Berzak et al.,OneStopQA,"Dataset comprises 30 articles from the Guardian in 3 parallel text difficulty versions and contains 1,458 paragraph-question pairs with multiple choice questions, along with manual span markings for both correct and incorrect answers.",30,English,"Question Answering, Reading Comprehension",Paper,Link
Scialom et al.,MLSUM,"Dataset was collected from online newspapers, it contains 1.5M+ article/summary pairs in 5 languages: French, German, Spanish, Russian, & Turkish.",1.5M+,Multi-Lingual,Summarization,Paper,Link
Alamri et al.,Audio Visual Scene-Aware Dialog (AVSD),Dataset consists of text-based human conversations about short videos from the Charades dataset.,"11,816",English,"Multi-Modal Learning, Video Question Answering, Dialogue",Paper,Link
Ponti et al.,Cross-lingual Choice of Plausible Alternatives (XCOPA),"Dataset is the translation and reannotation of the English COPA and covers 11 languages: Estonian, Haitian Creole, Indonesian, Italian, Quechua, Swahili, Tamil, Thai, Turkish, Vietnamese & Mandarin Chinese. The dataset requires both the command of world knowledge and the ability to generalise to new languages.",n/a,Multi-Lingual,Commonsense Reasoning,Paper,Link
Conforti et al.,Will-They-Won't-They (WT-WT),Dataset of English tweets targeted at stance detection for the rumor verification task.,"51,284",English,Stance Detection,Paper,Link
Jain et al.,SciREX,"Dataset is fully annotated with entities, their mentions, their coreferences, and their document level relations.",438,English,Information Extraction,Paper,Link
Demszky et al.,GoEmotions,"Dataset contains 58K carefully curated Reddit comments labeled for 27 emotion categories: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, & surprise.",58,English,"Classification, Emotion Recognition",Paper,Link
Przepiorkowski,NKJP-NER,Dataset contains extracted sentences with named entities of exactly one type. The task is to predict the type of the named entity.,20,Polish,Named Entity Recognition (NER),Paper,Link
Wroblewska et al.,Compositional Distributional Semantics Corpus (CDSC | E & R),Dataset is s human-annotated for semantic relatedness and entailment by 3 human judges experienced in Polish linguistics.,10,Polish,Natural Language Inference (NLI),Paper,Link
Ptaszynski et al.,Cyberbullying Detection (CBD),Dataset contains annotated tweets that identify harmful or non-harmful content.,n/a,Polish,Classification,Paper,Link
Kocon et al.,PolEmo2.0-IN & OUT,Dataset contains online reviews from medicine and hotels domains. The task is to predict the sentiment of a review.,"8,216",Polish,Sentiment Analysis,Paper,Link
Marcinczuk et al.,Did You Know (DYK),"Dataset contains of 4,721 question–answer pairs obtained from Czy wiesz (Do you know) Wikipedia project.","4,721",Polish,Question Answering,Paper,Link
Ogrodniczuk et al.,Polish Summaries Corpus (PSC),Dataset contains news articles and their summaries.,723,Polish,Summarization,Paper,Link
Kang et al.,Post-Modifier Dataset (PoMo),"Dataset for developing post-modifier generation systems. It's a collection of sentences that contain entity post-modifiers, along with a collection of facts about the entities obtained from Wikidata.","231,057",English,Post-Modifier Generation,Paper,Link
Campos et al.,DoQa,"Dataset contains domain specific FAQs via conversational QA that contains 2,437 information-seeking question/answer dialogues (10,917 questions in total) on three different domains: cooking, travel and movies.","10,917",English,"Question Answering, Dialogue",Paper,Link
Cruz et al.,WikiText-TL-39,"Dataset is a large scale, unlabeled text dataset with 39M tokens in the training set.",n/a,Filipino,"Text Corpora, Language Modeling",Paper,Link
Cabasag et al.,Hate Speech Dataset,Dataset contains tweets that are labeled as hate speech or non-hate speech. Collected during the 2016 Philippine Presidential Elections.,"18,464",Filipino,Classification,Paper,Link
Livelo et al.,Dengue Dataset,"Dataset for multi-class (5) classification on tweets: 5 classes: absent, dengue, health, mosquito & sick.","5,015",Filipino,Classification,Paper,Link
Eisenberg et al.,Personal Events in Dialogue Corpus,"Dataset is a corpus containing annotated dialogue transcripts from fourteen episodes of the podcast This American Life. It contains 1,038 utterances, made up of 16,962 tokens, of which 3,664 represent events.","1,038",English,"Information Extraction, Event Extraction, Dialogue",Paper,Link
Fu et al.,Quda,"Dataset contains 14,035 diverse user queries annotated with 10 low-level analytic tasks that assist in the deployment of state-of-the-art machine/deep learning techniques for parsing complex human language.","14,035",English,"Information Extraction, Visualization",Paper,Link
Kyubyong Park,Korean Single Speaker Dataset (KSS),Dataset consists of audio files recorded by a professional female voice actress and their aligned text extracted from books.,"12,853",Korean,Text-to-Speech,Paper,Link
Choi et al.,DramaQA,"Dataset contains 16,191 question answer pairs from 23,928 various length video clips, with each question answer pair belonging to one of four difficulty levels.","23,928",English,"Question Answering, Visual",Paper,Link
Holzenberger et al.,Statutory Reasoning Assessment (SARA),"Dataset contains a set of rules extracted from the statutes of the US Internal Revenue Code (IRC), together with a set of natural language questions which may only be answered correctly by referring to the rules.",100,English,Text Corpora,Paper,Link
Gupta et al.,InfoTabs,Dataset contains human-written textual hypotheses based on premises that are tables extracted from Wikipedia info-boxes.,"2,54",English,Natural Language Inference (NLI),Paper,Link
Pampari et al.,emrQA,"Dataset contains 1M question-logical form and 400,000+ question answer evidence pairs on electronic medical records. In total, there are 2,495 clinical notes.","2,495",English,"Question Answering, Reading Comprehension",Paper,Link
Asgari-Bidhendi et al.,Perlex,Dataset is an expert translated version of the Semeval-2010-Task-8 dataset.,"10,717",Persian,Relation Extraction,Paper,Link
Mitra et al.,Credbank,"Dataset comprises more than 60M tweets grouped into 1,049 real-world events, each annotated by 30 human annotators.",60M,English,Credibility,Paper,Link
Santia et al.,BuzzFace,"Dataset focused on news stories (which are annotated for veracity) posted to Facebook during September 2016 consisting of: Nearly 1.7 million Facebook comments discussing the news content, Facebook plugin comments, Disqus plugin comments, Associated webpage content of the news articles.","2,263",English,"Classification, Fake News Detection",Paper,Link
Tacchini et al.,Some Like it Hoax,"Dataset contains 15,500 posts from 32 pages (14 conspiracy and 18 scientific).","15,5",English,"Classification, Fake News Detection",Paper,Link
El-Haj et al.,Arabic in Business and Management Corpora (ABMC),"Dataset contains 400 Arab companies chairman and chief executive manager statements, 400 Arabic economic news articles, 400 Arabic stock market news articles.","1,2",Arabic,Text Corpora,Paper,Link
Kruiper et al.,Focused Open Biology Information Extraction (FOBIE),"Dataset contains 1,500 manually-annotated sentences that express domain-independent relations between central concepts in a scientific biology text, such as trade-offs and correlations.","1,5",English,Relation Extraction,Paper,Link
Gardent et al.,WebNLG (Enriched),"Dataset consists of 25,298 (data,text) pairs and 9,674 distinct data units. The data units are sets of RDF triples extracted from DBPedia and the texts are sequences of one or more sentences verbalising these data units.","25,298","German, English",Data-To-Text Generation,Paper,Link
Jiang et al.,FreebaseQA,"Dataset contains 28,348 unique questions for open domain QA over the Freebase knowledge graph.","28,348",English,"Question Answering, Knowledge Graph",Paper,Link
Spala et al.,Deft,"Dataset contains annotated content from two different data sources: 1) 2,443 sentences from various 2017 SEC contract filings from the publicly available US Securities and Exchange Commission EDGAR (SEC) database, and 2) 21,303 sentences from open source textbooks including topics in biology, history, physics, psychology, economics, sociology, and government.","23,746",English,"Information Extraction, Definition Extraction",Paper,Link
Issa Annamoradnejad,Clash of Clans,"Dataset contains 50K user comments, both from the iTunes App Store and Google Play. The dataset spans from Oct 18, 2018 to Feb 1, 2019.",50,English,Sentiment Analysis,Paper,Link
Kummerfeld et al.,IRC Disentanglement,"Dataset contains 77,563 messages of internet relay chat (IRC). Almost all are from the Ubuntu IRC Logs.","77,563",English,Dialogue,Paper,Link
Maciej Ogrodniczuk,Polish Parliamentary Corpus (PPC),"Dataset is a collection of linguistically analysed documents from the proceedings of Polish Parliament, Sejm and Senate. It is based on the Polish Sejm Corpus.","3,000+",Polish,Text Corpora,Paper,Link
ParaCrawl Project,ParaCrawl Corpus,Multiple parallel datasets of European languages for machine translation.,n/a,Multi-Lingual,Machine Translation,Paper,Link
Shridhar et al.,Action Learning From Realistic Environments and Directives (ALFRED),"Dataset contains 8k+ expert demostrations with 3 or more language annotations each comprising of 25,000 language directives. A trajectory consists of a sequence of expert actions, the corresponding image observations, and language annotations describing segments of the trajectory.","8,055",English,Multi-Modal Learning,Paper,Link
Huang et al.,Visual Storytelling Dataset (VIST),"Dataset contains 81,743 unique photos in 20,211 sequences, aligned to descriptive and story language. VIST is previously known as ""SIND"", the Sequential Image Narrative Dataset (SIND).","81,743",English,Multi-Modal Learning,Paper,Link
Andrew Thompson,All the News 2.0,"Dataset contains 2.7 million articles from 26 different publications from January 2016 to April 1, 2020.",2.7M,English,Text Corpora,Paper,Link
various,Datasets Knowledge Embedding,Several datasets containing edges and nodes for knowledge base building.,n/a,English,Embeddings,Paper,Link
Alam et al.,BARD Bangla Article Classifier,"A large corpus of Bangla documents classified into 5 classes: sports, state, economy, entertainment, and international.","376,226",Bengali,Classification,Paper,Link
Asri et al.,Frames,"Dataset contains 1,369 human-human dialogues with an average of 15 turns per dialogue. This corpus contains goal-oriented dialogues between users who are given some constraints to book a trip and assistants who search a database to find appropriate trips.","1,369",English,Dialogue,Paper,Link
Goldhahn et al.,Leipzig Corpora Collection,Dataset containing 252 languages of web crawled news corpora.,n/a,Multi-Lingual,Text Corpora,Paper,Link
Jurgens et al.,SemEval-2014 Task 3,"Dataset is used for cross-level semantic similarity which measures the degree to which the meaning of a larger linguistic item, such as a paragraph, is captured by a smaller item, such as a sentence.",2,English,Semantic Textual Similarity,Paper,Link
Hossain et al.,BanFakeNews,A Dataset for detecting fake news in Bangla. News articles were scraped from news portals in Bengladesh.,n/a,Bengali,"Classification, Fake News Detection",Paper,Link
Zampeiri et al.,SemEval-2019 Task 6,Dataset containing tweets as either offensive or not offensive (Sub-task A) and further classifies offensive tweets into categories (Sub-tasks B – C).,"14,1",English,Classification,Paper,Link
Derczynski et al.,WNUT 2017,"Dataset containing tweets, reddit comments, YouTube comments, and StackExchange were annotated with 6 entities: Person, Location, Corporation, Consumer good, Creative work, and Group.","2,295",English,Named Entity Recognition (NER),Paper,Link
Deng et al.,MPQA Opinion Corpus,"Dataset contains news articles and other text documents manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations, etc.).",70,English,Sentiment Analysis,Paper,Link
Joachims,Ohsumed Dataset,"Dataset containing references from MEDLINE, the on-line medical information database, consisting of titles and/or abstracts from 270 medical journals over a five-year period (1987-1991).",n/a,English,Classification,Paper,Link
Jurczyk et al.,SelQA,"Dataset provides crowdsourced annotation for two selection-based question answer tasks, answer sentence selection and answer triggering. Our dataset composes about 8K factoid questions for the top-10 most prevalent topics among Wikipedia articles.",8,English,"Question Answering, Reading Comprehension",Paper,Link
Mencía et al.,The EUR-Lex Dataset,"Dataset is a collection of documents about European Union law. It contains many different types of documents, including treaties, legislation, case-law and legislative proposals, which are indexed with almost 4,000 labels.",n/a,Multi-Lingual,Classification,Paper,Link
Dernoncourt et al.,PubMed 200k RCT Dataset,"Dataset is based on PubMed for sequential sentence classification. The dataset consists of approximately 200,000 abstracts of randomized controlled trials, totaling 2.3 million sentences.",200,English,Classification,Paper,Link
Xu et al.,MATINF,"A labeled dataset for classification, question answering and summarization. MATINF contains 1.07 million question-answer pairs with human-labeled categories and usergenerated question descriptions.",1.07M,Chinese,"Classification, Question Answering, Summarization",Paper,Link
Sabir et al.,Textual Visual Semantic Dataset,"A dataset consisting of detecting and recognizing text appearing in images (e.g. signboards, traffic signals or brands in clothing or objects). Around 82,000 images.",82,English,Automatic Image Captioning,Paper,Link
Csaky et al.,Gutenberg Dialogue,"A dataset created by extracting dialogue from the Gutenberg book collection, comprising of ~60,000 books. Currently it supports English, German, Dutch, Spanish, Portuguese, Italian, and Hungarian.","59,971",Multi-Lingual,Dialogue,Paper,Link
Souza Costa et al.,EventQA,"A dataset for answering Event-Centric questions over Knowledge Graphs (KGs). It contains 1,000 semantic queries and the corresponding verbalisations.",1,English,"Question Answering, Knowledge Base",Paper,Link
Iyyer et al.,Sequential Question Answering (SQA),"Dataset was created to explore the task of answering sequences of inter-related questions on HTML tables. It has 6,066 sequences with 17,553 questions in total.","17,553",English,"Question Answering, Semantic Parsing",Paper,Link
Pasupat et al.,WikiTablesQuestions,Dataset is for the task of question answering on semi-structured HTML tables.,"22,033",English,"Question Answering, Semantic Parsing",Paper,Link
Yao et al.,DocRed,Dataset was constructed from Wikipedia and Wikidata. It annotates both named entities and relations.,"107,05",English,Relation Extraction,Paper,Link
Saha et al.,Complex Sequential Question Answering (CSQA),"Dataset contains around 200K dialogs with a total of 1.6M turns. Further, unlike existing large scale QA datasets which contain simple questions that can be answered from a single tuple, the questions in the dialogs require a larger subgraph of the KG.",200,English,"Question Answering, Knowledge Base",Paper,Link
Logan et al.,Linked WikiText-2,"Dataset contains over 2 million tokens from Wikipedia articles, along with annotations linking mentions to their corresponding entities and relations in Wikidata.",2M,English,Knowledge Graph,Paper,Link
Moon et al.,OpenDialKG,Dataset of conversations between two crowdsourcing agents engaging in a dialog about a given topic. Each dialog turn is paired with its corresponding “KG paths” that weave together the KG entities and relations that are mentioned in the dialog.,15,English,"Dialogue, Knowledge Graph",Paper,Link
muvvasandeep,BuGL,"Dataset consists of 54 GitHub projects of four different programming languages namely C, C++, Java and Python with around 10,187 issues.","10,187",English,Text Corpora,Paper,Link
Shen et al.,HJDataset,"Dataset contains over 250,000 layout element annotations of seven types in Japanese documents.","250,000+",Japanese,Text Corpora,Paper,Link
Chen et al.,HybridQA,"Dataset contains over 70K question-answer pairs based on 13,000 tables, each table is in average linked to 44 passages.",70,English,"Question Answering, Knowledge Base",Paper,Link
Hipson et al.,PoKi,"Dataset is a corpus of 61,330 poems written by children from grades 1 to 12.","61,33",English,Text Corpora,Paper,Link
Cui et al.,MuTual,"Retrieval-based dataset for multi-turn dialogue reasoning, which is modified from Chinese high school English listening comprehension test data.","8,86",English,Dialogue,Paper,Link
Zhou et al.,KdConv,"Dataset is a Chinese multi-domain dataset, grounding the topics in multi-turn conversations to knowledge graphs. KdConv contains 4.5K conversations from three domains (film, music, and travel), and 86K utterances with an average turn number of 19.0.","4,5",Chinese,"Dialogue, Knowledge Graph",Paper,Link
Ham et al.,KorNLI,Dataset used for natural language inference for the Korean language.,"950,354",Korean,Natural Language Inference (NLI),Paper,Link
Ham et al.,KorSTS,Dataset used for semantic textual similarity for the Korean language.,"8,628",Korean,Semantic Textual Similarity,Paper,Link
Parikh et al.,ToTTo,"Dataset is used for the controlled generation of descriptions of tabular data comprising over 100,000 examples. Each example is a aligned pair of a highlighted table and the description of the highlighted content.","120,000+",English,Table-to-Text,Paper,Link
Budur et al.,Natural Language Inference in Turkish (NLI-TR),Datasets that were obtained by translating the SNLI and MNLI corpora into Turkish.,n/a,Turkish,Natural Language Inference (NLI),Paper,Link
Cui et al.,Chinese Machine Reading Comprehension (CMRC),"Dataset (cloze style) contains over 100K blanks (questions) within over 10K passages, which was originated from Chinese narrative stories.","10,438",Chinese,Reading Comprehension,Paper,Link
Liu et al.,VIdeO-and-Language INference (VIOLIN),"Dataset contains 95,322 video-hypothesis pairs from 15,887 video clips, spanning over 582 hours of video (YouTube and TV shows). Inference descriptions of video content were annotated. Inferences are used to measure entailment vs video clip.","15,887",English,Multi-Modal Learning,Paper,Link
Gruppi et al.,NELA-GT-2019,Dataset contains 1.12M news articles from 260 sources collected between January 1st 2019 and December 31st 2019. Included are source-level ground truth labels from 7 different assessment sites.,1.12M,English,"Text Corpora, Classification",Paper,Link
Zhu et al.,CrossWOZ,"Dataset is a cross-domain wizard-of-oz task-oriented dataset. It contains dialogue sessions and utterances for 5 domains: hotel, restaurant, attraction, metro, and taxi.",6,Chinese,Dialogue,Paper,Link
Yu et al.,ReClor,Dataset contains logical reasoning questions of standardized graduate admission examinations.,"6,138",English,Reading Comprehension,Paper,Link
Keysers et al.,Compositional Freebase Questions (CFQ),Dataset contains questions and answers that also provides for each question a corresponding SPARQL query against the Freebase knowledge base.,"239,357",English,"Question Answering, Knowledge Base",Paper,Link
Zhang et al.,MoviE Text Audio QA (MetaQA),"Dataset contains more than 400K questions for both single and multi-hop reasoning, and provides more realistic text and audio versions. MetaQA serves as a comprehensive extension of WikiMovies.","400,000+",English,"Question Answering, Knowledge Base",Paper,Link
Berant et al.,WebQuestions,"Dataset contains 6,642 question/answer pairs. The questions are supposed to be answerable by Freebase, a large knowledge graph. The questions are mostly centered around a single named entity.","6,642",English,"Question Answering, Knowledge Base",Paper,Link
Amini et al.,MathQA,Dataset contains English multiple-choice math word problems covering multiple math domain categories by modeling operation programs corresponding to word problems in the AQuA dataset.,37,English,"Question Answering, Reading Comprehension",Paper,Link
Schmitt et al.,SherLIiC,"Dataset contains manually annotated inference rule candidates (InfCands), accompanied by ~960k unlabeled InfCands, and ~190k typed textual relations between Freebase entities extracted from the large entity-linked corpus ClueWeb09.","~960,000",English,"Natural Language Inference (NLI), Lexical Inference/Entailment",Paper,Link
Bawden et al.,DiaBLa,"Parallel dataset of spontaneous, written, bilingual dialogues for the evaluation of Machine Translation, annotated for human judgments of translation quality.","5,700+","French, English","Machine Translation, Dialogue",Paper,Link
Castro et al.,Multimodal Sarcasm Detection Dataset (MUStARD),"The dataset, a multimodal video corpus, consists of audiovisual utterances annotated with sarcasm labels. Each utterance is accompanied by its context, which provides additional information on the scenario where the utterance occurs.","6,365",English,Multi-Modal Learning,Paper,Link
Poria et al.,Multimodal EmotionLines Dataset (MELD),"Dataset contains the same dialogue instances available in EmotionLines dataset, but it also encompasses audio and visual modality along with text. It has more than 1,400 dialogues and 13,000 utterances from Friends TV series. Each utterance in a dialogue has been labeled by any of these seven emotions: Anger, Disgust, Sadness, Joy, Neutral, Surprise and Fear. It also has sentiment (positive, negative and neutral) annotation for each utterance.","1,4",English,Multi-Modal Learning,Paper,Link
Simakis,Book Depository Dataset,"Dataset contains books from bookdepository.com, not the actual content of the book but a list of metadata like title, description, dimensions, category and others.",n/a,English,"Topic Modeling, Classification",Paper,Link
Alomari,Arabic Jordanian General Tweets (AJGT),"Dataset consists of 1,800 tweets annotated as positive and negative. Modern Standard Arabic (MSA) or Jordanian dialect.","1,8",Arabic,"Classification, Sentiment Analysis",Paper,Link
Suwaileh et al.,ArabicWeb16,"Dataset contains 150,211,934 Arabic Web pages with high coverage of dialectal Arabic as well as Modern Standard Arabic (MSA).",150M,Arabic,Text Corpora,Paper,Link
Suwaileh et al.,Content-Based Categorized Dataset,Dataset contains 996 Web pages from the ArabicWeb16 dataset were extracted and labeled.,996,Arabic,Text Classification,Paper,Link
Nabil et al.,ASTD: Arabic Sentiment Tweets Dataset,"Dataset contains over 10k Arabic sentiment tweets classified into 4 classes: subjective positive, subjective negative, subjective mixed, and objective.","10,000+",Arabic,"Classification, Sentiment Analysis",Paper,Link
El-khair et al.,1.5 billion Words Arabic Corpus,"The data were collected from newspaper articles in ten major news sources from eight Arabic countries, over a period of fourteen years.",5M,Arabic,Text Corpora,Paper,Link
Piskorski et al.,BSNLP-2019,"Dataset used to classify named entities in web documents in Slavic languages, their lemmatization, and cross-language matching. Dataset covers 4 languages: Bulgarian, Czech, Polish, and Russian.",n/a,Multi-Lingual,"Named Entity Recognition (NER), Entity Linking",Paper,Link
Allen Institute,COVID-19 Open Research Dataset (CORD-19),"Dataset contains 44,000 scholarly articles, including over 29,000 with full text, about COVID-19 and the coronavirus family of viruses for use by the global research community.",44,English,Text Corpora,Paper,Link
Habash et al.,The Arabic Parallel Gender Corpus,Dataset is designed to support research on gender bias in natural language processing applications working on Arabic. Requires to submit application for approval.,"~12,000",Arabic,Gender Identification,Paper,Link
Halabi,Arabic Speech Corpus,"Dataset was recorded in south Levantine Arabic (Damascian accent) using a professional studio. Synthesized speech as an output using this corpus has produced a high quality, natural voice.",n/a,Arabic,Speech Corpora,Paper,Link
Abbas et al.,Khaleej-2004 Corpus,"Dataset contains more than 5,000 articles which correspond to nearly 3 millions words across 4 topics: International News, Local News, Economy, and Sports.","5,69",Arabic,Text Corpora,Paper,Link
Abbas et al.,Watan-2004 Corpus,"Dataset contains about 20,000 articles talking about 6 topics: culture, religion, economy, local news, international news and sports.",20,Arabic,Text Corpora,Paper,Link
Abbas et al.,Parallel Arabic DIalectal Corpus (PADIC),Dataset is a multi-dialectal corpus - contains six dialects in addition to MSA in Buckwalter format.,"6,000+",Arabic,Text Corpora,Paper,Link
Prettenhofer et al.,Webis-CLS-10,"The Cross-Lingual Sentiment (CLS) dataset comprises about 800,000 Amazon product reviews in the 4 languages: English, German, French, and Japanese.",800,Multi-Lingual,"Classification, Sentiment Analysis",Paper,Link
Cui et al.,Chinese Machine Reading Comprehension (CMRC 2018),"Dataset is composed by near 20,000 real questions annotated on Wikipedia paragraphs by human experts.",20,Chinese,"Question Answering, Reading Comprehension",Paper,Link
Shao et al.,Delta Reading Comprehension Dataset,"Dataset organizes 10,014 paragraphs from 2,108 wiki entries and highlights more than 30,000 questions from the paragraphs.","10,014",Chinese,"Question Answering, Reading Comprehension",Paper,Link
Güngör & Sohrab et al.,Finnish News Corpus for Named Entity Recognition,"Dataset contains 953 articles (193,742 word tokens) with 6 named entity classes: organization, location, person, product, event, and date.",953,Finnish,Named Entity Recognition (NER),Paper,Link
Conneau et al.,The Cross-lingual Natural Language Inference corpus (XNLI),"Dataset contains collection of 5,000 test and 2,500 dev pairs for the MultiNLI corpus. The pairs are annotated with textual entailment and translated into 14 languages: French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi, Swahili and Urdu.","112,5",Multi-Lingual,Entailment,Paper,Link
Pan et al.,WikiAnn,"Dataset with NER annotations for PER, ORG and LOC. It has been constructed using the linked entities in Wikipedia pages for 282 different languages.","95,924",Multi-Lingual,Named Entity Recognition (NER),Paper,Link
Schneidermann,Danish-Similarity-Dataset,Dataset consists of 99 word pairs rated by 38 human judges according to their semantic similarity.,99,Danish,Semantic Textual Similarity,Paper,Link
Vamvas et al.,X-Stance,"Dataset contains more than 150 political questions, and 67k comments written by candidates on those questions. The questions are available in German, French, Italian and English.",67,Multi-Lingual,Stance Detection,Paper,Link
Nakov et al.,SemEval-2016 Task 4,Dataset contains 5 subtasks involving the sentiment analysis of tweets.,"~75,000",English,"Classification, Sentiment Analysis",Paper,Link
Boito et al.,Multilingual Corpus of Sentence-Aligned Spoken Utterances (MaSS),"Dataset of 8,130 parallel spoken utterances across 8 languages (56 language pairs). Languages: Basque, English, Finnish, French. Hungarian, Romanian, Russian, Spanish.","8,13",Multi-Lingual,Speech Corpora,Paper,Link
Elliott et al.,Multi30k,Dataset of images paired with sentences in English and German. This dataset extends the Flickr30K dataset.,"31,014","German, English","Machine Translation, Multi-Modal Learning",Paper,Link
Giannakopoulos et al.,MultiLing Pilot 2011 Dataset,"Dataset is derived from publicly available WikiNews English texts and translated into 7 languages: Arabic, Czech, English, French, Greek, Hebrew, Hindi.",n/a,Multi-Lingual,Summarization,Paper,Link
Lin et al.,CommonGen,Dataset consists of 30k concept-sets with humanwritten sentences as references.,30,English,Text Generation,Paper,Link
Pryzant et al.,Neutralizing Biased Text,"A parallel corpus of 180,000+ sentence pairs where one sentence is biased and the other is neutralized. The data were obtained from debiasing wikipedia edits.",180,English,Biased Text Neutralization,Paper,Link
Parth Parikh,Wikipedia News Corpus,Text from Wikipedia's current events page with dates.,"~25,000",English,Text Corpora,Paper,Link
Lapshinova-Koltunski et al.,ParCorFull,A parallel corpus annotated for the task of translation of corefrence across languages.,"14,927","German, English","Machine Translation, Coreference Resolution",Paper,Link
Byrne et al.,Taskmaster-2,"Dataset consists of 17,289 dialogs in seven domains: restaurants (3276), food ordering (1050), movies (3047), hotels (2355), flights (2481), music (1602), and sports (3478). It consists entirely of spoken two-person dialogs.","17,289",English,Dialogue,Paper,Link
Parida et al.,WAT 2019 Hindi-English,"Dataset consists of multimodal English-to-Hindi translation. It inputs an image, rectangular region in the image and english caption. It outputs a caption in Hindi.","32,925","Hindi, English","Machine Translation, Multi-Modal Learning",Paper,Link
Yuhao et al.,The TAC Relation Extraction Dataset (TACRED),A relation extraction dataset containing 106k+ examples covering 42 TAC KBP relation types. Costs $25 for non-members.,"106,264",English,Relation Extraction,Paper,Link
Volske et al.,Webis-TLDR-17 Corpus,Dataset contains 3 Million pairs of content and self-written summaries mined from Reddit. It is one of the first large-scale summarization dataset from the social media domain.,"3,084,410",English,Summarization,Paper,Link
Chen et al.,Webis-Snippet-20 Corpus,"Dataset comprises four abstractive snippet dataset from ClueWeb09, Clueweb12, and DMOZ descriptions. More than 10 million <webpage, abstractive snippet> pairs / 3.5 million <query, webpage, abstractive snippet> pairs were collected.",3.5M,English,Summarization,Paper,Link
Pasini et al.,Train-O-Matic Large,Automatically-generated corpora in multiple languages with sense annotations for nouns using WordNet for English and BabelNet for all other languages as inventories of senses.,10M+,Multi-Lingual,Word Sense Disambiguation,Paper,Link
Pasini et al.,Train-O-Matic Small,Automatically-generated corpora in multiple languages with sense annotations for nouns using WordNet for English and BabelNet for all other languages as inventories of senses.,1M+,Multi-Lingual,Word Sense Disambiguation,Paper,Link
Scarlini et al.,OneSeC Small,Automatically-generated corpora in multiple languages with sense annotations for nouns using WordNet for English and BabelNet for all other languages as inventories of senses.,1M+,Multi-Lingual,Word Sense Disambiguation,Paper,Link
Raganato et al.,WSD English All-Words Fine-Grained Datasets,Unified five standard all-words Word Sense Disambiguation datasets.,"7,000+",English,Word Sense Disambiguation,Paper,Link
Curation Corporation,Curation Corpus,"Dataset is a collection of 40,000 professionally-written summaries of news articles, with links to the articles themselves.",40,English,Text Corpora,Paper,Link
Di Gangi et al.,MuST-C,"Dataset is a speech translation corpus containing 385 hours from Ted talks for speech translation from English into several languages: Dutch, French, German, Italian, Portuguese, Romanian, Russian, & Spanish. Requires filling request form.",385 Hours,Multi-Lingual,Speech Translation,Paper,Link
Bouscarrat et al.,CASS,Dataset is composed of decisions made by the French Court of cassation and summaries of these decisions made by lawyer.,"129,445",French,Summarization,Paper,Link
Sanabria et al.,How2,"Dataset of instructional videos covering a wide variety of topics across video clips (about 2,000 hours), with word-level time alignments to the ground-truth English subtitles. And 300 hours was translated into Portuguese subtitles.","~2,000 Hours","Portuguese, English","Speech-to-Text, Translation, Summarization, Visual",Paper,Link
Beilharz et al.,LibriVoxDeEn,"Dataset contains sentence-aligned triples of German audio, German text, and English translation, based on German audio books. The corpus consists of over 100 hours of audio material and over 50k parallel sentences.","50,000+","German, English","Speech Translation, Machine Translation",Paper,Link
Iranzo-Sánchez et al.,Europarl-ST,"Dataset contains paired audio-text samples for speech translation, constructed using the debates carried out in the European Parliament in the period between 2008 and 2012. Contains 6 Euro languages: German, English, Spanish, French, Italian and Portuguese.",n/a,Multi-Lingual,Speech Translation,Paper,Link
Kocabiyikoglu et al.,Translation-Augmented-LibriSpeech-Corpus (Libri-Trans),Dataset is an augmentation of LibriSpeech ASR and contains English utterances (from audiobooks) automatically aligned with French text. It offers ~236h of speech aligned to translated text.,~236 Hours,"French, English",Speech Translation,Paper,Link
Wachsmuth et al.,ArguAna TripAdvisor Corpus,"Dataset contains 2,100 hotel reviews balanced with respect to the reviews’ sentiment scores. reviews are segmented into subsentence-level statements that have been manually classified as a fact, a positive, or a negative opinion.","2,1",English,"Classification, Sentiment Analysis",Paper,Link
Lim et al.,KorQuAD,"Dataset containing a total of 100,000+ question answer pairs.","102,96",Korean,"Question Answering, Reading Comprehension",Paper,Link
Efimov et al.,SberQuAD,Dataset consists of a question answers modeleld after SQuAD.,"50,364",Russian,"Question Answering, Reading Comprehension",Paper,Link
d’Hoffschmidt et al.,FQuAD,"Dataset contains 25,000+ questions on a set of Wikipedia articles, modeled after SQuAD.","25,000+",French,"Question Answering, Reading Comprehension",Paper,Link
Artetxe et al.,XQuAD,"Dataset consists of a subset of 240 context paragraphs and 1,190 question-answer pairs from the development set of SQuAD v1.1 with their translations in 10 languages: Spanish, German, Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, and Hindi.","1,19",Multi-Lingual,"Question Answering, Reading Comprehension",Paper,Link
Mozannar et al.,Arabic Reading Comprehension Dataset (ARCD),"Dataset contains 1,395 questions posed by crowdworkers on Wikipedia articles, and a machine translation of the Stanford Question Answering Dataset (Arabic-SQuAD) containing 48,344 questions.","~50,000",Arabic,"Question Answering, Reading Comprehension",Paper,Link
Dubey et al.,LC-QuAD 2.0,Dataset contains questions and SPARQL queries. LC-QuAD uses DBpedia v04.16 as the target KB.,30,English,"Question Answering, Knowledge Graph",Paper,Link
Croce et al.,SQuAD-it,"The dataset contains more than 60,000 question/answer pairs in Italian derived from the original English SQuAD dataset.","60,000+",Italian,"Question Answering, Reading Comprehension",Paper,Link
Narayan et al.,X-Sum,"The XSum dataset consists of 226,711 Wayback archived BBC articles (2010 to 2017) and covering a wide variety of domains: News, Politics, Sports, Weather, Business, Technology, Science, Health, Family, Education, Entertainment and Arts.","226,711",English,Summarization,Paper,Link
Christodoulopoulos et al.,Bible Corpus,A parallel corpus created from translations of the Bible containing 102 languages.,2.84M,Multi-Lingual,Machine Translation,Paper,Link
Ataman et al.,Bianet,"Dataset is a parallel news corpus with 3,214 Turkish articles with their sentence-aligned Kurdish or English translations from the Bianet online newspaper. Requires a request submission for dataset.","3,214",Multi-Lingual,Machine Translation,Paper,Link
Tiedemann et al.,CAPES,A parallel corpus of theses and dissertation abstracts in Portuguese and English from CAPES.,2.32M,"Portuguese, English",Machine Translation,Paper,Link
Tiedemann et al.,DOGC,A collection of documents from the official journal of the Catalan Goverment in Catalan and Spanish.,21.87M,"Catalan, Spanish","Text Corpora, Machine Translation",Paper,Link
Tiedemann et al.,ECB Corpus,Website and documentation from the European Central Bank. Contains 19 languages.,30.55M,Multi-Lingual,"Text Corpora, Machine Translation",Paper,Link
Tiedemann et al.,EMEA,A parallel corpus made out of PDF documents from the European Medicines Agency. Contains 22 languages.,26.51M,Multi-Lingual,Machine Translation,Paper,Link
Tiedemann et al.,Eubookshop,Corpus of documents from the EU bookshop. Contains 48 languages.,173.20M,Multi-Lingual,"Text Corpora, Machine Translation",Paper,Link
Tiedemann et al.,Finlex,"Dataset is a collection of legislative and other judicial information of Finland, which is available in Finnish and Swedish.",7.98M,"Finnish, Swedish","Text Corpora, Machine Translation",Paper,Link
Tiedemann et al.,Fiskmö,Dataset is a parallel corpus of Finnish and Swedish Languages.,4.24M,"Finnish, Swedish",Machine Translation,Paper,Link
Kuznetsova et al.,Open Images V6,Dataset containing millions of images that have been annotated with image-level labels and object bounding boxes.,"9,178,275",English,Automatic Image Captioning,Paper,Link
Fan et al.,Explain Like I’m Five (ELI5),"The dataset contains 270K threads of open-ended questions that require multi-sentence answers. It was extracted from subreddit titled “Explain Like I’m Five” (ELI5), in which an online community answers questions with responses that 5-year-olds can comprehend. Facebook scripts allow you to preprocess data.",270,English,"Question Answering, Reading Comprehension",Paper,Link
Moghe et al.,Background Knowledge Dialogue Dataset,"Dataset containing movie chats wherein each response is explicitly generated by copying and/or modifying sentences from unstructured background knowledge such as plots, comments and reviews about the movie.",90,English,Dialogue,Paper,Link
Banerjee et al.,Code-Mixed-Dialog,"A goal-oriented dialog dataset containing code-mixed conversations. Specifically, text from the DSTC2 restaurant reservation dataset and create code-mixed versions of it in Hindi-English, Bengali-English, Gujarati-English and Tamil-English.","49,167",Multi-Lingual,Dialogue,Paper,Link
Camacho-Collados et al.,A Novel Approach to a Semantically-Aware Representation of Items (NASARI),"Dataset contains semantic vector representations for BabelNet synsets and Wikipedia pages in several languages: English, Spanish, French, German and Italian. Currently available three vector types: lexical, unified and embedded.",610K-4.4M depending on language,Multi-Lingual,Semantic Textual Similarity,Paper,Link
Li et al.,Academic,"Questions about the Microsoft Academic Search (MAS) database, derived by enumerating every logical query that could be expressed using the search page of the MAS website and writing sentences to match them.",196,English,"Semantic Parsing, Text-to-SQL",Paper,Link
Finegan-Dollak et al.,Advising,"Dataset contains questions regarding course information at the University of Michigan, but with fictional student records.","4,57",English,"Semantic Parsing, Text-to-SQL",Paper,Link
Ayman et al.,Arabic Violence Twitter Corpus,"Annotated Arabic tweets which mention a violent act. Tweets were classifed into 8 classes: Crime, Accident, Crisis, Conflict, Human Rights Abuse, Violence, Opinion, or other. Requires using Twitter API to match IDs with tweets for retrieval.",20,Arabic,Classification,Paper,Link
Dahl/Iyer et al.,ATIS,"Dataset is a collection of utterances to a flight booking system, accompanied by a relational database and SQL queries to answer the questions.",877,English,"Semantic Parsing, Text-to-SQL",Paper,Link
Wolfson et al.,Break,"Dataset contains 83,978 examples sampled from 10 question answering datasets over text, images and databases. Dataset used to obtain the Question Decomposition Meaning Representation (QDMR) for questions.","83,978",English,Natural Question Understanding (NQU),Paper,Link
Schwenk et al.,CCMatrix,4.5 billion parallel sentences in 576 language pairs pulled from snapshots of the CommonCrawl public dataset.,4.5B,Multi-Lingual,Machine Translation,Paper,Link
Zhang et al.,Coarse Discourse,Dataset contains discourse annotations and relations on threads from Reddit during 2016. Requires merging using Reddit API.,"9,473",English,Text Corpora,Paper,Link
Abujabal et al.,Complex Factoid Question Answering with Paraphrase Clusters (ComQA),"The dataset contains questions with various challenging phenomena such as the need for temporal reasoning, comparison (e.g., comparatives, superlatives, ordinals), compositionality (multiple, possibly nested, subquestions with multiple entities), and unanswerable questions.","11,214",English,"Question Answering, Reading Comprehension",Paper,Link
Tjong et al.,Conference on Computational Natural Language Learning (CoNLL 2002),"Spanish data is a collection of newswire articles made available by the Spanish EFE News Agency.The Dutch data consist of four editions of the Belgian newspaper ""De Morgen"" of 2000. IOB2 format.",n/a,"Spanish, Dutch",Named Entity Recognition (NER),Paper,Link
Spasojevic et al.,Densely Annotated Wikipedia Texts (DAWT),"Dataset contains a total of 13.6M articles across several languages: English, Spanish, Italian, German, French and Arabic. The annotations include labeled text mentions mapping to entities (represented by their Freebase machine ids) as well as the type of entity.",13.6M,Multi-Lingual,Named Entity Recognition (NER),Paper,Link
Neudecker,Europeana Newspapers,"Named Entity Recognition corpora for Dutch, French, German languages from Europeana Newspapers. Data is encoded in the IOB format.","486,218",Multi-Lingual,Named Entity Recognition (NER),Paper,Link
Webster et al.,GAP Coreference Dataset,"Dataset contains 8,908 gender-balanced coreference-labeled pairs of (ambiguous pronoun, antecedent name), sampled from Wikipedia.","8,908",English,Coreference Resolution,Paper,Link
Zelle & Iyer et al.,GeoQuery,Dataset contains utterances issued to a database of US geographical facts.,877,English,"Semantic Parsing, Text-to-SQL",Paper,Link
NICT & Kyoto Univ.,Indic Languages Multilingual Parallel Corpus,"Dataset contains several languages: Bengali, Hindi, Malayalam, Tamil, Telugu, Sinhalese, Urdu and English. The corpus has been collected from OPUS and belongs to the spoken language (OpenSubtitles) domain.",n/a,Indian,Machine Translation,Paper,Link
Zastrow,"Named Entity Model for German, Politics (NEMGP)","Dataset contains texts from Wikipedia and WikiNews, manually annotated with named entity information.","5,094",German,Named Entity Recognition (NER),Paper,Link
Rae et al.,PG-19,"Dataset contains a set of books extracted rom the Project Gutenberg books library, that were published before 1919. It also contains metadata of book titles and publication dates.","28,752",English,"Text Corpora, Language Modeling",Paper,Link
Tang/Popescu/,Restaurants,"Dataset contains user questions about restaurants, their food types, and locations.",378,English,"Semantic Parsing, Text-to-SQL",Paper,Link
Iyer et al.,Scholar,"User questions about academic publications, with automatically generated SQL that was checked by asking the user if the output was correct.",817,English,"Semantic Parsing, Text-to-SQL",Paper,Link
Minard et al.,The NewsReader MEANTIME Corpus,"480 news articles: 120 English Wikinews articles on four topics (i.e. Airbus and Boeing, Apple Inc., Stock market, and General Motors, Chrysler and Ford) and their translations in Spanish, Italian, and Dutch. Annotated with entities, events, temporal, semantic roles and event/entity coreference.",480,Multi-Lingual,Named Entity Recognition (NER),Paper,Link
Dietz et al.,Trec CAR Dataset,"Dataset contains topics, outlines, and paragraphs that are extracted from English Wikipedia (2016 XML dump). Wikipedia articles are split into the outline of sections and the contained paragraphs.","~285,000",English,Information Retrieval,Paper,Link
Schwenk et al.,WikiMatrix,"Dataset contains 135 million parallel sentences for 1,620 different language pairs in 85 different languages.",135M,Multi-Lingual,Machine Translation,Paper,Link
Facebook Research,Wikipedia,The 2016-12-21 dump of English Wikipedia.,"5,075,182",English,Text Corpora,Paper,Link
Botha et al.,WikiSplit,"Dataset contains 1 million English sentences, each split into two sentences that together preserve the original meaning, extracted from Wikipedia edits.",1M,English,Sentence Simplification,Paper,Link
Zhong et al.,WikiSQL,A large collection of automatically generated questions about individual tables from Wikipedia.,"80,654",English,"Semantic Parsing, Text-to-SQL",Paper,Link
Zhang et al.,AG News,"Dataset contains more than 1 million news articles for topic classification. The 4 classes are: World, Sports, Business, and Sci/Tech.",1M+,English,Classification,Paper,Link
Tiedemann,Books Corpus,Dataset contains a collection of copyright free books. Corpus consists of 16 languages and 0.91M sentence fragments and 19.50M tokens.,0.91M,Multi-Lingual,Machine Translation,Paper,Link
Sang et al.,Conference on Computational Natural Language Learning (CoNLL 2003),"Dataset contains news articles whose text are segmented in 4 columns: the first item is a word, the second a part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag.","English 1,393; German 909","German, English","Named Entity Recognition (NER), Part-of-Speech (POS)",Paper,Link
Eichler et al.,Customer Interaction Data of German Emails and Online Requests,Dataset is used to evaluate the task of automatically categorizing German customer requests. The dataset consists of a set emails and online requests sent to the support center of a multimedia software company.,627,German,Text Corpora,Paper,Link
Kotlerman et al.,Excitement Datasets,Datasets contain negative feedbacks from customers where they state reasons for dissatisfaction with a given company. The datasets are available in English and Italian.,n/a,"Italian, English","Classification, Sentiment Analysis",Paper,Link
Benikova et al.,GermEval 2014 NER Shared Task,"The data was sampled from German Wikipedia and News Corpora as a collection of citations.The dataset covers over 31,000 sentences corresponding to over 590,000 tokens.","31,000+",German,Named Entity Recognition (NER),Paper,Link
CASMACAT,Global Voices Parallel Corpus,Dataset contains news articles from the web site Global Voices in multiple languages.,n/a,Multi-Lingual,Machine Translation,Paper,Link
University of Groningen,Groningen Meaning Bank,"Datasets contains texts in raw and tokenised format, tags for part of speech, named entities and lexical categories, and discourse representation structures compatible with first-order logic.",10,English,Text Corpora,Paper,Link
Hong et al.,IWSLT'15 English-Vietnamese,Parallel corpus used for machine translation English-Vietnamese.,"~130,000",Multi-Lingual,Machine Translation,Paper,Link
Kensho R&D,Kensho Derived Wikimedia Dataset (KDWD),Dataset contains two main components - a link annotated corpus of English Wikipedia pages and a compact sample of the Wikidata knowledge base.,n/a,English,"Text Corpora, Knowledge Base",Paper,Link
Paperno et al.,Language Modeling Broadened to Account for Discourse Aspects (LAMBADA),"Dataset contains narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word.","10,022",English,"Natural Language Understanding, Language Modeling",Paper,Link
Maas et al.,Large Movie Review Dataset - Imdb,"Dataset contains 25,000 highly polar movie reviews for training, and 25,000 for testing.",50,English,"Classification, Sentiment Analysis",Paper,Link
Bamman et al.,LitBank,"Dataset contains 100 works of English-language fiction. It currently contains annotations for entities, events and entity coreference in a sample of ~2,000 words from each of those texts, totaling 210,532 tokens.",100,English,Named Entity Recognition (NER),Paper,Link
Lewis et al.,MultiLingual Question Answering (MLQA),"Dataset for evaluating cross-lingual question answering performance. ~12K QA instances in English and 5K in each other language in SQuAD format in seven languages - English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese.","46,444",Multi-Lingual,"Question Answering, Reading Comprehension",Paper,Link
University of Groningen,Parallel Meaning Bank,"Dataset contains sentences and texts in raw and tokenised format, syntactic analysis, word senses, thematic roles, reference resolution, and formal meaning representations. The annotated parallel corpus inclues English, German, Dutch and Italian languages.","8,705",Multi-Lingual,Text Corpora,Paper,Link
Khot et al.,QASC,"QASC is a question-answering dataset with a focus on sentence composition. It consists of 9,980 8-way multiple-choice questions about grade school science (8,134 train, 926 dev, 920 test), and comes with a corpus of 17M sentences.","9,98",English,"Question Answering, Reading Comprehension",Paper,Link
Dasigi et al.,Quoref,"Dataset which tests the coreferential reasoning capability of reading comprehension systems. In this span-selection benchmark containing 24K questions over 4.7K paragraphs from Wikipedia, a system must resolve hard coreferences before selecting the appropriate span(s) in the paragraphs for answering questions.",24,English,"Question Answering, Reading Comprehension",Paper,Link
Negi et al.,SemEval-2019 Task 9 - Subtask A,Suggestion Mining from Online Reviews and Forums: Dataset contains corpora of unstructured text with the intent for mining it for suggestions.,"~6,300",English,Suggestion Mining,Paper,Link
Negi et al.,SemEval-2019 Task 9 - Subtask B,Suggestion Mining from Hotel Reviews: Dataset contains corpora of unstructured text with the intent for mining it for suggestions.,~800,English,Suggestion Mining,Paper,Link
Marelli et al.,Sentences Involving Compositional Knowledge (SICK),"Dataset contains sentence pairs, generated from two existing sets: the 8K ImageFlickr data set and the SemEval 2012 STS MSR-Video Description.","~10,000",English,"Semantic Textual Similarity, Entailment",Paper,Link
Clark et al.,TyDi QA,"TyDi QA includes question-answer pairs from 11 languages: Arabic, Bengali, English, Finnish, Indonesian, Kiswahili, Russian. Japanese, Korean, Thai, and Telugu.",204,Multi-Lingual,"Question Answering, Reading Comprehension",Paper,Link
Ziemski et al.,United Nations Parallel Corpus,"Parallel corpus presented consists of manually translated UN documents from the last 25 years (1990 to 2014) for the six official UN languages: Arabic, Chinese, English, French, Russian, and Spanish.","799,276",Multi-Lingual,Machine Translation,Paper,Link
Geiß et al.,Wikidata NE dataset,"Dataset has 2 parts: the Named Entity files and the link files. The Named Entity files include the most important information about the entities, whereas the link files contain the links and ids in other databases.",n/a,"German, English","Named Entity Recognition, Knowledge Base",Paper,Link
Merity et al.,WikiText-103 & 2,Dataset contains word and character level tokens extracted from Wikipedia,100M+,English,Language Modeling,Paper,Link
Wisesight,Wisesight Sentiment Corpus,"Dataset contains around 26,700 messages in Thai language from various social media with human-annotated sentiment classification (positive, neutral, negative, and question).","~26,700",Thai,"Classification, Sentiment Analysis",Paper,Link
Johnson et al.,A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning (CLEVR & CoGenT),"Visual question answering dataset contains 100,000 images and 999,968 questions.","999,968 questions; 100,000 images",English,"Question Answering, Visual",Paper,Link
Bhagavatula et al.,Abductive Natural Language Inference (aNLI),"Dataset is a binary-classification task, the goal is to pick the most plausible explanatory hypothesis given two observations from narrative contexts. It contains 20k commonsense narrative contexts and 200k explanations.""",20,English,"Classification, Commonsense",Paper,Link
Lin et al.,Common Objects in Context (COCO),"COCO is a large-scale object detection, segmentation, and captioning dataset. Dataset contains 330K images (>200K labeled) 1.5 million object instances, 80 object categories, 91 stuff categories, 5 captions per image.",330,English,Automatic Image Captioning,Paper,Link
Suhr et al.,Cornell Natural Language for Visual Reasoning (NLVR and NLVR2),Dataset contains two language grounding datasets containing natural language sentences grounded in images. The task is to determine whether a sentence is true about a visual input.,"NLVR2 107,292; NLVR 92,244",English,"Question Answering, Visual",Paper,Link
Welleck et al.,Dialogue Natural Language Inference (NLI),"Dataset used to improve the consistency of a dialogue model. It consists of sentence pairs labeled as entailment (E), neutral (N), or contradiction (C).""","340,000+",English,"Dialogue, Entailment",Paper,Link
Buechel et al.,EmoBank,Dataset is a large-scale text corpus manually annotated with emotion according to the psychological Valence-Arousal-Dominance scheme.,10,English,Classification,Paper,Link
Rashkin et al.,EmpatheticDialogues,Dataset of 25k conversations grounded in emotional situations.,25,English,Dialogue,Paper,Link
Wang et al.,Fact-based Visual Question Answering (FVQA),Dataset contains image question anwering triples,"5,826 questions; 2,190 images",English,"Question Answering, Visual",Paper,Link
Zellers et al.,HellaSwag,Dataset for studying grounded commonsense inference. It consists of 70k multiple choice questions about grounded situations: each question comes from one of two domains -- activitynet or wikihow -- with four answer choices about what might happen next in the scene.,70,English,Commonsense Reasoning,Paper,Link
Feng et al.,InsuranceQA,"Dataset contains questions and answers collected from the website Insurance Library. It consists of questions from real world users, the answers with high quality were composed by professionals with deep domain knowledge. There are 16,889 questions in total.","16,889",English,"Question Answering, Reading Comprehension",Paper,Link
Ling et al.,Irony Sarcasm Analysis Corpus,"Dataset contains tweets in 4 subgroups: irony, sarcasm, regular and figurative. Requires using Twitter API in order to obtain tweets.",33,English,"Classification, Sentiment Analysis",Paper,Link
Udagawa et al.,OneCommon,"Dataset contains 6,760 dialogues.","6,76",English,Dialogue,Paper,Link
Suárez et al.,Open Super-Large Crawled Almanach Corpus (OSCAR),Multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture.166 different languages available.,n/a,Multi-Lingual,Text Corpora,Paper,Link
Tiedemann et al.,OpenSubtitles,Dataset of multi-lingual dialogs from movie scripts. Includes 62 languages.,n/a,Multi-Lingual,Dialogue,Paper,Link
Bisk et al.,Physical IQA,"Dataset is used for commonsense QA benchmark for naive physics reasoning focusing on how we interact with everyday objects in everyday situations. The dataset includes 20,000 QA pairs that are either multiple-choice or true/false questions.",20,English,"Question Answering, Commonsense",Paper,Link
FitzGerald et al.,QA-SRL Bank,"Dataset contains question answer pairs for 64,000 sentences. Dataset is used to train model for semantic role labeling",64,English,"Question Answering, Semantic Role Labeling",Paper,Link
Levy et al.,QA-ZRE,"Dataset contain question answer pairs with each instance containing a relation, a question, a sentence, and an answer set.",30M,English,"Question Answering, Relation Extraction",Paper,Link
Vashishth et al.,"ReVerb45k, Base and Ambiguous","3 Datasets. In total, there are 91K triples.",91,English,"Information Retrieval, Knowledge Base",Paper,Link
Sänger et al.,Sentiment Corpus of App Reviews with Fine-grained Annotations in German (SCARE),"Dataset consists of fine-grained annotations for mobile application reviews from the Google Play Store. For each user review the mentioned application aspects, i.e., the design or the usability, as well as subjective phrases, which evaluate these aspects, are annotated. In addition, the polarity (positive, negative or neutral) of each subjective phrase is recorded as well as the relationship of an aspect to the main app in discussion. Requires emailing source for password to retrieve data.",800,German,"Classification, Sentiment Analysis",Paper,Link
Lake et al.,Simplified Versions of the CommAI Navigation tasks (SCAN),Dataset used for for studying compositional learning and zero-shot generalization. SCAN consists of a set of commands and their corresponding action sequences.,"20,000+",English,Compositional Learning,Paper,Link
Sap et al.,Social IQA,Dataset used fo question-answering benchmark for testing social commonsense intelligence.,"37,000+",English,"Question Answering, Commonsense",Paper,Link
Marsan Ma,Twitter Chat Corpus,Dataset contains Twitter question-answer pairs.,5M,English,Dialogue,Paper,Link
Das et al.,VisDial,"Dataset contains images from COCO training set, and dialogues. Meant to be used for model to be trained in answering questions about images during conversation. Contains 1.2M dialog question-answers.",1.2M,English,"Question Answering, Visual, Dialogue",Paper,Link
Cettolo et al.,Web Inventory of Transcribed and Translated Talks (WIT3),"Dataset contains a collection of transcribed and translated talks. The core of the dataset is from Ted Talks corpus. As of 2016, It holds 109 languages.",n/a,Multi-Lingual,Machine Translation,Paper,Link
Hewlett & Kenter et al.,WikiReading,"The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles. Includes English, Russian and Turkish.",18M,Multi-Lingual,"Knowledge Base, NLU",Paper,Link
Sakaguchi et al.,WinoGrande,"Formulated as a fill-in-a-blank task with binary options, the goal is to choose the right option for a given sentence which requires commonsense reasoning.",44,English,Commonsense Reasoning,Paper,Link
Strapparava et al.,Affective Text,"Classification of emotions in 250 news headlines. Categories: anger, disgust, fear, joy, happiness, sadness, surprise.",250,English,Emotion Classification,Paper,Link
Google,Argentinian Spanish [es-ar] Speech Multi-Speaker Dataset,"Speech dataset containing about 5,900 transcribed high-quality audio from Argentinian Spanish [es-ar] sentences recorded by volunteers.","~5,900",Spanish (Argentinan),Speech Recognition,Paper,Link
Kim et al.,Classify Emotional Relationships of Fictional Characters,"Dataset contains 19 short stories that are shorter than 1,500 words, and depict at least four different characters.",19,English,"Text Corpora, Emotion Classification",Paper,Link
Li et al.,DailyDialog,"A manually labelled conversations dataset. Categories: no emotion, anger, disgust, fear, happiness, sadness, surprise.","13,118",English,Emotion Classification,Paper,Link
Larson et al.,Dataset for Intent Classification and Out-of-Scope Prediction,Dataset is a benchmark for evaluating intent classification systems for dialog systems / chatbots in the presence of out-of-scope queries.,"23,000+",English,Intent Classification,Paper,Link
Geva et al.,DiscoFuse,Dataset contains examples for training sentence fusion models. Sentence fusion is the task of joining several independent sentences into a single coherent text. The data has been collected from Wikipedia and from Sports articles.,~60M,English,Sentence Fusion,Paper,Link
van der Burgh,Dutch Book Reviews,Dataset contains book reviews along with associated binary sentiment polarity labels.,"118,516",Dutch,"Classification, Sentiment Analysis",Paper,Link
Ghazi et al.,Emotion-Stimulus,"Dataset annotated with both the emotion and the stimulus using FrameNet’s emotions-directed frame. 820 sentences with both cause and emotion and 1594 sentences marked with their emotion tag. Categories: happiness, sadness, anger, fear, surprise, disgust and shame.","2,414",English,Emotion Classification,Paper,Link
Troiano et al.,Event-focused Emotion Corpora for German and English,"German and English emotion corpora for emotion classification, annotated with crowdsourcing in the style of the ISEAR resources.","2,002","German, English","Text Corpora, Emotion Classification",Paper,Link
Rashkin et al.,Event2Mind,"Dataset contains 25,000 events and free-form descriptions of their intents and reactions",25,English,Commonsense Inference,Paper,Link
Kunchukuttan et al.,IIT Bombay English-Hindi Corpus,Dataset contains parallel corpus for English-Hindi as well as monolingual Hindi corpus collected from a variety of existing sources.,1.49M,"Hindi, English",Machine Translation,Paper,Link
Weischedel et al.,OntoNotes 5.0,"Dataset contains various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in three languages (English, Chinese, and Arabic) with structural information (syntax and predicate argument structure) and shallow semantics (word sense linked to an ontology and coreference).",n/a,Multi-Lingual,"Information Retrieval, Syntactic Parsing",Paper,Link
Zhang et al.,Paraphrase Adversaries from Word Scrambling (PAWS),"Dataset contains 108,463 human-labeled and 656k noisily labeled pairs that feature the importance of modeling structure, context, and word order information for the problem of paraphrase identification.","750,000+",English,Paraphrasing Identification,Paper,Link
Yang et al.,Paraphrase Adversaries from Word Scrambling (PAWS-X),"Dataset contains 23,659 human translated PAWS evaluation pairs and 296,406 machine translated training pairs in six typologically distinct languages: French, Spanish, German, Chinese, Japanese, and Korean. All translated pairs are sourced from examples in PAWS-Wiki.","300,000+",Multi-Lingual,Paraphrasing Identification,Paper,Link
Boğaziçi University,Portuguese Newswire Corpus,"Dataset contains x number of newswire articles collected between years 1994-2016. Requires preprocesing of HTML pages, found in GitHub in the download link.",n/a,Portuguese (Brazil),Text Corpora,Paper,Link
Carvalho et al.,Portuguese SQuAD v1.1,Portuguese translation of the SQuAD dataset. The translation was performed using the Google Cloud API.,"~100,000",Portuguese,"Question Answering, Reading Comprehension",Paper,Link
Google,Relation Extraction Corpus,"A human-judged dataset of two relations involving public figures on Wikipedia: about 10,000 examples of ""place of birth"" and 40,000 examples of ""attended or graduated from an institution.""",10,English,Relation Extraction,Paper,Link
"SDA Lab, Uni. Of Bonn & Volkswagen Research",Soccer Dialogues,Dataset contains soccer dialogues over a knowledge graph,"2,89",English,"Knowledge Graphs, Dialogue",Paper,Link
Sarker et al.,Social Media Mining for Health (SMM4H),Dataset contains medication-related text classification and concept normalization from Twitter,"25,678",English,Classification,Paper,Link
Bates et al.,Switchboard Dialogue Act Corpus (SwDA),"A subset of the Switchboard-1 corpus consisting of 1,155 conversations and 42 tags","1,155",English,Dialogue Act Classification,Paper,Link
CrowdFlower,The Emotion in Text,"Dataset of tweets labelled with emotion. Categories: empty, sadness, enthusiasm, neutral, worry, sadness, love, fun, hate, happiness, relief, boredom, surprise, anger.",40,English,Emotion Classification,Paper,Link
Redy et al.,A Conversational Question Answering Challenge (CoQA),Dataset for measuring the ability of machines to understand a text passage and answer a series of interconnected questions that appear in a conversation.,"127,000+",English,"Question Answering, Reading Comprehension",Paper,Link
Dua et al.,A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs (DROP),"Dataset is used to resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting).",96,English,"Question Answering, Reading Comprehension",Paper,Link
Rohit Kulkarni,ABC Australia News Corpus,Entire news corpus of ABC Australia from 2003 to 2019.,"1,186,018",English,Text Corpora,Paper,Link
Yu et al.,Activitynet-QA,"Dataset contains 58,000 human-annotated QA pairs on 5,800 videos derived from the popular ActivityNet dataset. The dataset provides a benckmark for testing the performance of VideoQA models on long-term spatio-temporal.",58,English,"Question Answering, Visual, Commonsense",Paper,Link
Clark et al.,AI2 Reasoning Challenge (ARC),"Dataset contains 7,787 genuine grade-school level, multiple-choice science questions.","7,787",English,"Question Answering, Reading Comprehension",Paper,Link
Allen Institute,AI2 Science Questions Mercury,Dataset consists of questions used in student assessments across elementary and middle school grade levels. Includes questions with diagrams and without.,"6,94",English,Reading Comprehension,Paper,Link
Allen Institute,AI2 Science Questions v2.1,Dataset consists of questions used in student assessments in the United States across elementary and middle school grade levels. Each question is 4-way multiple choice format and may or may not include a diagram element.,"5,06",English,"Question Answering, Reading Comprehension",Paper,Link
McAuley et al.,Amazon Fine Food Reviews,Dataset consists of reviews of fine foods from amazon.,"568,454",English,"Classification, Sentiment Analysis",Paper,Link
McAuley et al.,Amazon Reviews,US product reviews from Amazon.,233.1M,English,"Classification, Sentiment Analysis",Paper,Link
Gashteovski et al.,An Open Information Extraction Corpus (OPIEC),"OPIEC is an Open Information Extraction (OIE) corpus, constructed from the entire English Wikipedia containing more than 341M triples.",341M,English,"Knowledge Base, Information Extraction",Paper,Link
Ling et al.,AQuA,Dataset containing algebraic word problems with rationales for their answers.,100,English,"Question Answering, Reading Comprehension",Paper,Link
Dalvi et al.,Aristo Tuple KB,"Dataset contains a collection of high-precision, domain-targeted (subject,relation,object) tuples extracted from text using a high-precision extraction pipeline, and guided by domain vocabulary constraints.","282,594",English,Knowledge Base,Paper,Link
n/a,arXiv Bulk Data,A collection of research papers on arXiv.,n/a,English,Text Corpora,Paper,Link
Zafarani et al.,ASU Twitter Dataset,"Twitter network data, not actual tweets. Shows connections between a large number of users.","11,316,811 users, 85,331,846 connections",English,"Clustering, Graph Analysis",Paper,Link
Google,AudioSet,"Dataset consists of an expanding ontology of 632 audio event classes and a collection of 2,084,320 human-labeled 10-second sound clips drawn from YouTube videos.",n/a,Multi-Lingual,"Speech Recognition, Visual",Paper,Link
The Hewlett Foundation,Automated Essay Scoring,Dataset contains student-written essays with scores.,n/a,English,Scoring Classification,Paper,Link
Several,Automatic Keyphrase Extraction,Multiple datasets for automatic keyphrase extraction.,n/a,English,Information Retrieval,Paper,Link
Weston et al.,bAbI 20 Tasks,"Dataset cotains a set of contexts, with multiple question-answer pairs available based on the contexts.",2,"Hindi, English","Question Answering, Reading Comprehension",Paper,Link
Bordes et al.,babI 6 Tasks Dialogue,Dataset contains 6 tasks for testing end-to-end dialog systems in the restaurant domain.,3,English,Dialogue,Paper,Link
Buza,BlogFeedback Dataset,Dataset to predict the number of comments a post will receive based on features of that post.,"60,021",English,Regression,Paper,Link
Schler et al.,Blogger Authorship Corpus,"Blog post entries of 19,320 people from blogger.com.","681,288",English,"Classification, Sentiment Analysis",Paper,Link
Clark et al.,BoolQ,Question answering dataset for yes/no questions.,"15,942",English,Question Answering,Paper,Link
Kawala et al.,Buzz in Social Media Dataset,Data from Twitter and Tom's Hardware. This dataset focuses on specific buzz topics being discussed on those sites.,140,English,Classification,Paper,Link
Bohanec,Car Evaluation Dataset,Car properties and their overall acceptability.,"1,728",English,Classification,Paper,Link
Hill et al.,Children’s Book Test (CBT),"Dataset contains ‘questions’ from chapters in the book by enumerating 21 consecutive sentences. In each question, the first 20 sentences form the context, and a word is removed from the 21st sentence, which becomes the query. Models must identify the answer word among a selection of 10 candidate answers appearing in the context sentences and the query.","~688,000",English,"Question Answering, Reading Comprehension",Paper,Link
Roemmele et al.,Choice of Plausible Alternatives (COPA),Dataset used for open-domain commonsense causal reasoning.,1,English,Commonsense Reasoning,Paper,Link
Šuster et al.,Clinical Case Reports for Machine Reading Comprehension (CliCR),"Dataset was built from clinical case reports, requiring the reader to answer the query with a medical problem/test/treatment entity.",100,English,"Question Answering, Reading Comprehension",Paper,Link
Gabrilovich et al.,ClueWeb Corpora,Annotated web pages from the ClueWeb09 and ClueWeb12 corpora.,"340,451,982",English,Classification,Paper,Link
MultiComp Lab,CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI),"Dataset contains more than 23,500 sentence utterance videos from more than 1000 online YouTube speakers. The dataset is gender balanced. All the sentences utterance are randomly chosen from various topics and monologue videos.","23,5",English,"Sentiment Analysis, Emotion Recognition, Visual",Paper,Link
Hermann et al.,CNN / Daily Mail Dataset,Cloze-style reading comprehension dataset created from CNN and Daily Mail news articles.,1M+,English,"Question Answering, Reading Comprehension",Paper,Link
Radlinski et al.,Coached Conversational Preference Elicitation,"Dataset consisting of 502 English dialogs with 12,000 annotated utterances between a user and an assistant discussing movie preferences in natural language.",12,English,Dialogue,Paper,Link
Marneffe et al.,CommitmentBank,"Dataset contains naturally occurring discourses whose final sentence contains a clause-embedding predicate under an entailment canceling operator (question, modal, negation, antecedent of conditional).","1,2",English,Natural Language Inference (NLI),Paper,Link
Mozilla,Common Voice,"Dataset containing audio in 29 languages and 2,454 recorded hours .",n/a,Multi-Lingual,Speech Recognition,Paper,Link
Common Crawl Foundation,CommonCrawl,Dataset contains data from 25 billion web pages.,25B,Multi-Lingual,Text Corpora,Paper,Link
Chen et al.,COmmonsense Dataset Adversarially-authored by Humans (CODAH),"Commonsense QA in the sentence completion style of SWAG. As opposed to other automatically generated NLI datasets, CODAH is adversarially constructed by humans who can view feedback from a pre-trained model and use this information to design challenging commonsense questions.","2,776",English,"Question Answering, Reading Comprehension, Commonsense",Paper,Link
Talmor et al.,CommonsenseQA,"Dataset contains multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers . It contains 12,102 questions with one correct answer and four distractor answers.","12,012",English,"Question Answering, Reading Comprehension, Commonsense",Paper,Link
Talmor et al.,ComplexWebQuestions,"Dataset includes pairs of simple questions and their corresponding SPARQL query. SPARQL queries were taken from WEBQUESTIONSSP and automatically created more complex queries that include phenomena such as function composition, conjunctions, superlatives and comparatives.","34,689",English,"Question Answering, Knowledge Base",Paper,Link
Sharma et al.,Conceptual Captions,Dataset contains ~3.3M images annotated with captions to be used for the task of automatically producing a natural-language description for an image.,"3,318,333",English,Automatic Image Captioning,Paper,Link
Yu et al.,Conversational Text-to-SQL Systems (CoSQL),"Dataset consists of 30k+ turns plus 10k+ annotated SQL queries, obtained from a Wizard-of-Oz collection of 3k dialogues querying 200 complex databases spanning 138 domains.It is the dilaogue version of the Spider and SParC tasks.",3,English,"Dialogue, SQL-to-Text",Paper,Link
Danescu et al.,Cornell Movie--Dialogs Corpus,"This corpus contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts. 220,579 conversational exchanges between 10,292 pairs of movie characters, involves 9,035 characters from 617 moviesin. total 304,713 utterances.","304,713",English,Dialogue,Paper,Link
Grusky et al.,Cornell Newsroom,Dataset contains 1.3 million articles and summaries written by authors and editors in the newsrooms of 38 major publications. The summaries are obtained from search and social metadata between 1998 and 2017.,1.3M,English,"Text Corpora, Summarization",Paper,Link
Crowdflower,Corporate Messaging Corpus,"Dataset contains classifed statements as information, dialog (replies to users, etc.), or action (messages that ask for votes or ask users to click on links, etc.","3,118",English,Classification,Paper,Link
Huang et al.,Cosmos QA,"Dataset containing thousands of problems that require commonsense-based reading comprehension, formulated as multiple-choice questions.",35,English,"Question Answering, Reading Comprehension, Commonsense",Paper,Link
Hossain et al.,Dataset for Fill-in-the-Blank Humor,"Dataset contains 50 fill-in-the-blank stories similar in style to Mad Libs. The blanks in these stories include the original word and the hint type (e.g. animal, food, noun, adverb).",50,English,Text Generation,Paper,Link
Richardson et al.,Dataset for the Machine Comprehension of Text,Stories and associated questions for testing comprehension of text.,660,English,"Question Answering, Reading Comprehension",Paper,Link
Dbpedia,Dbpedia,"The English version of the DBpedia knowledge base currently describes 6.6M entities of which 4.9M have abstracts, 1.9M have geo coordinates and 1.7M depictions. In total, 5.5M resources are classified in a consistent ontology.",6.6M,Multi-Lingual,Knowledge Base,Paper,Link
Lewis et al.,Deal or No Deal? End-to-End Learning for Negotiation Dialogues,"This dataset consists of 5,808 dialogues, based on 2,236 unique scenarios dealing with negotiations and complex communication.","5,808",English,Dialogue,Paper,Link
Reuters,DEXTER Dataset,"Task given is to determine, from features given, which articles are about corporate acquisitions.","2,6",English,Classification,Paper,Link
Tang et al.,DSL Corpus Collection (DSLCC),Dataset contains short excerpts of journalistic texts in similar languages and dialects.,294,Multi-Lingual,Discriminating between similar languages,Paper,Link
He et al.,DuReader,"DuReader version 2.0 contains more than 300K question, 1.4M evidence documents and 660K human generated answers.","1,431,429",Mandarin,"Question Answering, Reading Comprehension",Paper,Link
Kafle et al.,DVQA,Dataset containing data visualizations and natural language questions.,"3,487,194",English,"Question Answering, Visual, Commonsense",Paper,Link
Klimt et al.,Enron Email Dataset,Emails from employees at Enron organized into folders.,"~500,000",English,Text Corpora,Paper,Link
Koehn et al.,European Parliament Proceedings (Europarl),The Europarl parallel corpus is extracted from the proceedings of the European Parliament. It includes versions in 21 European languages.,10M+,Multi-Lingual,"Text Corpora, Machine Translation",Paper,Link
Rohit Kulkarni,Examiner Pseudo-News Corpus,"Clickbait, spam, crowd-sourced headlines from 2010 to 2015.","3,089,781",English,"Clustering, Events, Sentiment Analysis",Paper,Link
Jansen et al.,Explanations for Science Questions,"Data contains: gold explanation sentences supporting 363 science questions, relation annotation for a subset of those explanations, and a graphical annotation tool with annotation guidelines.","1,363",English,"Question Answering, Reading Comprehension",Paper,Link
Google,Google Books N-grams,N-grams from a very large corpus of books.,2.2 TB of text,Multi-Lingual,"Classification, Clustering",Paper,Link
Hudson et al.,GQA,Question answering on image scene graphs.,22M,English,"Question Answering, Visual, Commonsense",Paper,Link
Guttenberg,Guttenberg Book Corpus,"Dataset contains 60,000 eBooks.",60,Multi-Lingual,Text Corpora,Paper,Link
Natural Language Group - USC,Hansards Canadian Parliament,Dataset contains pairs of aligned text chunks (sentences or smaller fragments) from the official records (Hansards) of the 36th Canadian Parliament.,1.3M,English,Text Corpora,Paper,Link
Harvard,Harvard Library,"Dataset contains books, journals, electronic resources, manuscripts, archival materials, scores, audio, video and other materials.",12.7M,English,Text Corpora,Paper,Link
Davidson et al.,Hate Speech Identification Dataset,"Dataset contains lexicons, notebooks containing content that is racist, sexist, homophobic, and offensive in general.",n/a,English,Classification,Paper,Link
Dzogang et al.,Historical Newspapers Daily Word Time Series Dataset,Dataset contains daily contents of newspapers published in the US and UK from 1836 to 1922.,25,English,Text Corpora,Paper,Link
Home Depot,Home Depot Product Search Relevance,Dataset contains a number of products and real customer search terms from Home Depot's website.,n/a,English,Classification,Paper,Link
Yang et al.,HotpotQA,"Dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems.",1.25M,English,"Question Answering, Reading Comprehension",Paper,Link
Li et al.,Human-in-the-loop Dialogue Simulator (HITL),"Dataset provides a framework for evaluating a bot’s ability to learn to improve its performance in an online setting using feedback from its dialog partner. The dataset contains questions based on the bAbI and WikiMovies datasets, with the addition of feedback from the dialog partner.",n/a,English,"Question Answering, Reading Comprehension",Paper,Link
Stanford,IWSLT 15 English-Vietnamese,Sentence pairs for translation.,133,Multi-Lingual,Machine Translation,Paper,Link
Bojan Tunguz,Jeapardy Questions Answers,"Dataset contains Jeopardy questions, answers and other data.",200,English,"Question Answering, Reading Comprehension",Paper,Link
Galgani et al.,Legal Case Reports,Federal Court of Australia cases from 2006 to 2009.,4,English,Classification,Paper,Link
OpenSLR,LibriSpeech ASR,Large-scale (1000 hours) corpus of read English speech.,n/a,English,Speech Recognition,Paper,Link
Androutsopoulos et al.,Ling-Spam Dataset,Corpus contains both legitimate and spam emails.,n/a,English,Classification,Paper,Link
Microsoft,Meta-Learning Wizard-of-Oz (MetaLWOz),"Dataset designed to help develop models capable of predicting user responses in unseen domains. It was created by crowdsourcing 37,884 goal-oriented dialogs, covering 227 tasks in 47 domains.","37,884",English,Dialogue,Paper,Link
Microsoft,Microsoft Information-Seeking Conversation (MISC) dataset,"Dataset contains recordings of information-seeking conversations between human “seekers” and “intermediaries”. It includes audio and video signals; transcripts of conversation; affectual and physiological signals; recordings of search and other computer use; and post-task surveys on emotion, success, and effort.",n/a,English,"Speech Recognition, Dialogue, Visual",Paper,Link
Bajaj et al.,Microsoft Machine Reading COmprehension Dataset (MS MARCO),"Dataset focused on machine reading comprehension, question answering, and passage ranking, keyphrase extraction, and conversational search studies.","1,010,916",English,"Question Answering, Reading Comprehension",Paper,Link
Dolan et al.,Microsoft Research Paraphrase Corpus (MRPC),"Dataset contains pairs of sentences which have been extracted from news sources on the web, along with human annotations indicating whether each pair captures a paraphrase/semantic equivalence relationship.","5,8",English,Paraphrasing Identification,Paper,Link
Sordoni et al.,Microsoft Research Social Media Conversation Corpus,A-B-A triples extracted from Twitter.,"4,232",English,Graph Analysis,Paper,Link
Microsoft,Microsoft Speech Corpus,"Dataset contains conversational and phrasal speech training and test data for Telugu, Tamil and Gujarati languages.",n/a,Indian,Speech Recognition,Paper,Link
Federmann et al.,Microsoft Speech Language Translation Corpus (MSLT),"Dataset contains conversational, bilingual speech test and tuning data for English, Chinese, and Japanese. It includes audio data, transcripts, and translations; and allows end-to-end testing of spoken language translation systems on real-world data.",n/a,Multi-Lingual,"Speech Recognition, Machine Translation",Paper,Link
Harper et al.,MovieLens,"Dataset contains 22,000,000 ratings and 580,000 tags applied to 33,000 movies by 240,000 users.",~22M,English,"Clustering, Classification, Regression",Paper,Link
Dooms,MovieTweetings,Movie rating dataset based on public and well-structured tweets.,"822,784",English,"Classification, Regression",Paper,Link
Microsoft,MSParS,Dataset for the open domain semantic parsing task.,"81,826",English,Semantic Parsing,Paper,Link
Budzianowski et al.,Multi-Domain Wizard-of-Oz Dataset (MultiWoz),Dataset of human-human written conversations spanning over multiple domains and topics. The dataset was collected based on the Wizard of Oz experiment on Amazon MTurk.,"10,438",English,Dialogue,Paper,Link
Yagcioglu et al.,Multimodal Comprehension of Cooking Recipes (RecipeQA),Dataset for multimodal comprehension of cooking recipes. It consists of over 36K question-answer pairs automatically generated from approximately 20K unique recipes with step-by-step instructions and images.,20,English,"Question Answering, Reading Comprehension",Paper,Link
Williams et al.,MultiNLI Matched/Mismatched,Dataset contains sentence pairs annotated with textual entailment information.,433,English,Entailment,Paper,Link
He et al.,MutualFriends,Task where two agents must discover which friend of theirs is mutual based on the friend's attributes.,n/a,English,Dialogue,Paper,Link
Kočiský et al.,NarrativeQA,"Dataset contains the list of documents with Wikipedia summaries, links to full stories, and questions and answers.","1,572",English,"Question Answering, Reading Comprehension",Paper,Link
Kwiatkowski et al.,Natural Questions (NQ),"Dataset contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question.","320,000+",English,"Question Answering, Reading Comprehension",Paper,Link
Misra,News Headlines Dataset for Sarcasm Detection,High quality dataset with Sarcastic and Non-sarcastic news headlines.,"26,709",English,"Clustering, Events, Language Detection",Paper,Link
Rohit Kulkarni,News Headlines Of India,"Dataset contains archive of noteable events in India during 2001-2018, recorded by the Times of India.","2,969,922",English,Text Corpora,Paper,Link
Trischler et al.,NewsQA,"Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN.","12,744",English,"Question Answering, Reading Comprehension",Paper,Link
Xu et al.,NLP Chinese Corpus,Large text corpora in Chinese.,10M+,Chinese,Text Corpora,Paper,Link
Forsyth et al.,NPS Chat Corpus,Posts from age-specific online chat rooms.,"~500,000",English,Dialogue,Paper,Link
Kan et al.,NUS SMS Corpus,"SMS messages collected between 2 users, with timing analysis.","67,093","Mandarin, English",Dialogue,Paper,Link
Dermouche et al.,NYSK Dataset,English news articles about the case relating to allegations of sexual assault against the former IMF director Dominique Strauss-Kahn.,"10,421",English,"Sentiment Analysis, Topic Extraction",Paper,Link
Rohit Kulkarni et al.,One Week of Global News Feeds,Dataset contains most of the new news content published online over one week in 2017 and 2018.,3.3M,Multi-Lingual,Text Corpora,Paper,Link
Ammar et al.,Open Research Corpus,"Dataset contains over 39 million published research papers in Computer Science, Neuroscience, and Biomedical.",39M,English,Text Corpora,Paper,Link
Mihaylov et al.,OpenBookQA,"Dataset modeled after open book exams for assessing human understanding of a subject. It consists of 5,957 multiple-choice elementary-level science questions (4,957 train, 500 dev, 500 test), which probe the understanding of a small ""book"" of 1,326 core science facts and the application of these facts to novel situations.","5,957",English,"Question Answering, Reading Comprehension",Paper,Link
Gokaslan et al.,OpenWebTextCorpus,Dataset contains millions of webpages text stemming from reddit urls totalling 38Gb of text data.,"8,013,769",English,Text Corpora,Paper,Link
Ganesan et al.,OpinRank Review Dataset,Reviews of cars and hotels from Edmunds.com and TripAdvisor.,"Edmunds: 42,230, TripAdivsor: 259,000",English,"Information Retrieval, Entity Ranking, Entiry Retrieval",Paper,Link
Xu et al.,Paraphrase and Semantic Similarity in Twitter (PIT),Dataset focuses on whether tweets have (almost) same meaning/information or not.,"18,762",English,Classification,Paper,Link
Luyckx et al.,Personae Corpus,Collected for experiments in Authorship Attribution and Personality Prediction. Consists of 145 Dutch-language essays.,145,Dutch,"Classification, Regression",Paper,Link
Joshi et al.,Personalized Dialog,Dataset of dialogs from movie scripts.,12,English,Dialogue,Paper,Link
Pungas et al.,Plaintext Jokes,"208,000 jokes in this database scraped from three sources.",208,English,Text Corpora,Paper,Link
Mishra et al.,ProPara Dataset,"Dataset is used for comprehension of simple paragraphs describing processes, e.g., photosynthesis. The comprehension task relies on predicting, tracking, and answering questions about how entities change during the process.",488,English,"Question Answering, Reading Comprehension",Paper,Link
Tajford et al.,QuaRel Dataset,"Dataset contains 2,771 story questions about qualitative relationships.","2,771",English,"Question Answering, Reading Comprehension",Paper,Link
Tajford et al.,QuaRTz Dataset,"Dataset contains 3,864 questions about open domain qualitative relationships. Each question is paired with one of 405 different background sentences (sometimes short paragraphs).","3,864",English,"Question Answering, Reading Comprehension",Paper,Link
Dhingra et al.,Quasar-S & T,"The Quasar-S dataset consists of 37,000 cloze-style queries constructed from definitions of software entity tags on the popular website Stack Overflow. The Quasar-T dataset consists of 43,000 open-domain trivia questions and their answers obtained from various internet sources.",80,English,"Question Answering, Reading Comprehension",Paper,Link
Choi et al.,Question Answering in Context (QuAC),"Dataset for modeling, understanding, and participating in information seeking dialog.",14,English,"Question Answering, Reading Comprehension",Paper,Link
Rajpurkar et al.,Question NLI,Dataset converts SQuAD dataset into sentence pair classification by forming a pair between each question and each sentence in the corresponding context.,110,English,Natural Language Inference (NLI),Paper,Link
Quora,Quora Question Pairs,The task is to determine whether a pair of questions are semantically equivalent.,400,English,Semantic Textual Similarity,Paper,Link
Lai et al.,ReAding Comprehension Dataset From Examinations (RACE),Dataset was collected from the English exams evaluating the students' ability in understanding and reasoning.,28,English,"Question Answering, Reading Comprehension",Paper,Link
Khashabi et al.,Reading Comprehension over Multiple Sentences (MultiRC),Dataset of short paragraphs and multi-sentence questions that can be answered from the content of the paragraph.,"~10,000",English,"Question Answering, Reading Comprehension",Paper,Link
Zhang et al.,Reading Comprehension with Commonsense Reasoning Dataset (Record),"Reading comprehension dataset which requires commonsense reasoning. Contains 120,000+ queries from 70,000+ news articles.","70,000+",English,"Question Answering, Reading Comprehension",Paper,Link
Welbl et al.,Reading Comprehension with Multiple Hops (Qangaroo),Reading Comprehension datasets focussing on multi-hop (alias multi-step) inference. There are 2 datasets: Wikihop (based on wikipedia) and Medhop (based on PubMed research papers).,"~53,000",English,"Question Answering, Reading Comprehension",Paper,Link
"Dagan et al, Bar Haim et al, Giampiccolo, and Bentivogli et al.",Recognizing Textual Entailment (RTE),Datasets are combined and converted to two-class classification: entailment and not_entailment.,n/a,English,Entailment,Paper,Link
Reddit,Reddit All Comments Corpus,All Reddit comments (as of 2017).,"3,329,219,008",English,Text Corpora,Paper,Link
Dstl,Relationship and Entity Extraction Evaluation Dataset (RE3D),Entity and Relation marked data from various news and government sources.,n/a,English,"Classification, Entity and Relation Recognition",Paper,Link
Lewis et al.,Reuters-21578 Benchmark Corpus,"Dataset is a collection of 10,788 documents from the Reuters financial newswire service, partitioned into a training set with 7769 documents and a test set with 3019 documents.","10,788",English,Classification,Paper,Link
Alhagri,Saudi Newspapers Corpus,"Dataset contains 31,030 Arabic newspaper articles.","31,03",Arabic,Text Corpora,Paper,Link
Rastogi et al.,Schema-Guided Dialogue State Tracking (DSTC 8),Dataset contains 18K dialogues between a virtual assistant and a user.,"~18,000",English,Dialogue State Tracking,Paper,Link
Welbl et al.,SciQ Dataset,"Dataset contains 13,679 crowdsourced science exam questions about Physics, Chemistry and Biology, among others. The questions are in multiple-choice format with 4 answer options each.","13,769",English,"Question Answering, Reading Comprehension",Paper,Link
Khot et al.,SciTail Dataset,Dataset is an entailment dataset created from multiple-choice science exams and web sentences. Each question and the correct answer choice are converted into an assertive statement to form the hypothesis.,"27,026",English,Entailment,Paper,Link
Dunn et al.,SearchQA,Dataset from Jeapardy archives which consists of more than 140k question-answer pairs with each pair having 49.6 snippets on average.,140,English,"Question Answering, Reading Comprehension",Paper,Link
Yu et al.,Semantic Parsing in Context (SParC),"Dataset consists of 4,298 coherent question sequences (12k+ unique individual questions annotated with SQL queries annotated byt. It is the context-dependent/multi-turn version of the Spider task.","4,298",English,"Semantic Parsing, SQL-to-Text",Paper,Link
Cer et al.,Semantic Textual Similarity Benchmark,The task is to predict textual similarity between sentence pairs.,"8,628",English,Semantic Textual Similarity,Paper,Link
Nakov et al.,SemEvalCQA,Dataset for community question answering.,n/a,"Arabic, English","Question Answering, Reading Comprehension",Paper,Link
Kotzias,Sentiment Labeled Sentences Dataset,"Dataset contains 3,000 sentiment labeled sentences.",3,English,"Classification, Sentiment Analysis",Paper,Link
Go et al.,Sentiment140,"Tweet data from 2009 including original text, time stamp, user and sentiment.","1,578,627",English,"Classification, Sentiment Analysis",Paper,Link
Saeidi et al.,Shaping Answers with Rules through Conversation (ShARC),ShARC is a Conversational Question Answering dataset focussing on question answering from texts containing rules.,32,English,"Question Answering, Reading Comprehension",Paper,Link
The Hewlett Foundation,Short Answer Scoring,Student-written short-answer responses.,n/a,English,Scoring Classification,Paper,Link
Zellers et al.,Situations With Adversarial Generations (SWAG),"Dataset consists of 113k multiple choice questions about grounded situations. Each question is a video caption from LSMDC or ActivityNet Captions, with four answer choices about what might happen next in the scene.",113,English,"Question Answering, Reading Comprehension",Paper,Link
Nguyen,Skytrax User Reviews Dataset,"User reviews of airlines, airports, seats, and lounges from Skytrax.","41,396",English,"Classification, Sentiment Analysis",Paper,Link
Almeida et al.,SMS Spam Collection Dataset,Dataset contains SMS spam messages.,"5,574",English,Classification,Paper,Link
McAuley et al.,SNAP Social Circles: Twitter Database,Large Twitter network data.,"Nodes: 81,306, Edges:1,768,149",English,"Clustering, Graph Analysis",Paper,Link
Zadeh et al.,Social-IQ Dataset,Dataset containing videos and natural language questions for visual reasoning.,"7,5",English,"Question Answering, Visual, Commonsense",Paper,Link
Hopkins et al.,Spambase Dataset,Dataset contains spam emails.,"4,601",English,Classification,Paper,Link
Yu et al.,Spider 1.0,"Dataset consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains.","10,181",English,"Semantic Parsing, SQL-to-Text",Paper,Link
Rajpurkar et al.,SQuAD v2.0,Paragraphs w/ questions and answers.,150,English,"Question Answering, Reading Comprehension",Paper,Link
Stack Overflow,Stack Overlow BigQuery Dataset,"BigQuery dataset includes an archive of Stack Overflow content, including posts, votes, tags, and badges.",n/a,English,Text Corpora,Paper,Link
Bowman et al.,Stanford Natural Language Inference (SNLI) Corpus,"Image captions matched with newly constructed sentences to form entailment, contradiction, or neutral pairs.",570,English,Natural Language Inference (NLI),Paper,Link
Elsahar et al.,T-REx,Dataset contains Wikipedia abstracts aligned with Wikidata entities.,11M aligned triples,English,Relation Extraction,Paper,Link
Chen et al.,TabFact,Dataset contains 16k Wikipedia tables as evidence for 118k human annotated statements to study fact verification with semi-structured evidence.,16,English,Natural Language Inference (NLI),Paper,Link
Byrne et al.,Taskmaster-1,"Dataset contains 13,215 task-based dialogs, including 5,507 spoken and 7,708 written dialogs created with two distinct procedures. Each conversation falls into one of six domains: ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations.","13,215",English,Dialogue,Paper,Link
Timo Block,Ten Thousand German News Articles Dataset (10kGNAD),Dataset consists of 10273 german language news articles from an austrian online newspaper categorized into nine topics.,"10,273",German,Classification,Paper,Link
Song et al.,Tencent AI Lab Embedding Corpus,"Dataset provides 200-dimension vector representations, a.k.a. embeddings, for over 8 million Chinese words and phrases.",8M,Chinese,Embeddings,Paper,Link
Kembhavi et al.,Textbook Question Answering,"The M3C task builds on the popular Visual Question Answering (VQA) and Machine Comprehension (MC) paradigms by framing question answering as a machine comprehension task, where the context needed to answer questions is provided and composed of both text and images.","26,62",English,"Question Answering, Reading Comprehension, Visual",Paper,Link
Singh et al.,TextVQA,"TextVQA requires models to read and reason about text in images to answer questions about them. Specifically, models need to incorporate a new modality of text present in the images and reason over it to answer TextVQA questions.","36,602",English,"Question Answering, Visual, Commonsense",Paper,Link
Warstadt et al.,The Benchmark of Linguistic Minimal Pairs (BLiMP),BLiMP is a challenge set for evaluating what language models (LMs) know about major grammatical phenomena in English.,"67 sub-datasets each with 1,000 minimal pairs",English,Language Modeling,Paper,Link
NeurIPS,The Conversational Intelligence Challenge 2 (ConvAI2),A chit-chat dataset based on PersonaChat dataset.,"3,127",English,Dialogue,Paper,Link
Warstadt et al.,The Corpus of Linguistic Acceptability (CoLa),Dataset used to classifiy sentences as grammatical or not grammatical.,"10,657",English,Grammatical Acceptability,Paper,Link
Weston,The Dialog-based Language Learning Dataset,Dataset was designed to measure how well models can perform at learning as a student given a teacher’s textual responses to the student’s answer.,n/a,English,"Question Answering, Reading Comprehension",Paper,Link
Rohit Kulkarni,The Irish Times IRS,Dataset contains 23 years of events from Ireland.,"1,425,460",English,"Clustering, Events, Language Detection",Paper,Link
Dodge et al.,The Movie Dialog Dataset,"Dataset measures how well models can perform at goal and non-goal orientated dialogue centered around the topic of movies (question answering, recommendation and discussion).",~3.5M,English,"Question Answering, Reading Comprehension",Paper,Link
Marcus et al.,The Penn Treebank Project,Naturally occurring text annotated for linguistic structure.,~1M words,English,POS,Paper,Link
Bordes et al.,The SimpleQuestions Dataset,"Dataset for question answering with human generated questions paired with a corresponding fact, formatted as (subject, relationship, object), that provides the answer but also a complete explanation.","108,442",English,"Question Answering, Knowledge Base",Paper,Link
Socher et al.,The Stanford Sentiment Treebank (SST),Sentence sentiment classification of movie reviews.,69,English,"Classification, Sentiment Analysis",Paper,Link
Mostafazadeh et al.,The Story Cloze Test | ROCStories,Dataset for story understanding that provides systems with four-sentence stories and two possible endings. The systems must then choose the correct ending to the story.,"100,000+",English,"Question Answering, Reading Comprehension",Paper,Link
Miller et al.,The WikiMovies Dataset,"Dataset contains only the QA part of the Movie Dialog dataset, but using three different settings of knowledge: using a traditional knowledge base (KB), using Wikipedia as the source of knowledge, or using IE (information extraction) over Wikipedia.","~100,000",English,"Question Answering, Reading Comprehension",Paper,Link
Levesque et al.,The Winograd Schema Challenge,Dataset to determine the correct referrent of the pronoun from among the provided choices.,150,Multi-Lingual,Coreference Resolution,Paper,Link
Gopalakrishnan et al.,Topical-Chat,A knowledge-grounded human-human conversation dataset where the underlying knowledge spans 8 broad topics and conversation partners don’t have explicitly defined roles.,"10,784",English,Dialogue,Paper,Link
Ch'ng et al.,Total-Text-Dataset,Dataset used to classify curved text in pictures.,"~1,500",English,Scene Text Detection,Paper,Link
Wang et al.,TrecQA,Dataset is commonly used for evaluating answer selection in question answering.,n/a,English,"Question Answering, Reading Comprehension",Paper,Link
Joshi et al.,TriviaQA,"Dataset containing over 650K question-answer-evidence triples. It includes 95K QA pairs authored by trivia enthusiasts and independently gathered evidence documents, 6 per question on average.","650,000+",English,"Question Answering, Reading Comprehension",Paper,Link
Allen Institute,TupleInf Open IE Dataset,"Dataset contains Open IE tuples extracted from 263K sentences that were used by the solver in ""Answering Complex Questions Using Open Information Extraction"" (referred as Tuple KB, T).",263,English,Knowledge Base,Paper,Link
Mitchell et al.,Twenty Newsgroups Dataset,Dataset is a collection newsgroup documents used for classification task.,20,English,"Classification, Clustering",Paper,Link
Abdulla,Twitter Dataset for Arabic Sentiment Analysis,Dataset contains Arabic tweets.,2,Arabic,"Classification, Sentiment Analysis",Paper,Link
Figure Eight,Twitter US Airline Sentiment,"Dataset contains airline-related tweets that were labeled with positive, negative, and neutral sentiment.","14,5",English,"Classification, Sentiment Analysis",Paper,Link
Hu et al.,Twitter100k,Pairs of images and tweets.,100,English,Multi-Modal Learning,Paper,Link
Lowe et al.,Ubuntu Dialogue Corpus,Dialogues extracted from Ubuntu chat stream on IRC.,930,English,"Text Corpora, Dialogue",Paper,Link
Anonymous,Urban Dictionary Dataset,"Corpus of words, votes and definitions.","2,606,522",English,Reading Comprehension,Paper,Link
Shaoul et al.,UseNet Corpus,UseNet forum postings.,7B,English,Dialogue,Paper,Link
Zellers et al.,Video Commonsense Reasoning (VCR),Dataset contains 290K multiple-choice questions on 110K images.,290,English,"Question Answering, Visual, Commonsense",Paper,Link
Antol et al.,Visual QA (VQA),"Dataset containing open-ended questions about images. These questions require an understanding of vision, language and commonsense to answer.","265,016 images",English,Visual Question Answering,Paper,Link
Various,Voices Obscured in Complex Environmental Settings (VOiCES),"Dataset contains a total of 15 hours (3,903 audio files) in male and female read speech.",n/a,English,Speech Recognition,Paper,Link
Nagrani et al.,VoxCeleb,"An audio-visual dataset consisting of short clips of human speech, extracted from interview videos uploaded to YouTube.",n/a,Multi-Lingual,"Speech Recognition, Visual",Paper,Link
Kowsari et al.,Web of Science Dataset,Hierarchical Datasets for Text Classification.,"46,985",English,Classification,Paper,Link
Yih et al.,WebQuestions Semantic Parses Dataset,"Dataset contains full semantic parses in SPARQL queries for 4,737 questions, and “partial” annotations for the remaining 1,073 questions for which a valid parse could not be formulated or where the question itself is bad or needs a descriptive answer.","5,81",English,Semantic Parsing,Paper,Link
Onishi et al.,Who Did What Dataset,"Dataset contains over 200,000 fill-in-the-gap (cloze) multiple choice reading comprehension problems constructed from the LDC English Gigaword newswire corpus.","200,000K",English,"Question Answering, Reading Comprehension",Paper,Link
Koupaee et al.,WikiHow,Dataset contains article and summary pairs extracted and constructed from an online knowledge base written by different human authors.,"230,000+",English,"Text Corpora, Summarization",Paper,Link
Singh et al.,WikiLinks,Dataset contains 40 million mentions over 3 million entities based on hyperlinks from Wikipedia.,~10M,English,Text Corpora,Paper,Link
Misra,News Headlines Dataset for Sarcasm Detection,High quality dataset with Sarcastic and Non-sarcastic news headlines.,"26,709",English,"Clustering, Events, Language Detection",Paper,Link
Rohit Kulkarni,News Headlines Of India,"Dataset contains archive of noteable events in India during 2001-2018, recorded by the Times of India.","2,969,922",English,Text Corpora,Paper,Link
Trischler et al.,NewsQA,"Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN.","12,744",English,"Question Answering, Reading Comprehension",Paper,Link
Xu et al.,NLP Chinese Corpus,Large text corpora in Chinese.,10M+,Chinese,Text Corpora,Paper,Link
Forsyth et al.,NPS Chat Corpus,Posts from age-specific online chat rooms.,"~500,000",English,Dialogue,Paper,Link
Kan et al.,NUS SMS Corpus,"SMS messages collected between 2 users, with timing analysis.","67,093","Mandarin, English",Dialogue,Paper,Link
Dermouche et al.,NYSK Dataset,English news articles about the case relating to allegations of sexual assault against the former IMF director Dominique Strauss-Kahn.,"10,421",English,"Sentiment Analysis, Topic Extraction",Paper,Link
Rohit Kulkarni et al.,One Week of Global News Feeds,Dataset contains most of the new news content published online over one week in 2017 and 2018.,3.3M,Multi-Lingual,Text Corpora,Paper,Link
Ammar et al.,Open Research Corpus,"Dataset contains over 39 million published research papers in Computer Science, Neuroscience, and Biomedical.",39M,English,Text Corpora,Paper,Link
Mihaylov et al.,OpenBookQA,"Dataset modeled after open book exams for assessing human understanding of a subject. It consists of 5,957 multiple-choice elementary-level science questions (4,957 train, 500 dev, 500 test), which probe the understanding of a small ""book"" of 1,326 core science facts and the application of these facts to novel situations.","5,957",English,"Question Answering, Reading Comprehension",Paper,Link
Gokaslan et al.,OpenWebTextCorpus,Dataset contains millions of webpages text stemming from reddit urls totalling 38Gb of text data.,"8,013,769",English,Text Corpora,Paper,Link
Ganesan et al.,OpinRank Review Dataset,Reviews of cars and hotels from Edmunds.com and TripAdvisor.,"Edmunds: 42,230, TripAdivsor: 259,000",English,"Information Retrieval, Entity Ranking, Entiry Retrieval",Paper,Link
Xu et al.,Paraphrase and Semantic Similarity in Twitter (PIT),Dataset focuses on whether tweets have (almost) same meaning/information or not.,"18,762",English,Classification,Paper,Link
Luyckx et al.,Personae Corpus,Collected for experiments in Authorship Attribution and Personality Prediction. Consists of 145 Dutch-language essays.,145,Dutch,"Classification, Regression",Paper,Link
Joshi et al.,Personalized Dialog,Dataset of dialogs from movie scripts.,12,English,Dialogue,Paper,Link
Pungas et al.,Plaintext Jokes,"208,000 jokes in this database scraped from three sources.",208,English,Text Corpora,Paper,Link
Mishra et al.,ProPara Dataset,"Dataset is used for comprehension of simple paragraphs describing processes, e.g., photosynthesis. The comprehension task relies on predicting, tracking, and answering questions about how entities change during the process.",488,English,"Question Answering, Reading Comprehension",Paper,Link
Tajford et al.,QuaRel Dataset,"Dataset contains 2,771 story questions about qualitative relationships.","2,771",English,"Question Answering, Reading Comprehension",Paper,Link
Tajford et al.,QuaRTz Dataset,"Dataset contains 3,864 questions about open domain qualitative relationships. Each question is paired with one of 405 different background sentences (sometimes short paragraphs).","3,864",English,"Question Answering, Reading Comprehension",Paper,Link
Dhingra et al.,Quasar-S & T,"The Quasar-S dataset consists of 37,000 cloze-style queries constructed from definitions of software entity tags on the popular website Stack Overflow. The Quasar-T dataset consists of 43,000 open-domain trivia questions and their answers obtained from various internet sources.",80,English,"Question Answering, Reading Comprehension",Paper,Link
Choi et al.,Question Answering in Context (QuAC),"Dataset for modeling, understanding, and participating in information seeking dialog.",14,English,"Question Answering, Reading Comprehension",Paper,Link
Rajpurkar et al.,Question NLI,Dataset converts SQuAD dataset into sentence pair classification by forming a pair between each question and each sentence in the corresponding context.,110,English,Natural Language Inference (NLI),Paper,Link
Quora,Quora Question Pairs,The task is to determine whether a pair of questions are semantically equivalent.,400,English,Semantic Textual Similarity,Paper,Link
Lai et al.,ReAding Comprehension Dataset From Examinations (RACE),Dataset was collected from the English exams evaluating the students' ability in understanding and reasoning.,28,English,"Question Answering, Reading Comprehension",Paper,Link
Khashabi et al.,Reading Comprehension over Multiple Sentences (MultiRC),Dataset of short paragraphs and multi-sentence questions that can be answered from the content of the paragraph.,"~10,000",English,"Question Answering, Reading Comprehension",Paper,Link
Zhang et al.,Reading Comprehension with Commonsense Reasoning Dataset (Record),"Reading comprehension dataset which requires commonsense reasoning. Contains 120,000+ queries from 70,000+ news articles.","70,000+",English,"Question Answering, Reading Comprehension",Paper,Link
Welbl et al.,Reading Comprehension with Multiple Hops (Qangaroo),Reading Comprehension datasets focussing on multi-hop (alias multi-step) inference. There are 2 datasets: Wikihop (based on wikipedia) and Medhop (based on PubMed research papers).,"~53,000",English,"Question Answering, Reading Comprehension",Paper,Link
"Dagan et al, Bar Haim et al, Giampiccolo, and Bentivogli et al.",Recognizing Textual Entailment (RTE),Datasets are combined and converted to two-class classification: entailment and not_entailment.,n/a,English,Entailment,Paper,Link
Reddit,Reddit All Comments Corpus,All Reddit comments (as of 2017).,"3,329,219,008",English,Text Corpora,Paper,Link
Dstl,Relationship and Entity Extraction Evaluation Dataset (RE3D),Entity and Relation marked data from various news and government sources.,n/a,English,"Classification, Entity and Relation Recognition",Paper,Link
Lewis et al.,Reuters-21578 Benchmark Corpus,"Dataset is a collection of 10,788 documents from the Reuters financial newswire service, partitioned into a training set with 7769 documents and a test set with 3019 documents.","10,788",English,Classification,Paper,Link
Alhagri,Saudi Newspapers Corpus,"Dataset contains 31,030 Arabic newspaper articles.","31,03",Arabic,Text Corpora,Paper,Link
Rastogi et al.,Schema-Guided Dialogue State Tracking (DSTC 8),Dataset contains 18K dialogues between a virtual assistant and a user.,"~18,000",English,Dialogue State Tracking,Paper,Link
Welbl et al.,SciQ Dataset,"Dataset contains 13,679 crowdsourced science exam questions about Physics, Chemistry and Biology, among others. The questions are in multiple-choice format with 4 answer options each.","13,769",English,"Question Answering, Reading Comprehension",Paper,Link
Khot et al.,SciTail Dataset,Dataset is an entailment dataset created from multiple-choice science exams and web sentences. Each question and the correct answer choice are converted into an assertive statement to form the hypothesis.,"27,026",English,Entailment,Paper,Link
Dunn et al.,SearchQA,Dataset from Jeapardy archives which consists of more than 140k question-answer pairs with each pair having 49.6 snippets on average.,140,English,"Question Answering, Reading Comprehension",Paper,Link
Yu et al.,Semantic Parsing in Context (SParC),"Dataset consists of 4,298 coherent question sequences (12k+ unique individual questions annotated with SQL queries annotated byt. It is the context-dependent/multi-turn version of the Spider task.","4,298",English,"Semantic Parsing, SQL-to-Text",Paper,Link
Cer et al.,Semantic Textual Similarity Benchmark,The task is to predict textual similarity between sentence pairs.,"8,628",English,Semantic Textual Similarity,Paper,Link
Nakov et al.,SemEvalCQA,Dataset for community question answering.,n/a,"Arabic, English","Question Answering, Reading Comprehension",Paper,Link
Kotzias,Sentiment Labeled Sentences Dataset,"Dataset contains 3,000 sentiment labeled sentences.",3,English,"Classification, Sentiment Analysis",Paper,Link
Go et al.,Sentiment140,"Tweet data from 2009 including original text, time stamp, user and sentiment.","1,578,627",English,"Classification, Sentiment Analysis",Paper,Link
Saeidi et al.,Shaping Answers with Rules through Conversation (ShARC),ShARC is a Conversational Question Answering dataset focussing on question answering from texts containing rules.,32,English,"Question Answering, Reading Comprehension",Paper,Link
The Hewlett Foundation,Short Answer Scoring,Student-written short-answer responses.,n/a,English,Scoring Classification,Paper,Link
Zellers et al.,Situations With Adversarial Generations (SWAG),"Dataset consists of 113k multiple choice questions about grounded situations. Each question is a video caption from LSMDC or ActivityNet Captions, with four answer choices about what might happen next in the scene.",113,English,"Question Answering, Reading Comprehension",Paper,Link
Nguyen,Skytrax User Reviews Dataset,"User reviews of airlines, airports, seats, and lounges from Skytrax.","41,396",English,"Classification, Sentiment Analysis",Paper,Link
Almeida et al.,SMS Spam Collection Dataset,Dataset contains SMS spam messages.,"5,574",English,Classification,Paper,Link
McAuley et al.,SNAP Social Circles: Twitter Database,Large Twitter network data.,"Nodes: 81,306, Edges:1,768,149",English,"Clustering, Graph Analysis",Paper,Link
Zadeh et al.,Social-IQ Dataset,Dataset containing videos and natural language questions for visual reasoning.,"7,5",English,"Question Answering, Visual, Commonsense",Paper,Link
Hopkins et al.,Spambase Dataset,Dataset contains spam emails.,"4,601",English,Classification,Paper,Link
Yu et al.,Spider 1.0,"Dataset consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains.","10,181",English,"Semantic Parsing, SQL-to-Text",Paper,Link
Rajpurkar et al.,SQuAD v2.0,Paragraphs w/ questions and answers.,150,English,"Question Answering, Reading Comprehension",Paper,Link
Stack Overflow,Stack Overlow BigQuery Dataset,"BigQuery dataset includes an archive of Stack Overflow content, including posts, votes, tags, and badges.",n/a,English,Text Corpora,Paper,Link
Bowman et al.,Stanford Natural Language Inference (SNLI) Corpus,"Image captions matched with newly constructed sentences to form entailment, contradiction, or neutral pairs.",570,English,Natural Language Inference (NLI),Paper,Link
Elsahar et al.,T-REx,Dataset contains Wikipedia abstracts aligned with Wikidata entities.,11M aligned triples,English,Relation Extraction,Paper,Link
Chen et al.,TabFact,Dataset contains 16k Wikipedia tables as evidence for 118k human annotated statements to study fact verification with semi-structured evidence.,16,English,Natural Language Inference (NLI),Paper,Link
Byrne et al.,Taskmaster-1,"Dataset contains 13,215 task-based dialogs, including 5,507 spoken and 7,708 written dialogs created with two distinct procedures. Each conversation falls into one of six domains: ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations.","13,215",English,Dialogue,Paper,Link
Timo Block,Ten Thousand German News Articles Dataset (10kGNAD),Dataset consists of 10273 german language news articles from an austrian online newspaper categorized into nine topics.,"10,273",German,Classification,Paper,Link
Song et al.,Tencent AI Lab Embedding Corpus,"Dataset provides 200-dimension vector representations, a.k.a. embeddings, for over 8 million Chinese words and phrases.",8M,Chinese,Embeddings,Paper,Link
Kembhavi et al.,Textbook Question Answering,"The M3C task builds on the popular Visual Question Answering (VQA) and Machine Comprehension (MC) paradigms by framing question answering as a machine comprehension task, where the context needed to answer questions is provided and composed of both text and images.","26,62",English,"Question Answering, Reading Comprehension, Visual",Paper,Link
Singh et al.,TextVQA,"TextVQA requires models to read and reason about text in images to answer questions about them. Specifically, models need to incorporate a new modality of text present in the images and reason over it to answer TextVQA questions.","36,602",English,"Question Answering, Visual, Commonsense",Paper,Link
Warstadt et al.,The Benchmark of Linguistic Minimal Pairs (BLiMP),BLiMP is a challenge set for evaluating what language models (LMs) know about major grammatical phenomena in English.,"67 sub-datasets each with 1,000 minimal pairs",English,Language Modeling,Paper,Link
NeurIPS,The Conversational Intelligence Challenge 2 (ConvAI2),A chit-chat dataset based on PersonaChat dataset.,"3,127",English,Dialogue,Paper,Link
Warstadt et al.,The Corpus of Linguistic Acceptability (CoLa),Dataset used to classifiy sentences as grammatical or not grammatical.,"10,657",English,Grammatical Acceptability,Paper,Link
Weston,The Dialog-based Language Learning Dataset,Dataset was designed to measure how well models can perform at learning as a student given a teacher’s textual responses to the student’s answer.,n/a,English,"Question Answering, Reading Comprehension",Paper,Link
Rohit Kulkarni,The Irish Times IRS,Dataset contains 23 years of events from Ireland.,"1,425,460",English,"Clustering, Events, Language Detection",Paper,Link
Dodge et al.,The Movie Dialog Dataset,"Dataset measures how well models can perform at goal and non-goal orientated dialogue centered around the topic of movies (question answering, recommendation and discussion).",~3.5M,English,"Question Answering, Reading Comprehension",Paper,Link
Marcus et al.,The Penn Treebank Project,Naturally occurring text annotated for linguistic structure.,~1M words,English,POS,Paper,Link
Bordes et al.,The SimpleQuestions Dataset,"Dataset for question answering with human generated questions paired with a corresponding fact, formatted as (subject, relationship, object), that provides the answer but also a complete explanation.","108,442",English,"Question Answering, Knowledge Base",Paper,Link
Socher et al.,The Stanford Sentiment Treebank (SST),Sentence sentiment classification of movie reviews.,69,English,"Classification, Sentiment Analysis",Paper,Link
Mostafazadeh et al.,The Story Cloze Test | ROCStories,Dataset for story understanding that provides systems with four-sentence stories and two possible endings. The systems must then choose the correct ending to the story.,"100,000+",English,"Question Answering, Reading Comprehension",Paper,Link
Miller et al.,The WikiMovies Dataset,"Dataset contains only the QA part of the Movie Dialog dataset, but using three different settings of knowledge: using a traditional knowledge base (KB), using Wikipedia as the source of knowledge, or using IE (information extraction) over Wikipedia.","~100,000",English,"Question Answering, Reading Comprehension",Paper,Link
Levesque et al.,The Winograd Schema Challenge,Dataset to determine the correct referrent of the pronoun from among the provided choices.,150,Multi-Lingual,Coreference Resolution,Paper,Link
Gopalakrishnan et al.,Topical-Chat,A knowledge-grounded human-human conversation dataset where the underlying knowledge spans 8 broad topics and conversation partners don’t have explicitly defined roles.,"10,784",English,Dialogue,Paper,Link
Ch'ng et al.,Total-Text-Dataset,Dataset used to classify curved text in pictures.,"~1,500",English,Scene Text Detection,Paper,Link
Wang et al.,TrecQA,Dataset is commonly used for evaluating answer selection in question answering.,n/a,English,"Question Answering, Reading Comprehension",Paper,Link
Joshi et al.,TriviaQA,"Dataset containing over 650K question-answer-evidence triples. It includes 95K QA pairs authored by trivia enthusiasts and independently gathered evidence documents, 6 per question on average.","650,000+",English,"Question Answering, Reading Comprehension",Paper,Link
Allen Institute,TupleInf Open IE Dataset,"Dataset contains Open IE tuples extracted from 263K sentences that were used by the solver in ""Answering Complex Questions Using Open Information Extraction"" (referred as Tuple KB, T).",263,English,Knowledge Base,Paper,Link
Mitchell et al.,Twenty Newsgroups Dataset,Dataset is a collection newsgroup documents used for classification task.,20,English,"Classification, Clustering",Paper,Link
Abdulla,Twitter Dataset for Arabic Sentiment Analysis,Dataset contains Arabic tweets.,2,Arabic,"Classification, Sentiment Analysis",Paper,Link
Figure Eight,Twitter US Airline Sentiment,"Dataset contains airline-related tweets that were labeled with positive, negative, and neutral sentiment.","14,5",English,"Classification, Sentiment Analysis",Paper,Link
Hu et al.,Twitter100k,Pairs of images and tweets.,100,English,Multi-Modal Learning,Paper,Link
Lowe et al.,Ubuntu Dialogue Corpus,Dialogues extracted from Ubuntu chat stream on IRC.,930,English,"Text Corpora, Dialogue",Paper,Link
Anonymous,Urban Dictionary Dataset,"Corpus of words, votes and definitions.","2,606,522",English,Reading Comprehension,Paper,Link
Shaoul et al.,UseNet Corpus,UseNet forum postings.,7B,English,Dialogue,Paper,Link
Zellers et al.,Video Commonsense Reasoning (VCR),Dataset contains 290K multiple-choice questions on 110K images.,290,English,"Question Answering, Visual, Commonsense",Paper,Link
Antol et al.,Visual QA (VQA),"Dataset containing open-ended questions about images. These questions require an understanding of vision, language and commonsense to answer.","265,016 images",English,Visual Question Answering,Paper,Link
Various,Voices Obscured in Complex Environmental Settings (VOiCES),"Dataset contains a total of 15 hours (3,903 audio files) in male and female read speech.",n/a,English,Speech Recognition,Paper,Link
Nagrani et al.,VoxCeleb,"An audio-visual dataset consisting of short clips of human speech, extracted from interview videos uploaded to YouTube.",n/a,Multi-Lingual,"Speech Recognition, Visual",Paper,Link
Kowsari et al.,Web of Science Dataset,Hierarchical Datasets for Text Classification.,"46,985",English,Classification,Paper,Link
Yih et al.,WebQuestions Semantic Parses Dataset,"Dataset contains full semantic parses in SPARQL queries for 4,737 questions, and “partial” annotations for the remaining 1,073 questions for which a valid parse could not be formulated or where the question itself is bad or needs a descriptive answer.","5,81",English,Semantic Parsing,Paper,Link
Onishi et al.,Who Did What Dataset,"Dataset contains over 200,000 fill-in-the-gap (cloze) multiple choice reading comprehension problems constructed from the LDC English Gigaword newswire corpus.","200,000K",English,"Question Answering, Reading Comprehension",Paper,Link
Koupaee et al.,WikiHow,Dataset contains article and summary pairs extracted and constructed from an online knowledge base written by different human authors.,"230,000+",English,"Text Corpora, Summarization",Paper,Link
Singh et al.,WikiLinks,Dataset contains 40 million mentions over 3 million entities based on hyperlinks from Wikipedia.,~10M,English,Text Corpora,Paper,Link
Yang et al.,WikiQA Corpus,Dataset contains Bing query logs as the question source. Each question is linked to a Wikipedia page that potentially has the answer.,"3,047",English,"Question Answering, Reading Comprehension",Paper,Link
Rudinger et al.,Winogender Schemas,"Dataset with pairs of sentences that differ only by the gender of one pronoun in the sentence, designed to test for the presence of gender bias in automated coreference resolution systems.",720,English,Coreference Resolution,Paper,Link
Stanford,WMT 14 English-German,Sentence pairs for translation.,4.5M,Multi-Lingual,Machine Translation,Paper,Link
Stanford,WMT 15 English-Czech,Sentence pairs for translation.,15.8M,Multi-Lingual,Machine Translation,Paper,Link
ACL Workshop,WMT 19 Multiple Datasets,Multiple text corpora in multiple languages.,n/a,Multi-Lingual,"Text Corpora, Machine Translation",Paper,Link
Pilehvar et al.,Words in Context,Dataset for evaluating contextualized word representations.,"2,4",English,Word Sense Disambiguation,Paper,Link
Rohit Kulkarni,Worldwide News - Aggregate of 20K Feeds,One week snapshot of all online headlines in 20+ languages.,"1,398,431",Multi-Lingual,"Clustering, Events, Machine Translation",Paper,Link
Yahoo!,Yahoo! Music User Ratings of Musical Artists,Over 10M ratings of artists by Yahoo users. May be used to validate recommender systems or collaborative filtering algorithms.,~10M,English,"Clustering, PCA",Paper,Link
Yelp,Yelp Open Dataset,"Dataset containing millions of reviews on Yelp. In addition it contains business data including location data, attributes, and categories.","6,685,900",English,"Classification, Sentiment Analysis",Paper,Link
Google,YouTube Comedy Slam Preference Dataset,User vote data for pairs of videos shown on YouTube. Users voted on funnier videos.,"1,138,562",English,Classification,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 2.3M.,n/a,Lingala,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 63M.,n/a,Lao,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 3.4G.,n/a,Lithuanian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 2.1G.,n/a,Latvian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 29M.,n/a,Malagasy,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 706M.,n/a,Macedonian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 831M.,n/a,Malayalam,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 397M.,n/a,Mongolian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 334M.,n/a,Marathi,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 2.1G.,n/a,Malay,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 46M.,n/a,Burmese,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 178M.,n/a,Burmese (Zawgyi),Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 393M.,n/a,Nepali,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 7.9G.,n/a,Dutch,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 13G.,n/a,Norwegian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 1.8M.,n/a,Northern Sotho,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 11M.,n/a,Oromo,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 56M.,n/a,Oriya,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 90M.,n/a,Punjabi,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 12G.,n/a,Polish,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 107M.,n/a,Pashto,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 13G.,n/a,Portuguese,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 1.5M.,n/a,Quechua,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 4.8M.,n/a,Romansh,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 16G.,n/a,Romanian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 46G.,n/a,Russian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 44M.,n/a,Sanskrit,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 452M.,n/a,Sinhala,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 143K.,n/a,Sardinian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 67M.,n/a,Sindhi,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 6.1G.,n/a,Slovak,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 2.8G.,n/a,Slovenian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 78M.,n/a,Somali,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 1.3G.,n/a,Albanian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 1.5G.,n/a,Serbian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 86K.,n/a,Swati,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 15M.,n/a,Sundanese,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 21G.,n/a,Swedish,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 332M.,n/a,Swahili,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 1.3G.,n/a,Tamil,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 68M.,n/a,Tamil Romanized,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 536M.,n/a,Telugu,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 79M.,n/a,Telugu Romanized,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 8.7G.,n/a,Thai,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 701M.,n/a,Tagalog,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 8.0M.,n/a,Tswana,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 5.4G.,n/a,Turkish,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 46M.,n/a,Uyghur,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 14G.,n/a,Ukrainian,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 884M.,n/a,Urdu,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 141M.,n/a,Urdu Romanized,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 155M.,n/a,Uzbek,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 28G.,n/a,Vietnamese,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 3.6M.,n/a,Wolof,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 25M.,n/a,Xhosa,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 51M.,n/a,Yiddish,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 1.1M.,n/a,Yoruba,Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 14G.,n/a,Chinese (Simplified),Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 5.3G.,n/a,Chinese (Traditional),Text Corpora,Paper,Link
Conneau & Wenzek et al.,CC100,This dataset is one of the 100 corpora of monolingual data that was processed from the January-December 2018 Commoncrawl snapshots from the CC-Net repository. The size of this corpus is 4.3M.,n/a,Zulu,Text Corpora,Paper,Link
Vilares et al.,HeadQA,"Dataset is a multichoice testbed of graduate-level questions about medicine, nursing, biology, chemistry, psychology, and pharmacology.","6,765","Spanish, English",Question Answering,Paper,Link
Leino et al.,FinChat,"Dataset contains conversations with message timestamps, sender’s id, and metadata information. It contains 86 conversations with 3,630 messages, 22,210 words with the average word length of 5.6, and on the average 14 turns per each conversation.","3,63",Finnish,Dialogue,Paper,Link
Chen et al.,Open Table-and-Text Question Answering (OTT-QA),Dataset contains open questions which require retrieving tables and text from the web to answer. The dataset is built on the HybridQA dataset.,45,English,Open Domain Question Answering,Paper,Link
Byrne et al.,Taskmaster-3,"Dataset consists of 23,757 movie ticketing dialogs. ""Movie ticketing"" is defined as conversations where the customer's goal is to purchase tickets after deciding on theater, time, movie name, number of tickets, and date, or opt out of the transaction.","23,757",English,Dialogue,Paper,Link
Asai et al.,XOR-TyDi QA,"Dataset is a multi-lingual open-retrieval QA dataset that enables cross-lingual answer retrieval. It consists of questions written by information-seeking native speakers in 7 typologically diverse languages and answer annotations that are retrieved from multilingual document collections. There are three sub-tasks: XOR-Retrieve, XOR-EnglishSpan, and XOR-Full.",40,Multi-Lingual,Question Answering,Paper,Link
Mosig et al.,STAR,"A schema-guided task oriented dialog dataset consisting of 127,833 utterances and knowledge base queries across 5,820 task-oriented dialogs in 13 domains that is especially designed to facilitate task and domain transfer learning in task-oriented dialog.","5,82",English,Dialogue,Paper,Link
Quan et al.,RiSAWOZ,"Dataset contains 11.2K human-to-human (H2H) multiturn semantically annotated dialogues, with more than 150K utterances spanning over 12 domains.","11,2",Chinese,Dialogue,Paper,Link
Pedro et al.,The BrWaC (Brazilian Portuguese Web as Corpus),"This dataset is a large corpus constructed in our lab following the Wacky framework, which was made public for research purposes.",n/a,Portuguese,"Text Corpora, Text Classification",Paper,Link
Henrique et al.,BlogSet-BR,"This dataset is a collection of blog posts crawled from Blogspot platform, containing texts by brazilian authors.",7.40M,Portuguese,"Text Corpora, Text Classification",Paper,Link
,Datasets of Neuropsychological Language Tests in Brazilian Portuguese (DNLT-BP),"This dataset contains data collected from participants in clinical or academic studies and research, by reading and signing the Informed Consent Form, and the research was evaluated and approved by the Research Ethics Committees of the institutions to which they are linked",n/a,Portuguese,"Text Corpora, Text Classification",Paper,Link
NILC-USP,Historical Portuguese Corpora (HPC),"Dataset is a sub-project of the Historical Dictionary of Brazilian Portuguese project, which is funded by CNPq, Brazil. In the HPC project tools and resources for manipulation of historical corpora and management of historical dictionaries are developed. The tools and resources were released under public domain",n/a,Portuguese,"Text Corpora, Text Classification",Paper,Link
,Rhetalho,A dataset annotated by Rhetorical Structure Theory – RST,n/a,Portuguese,,Paper,Link
NILC-USP,Lex2Kids,"Este dataset contêm representação léxica em português mais ouvido por crianças. Contém 36,413 legendas de filmes e séries dos gêneros Família e Animação",n/a,Portuguese,Text Corpora,Paper,Link
,ITD - Dataset de Acordãos do STF de 2010 a 2018,"A base Iudicium Textum Dataset (ITD), contêm os textos extraídos dos Acórdãos do Supremo Tribunal Federal de 2010 a 2018. Os textos estão separados por seção, com os votos e os relatórios identificados por autor (ministro). O texto original também foi mantido de forma integral e as partes envolvidas, em grande parte, estão identificadas. Os dados estão organizados em um arquivo json, podendo ser importado para um banco MongoDB. Junto com a base, estão disponíveis também os arquivos pdfs originais, bem como as ferramentas e os códigos que foram utilizados para download, extração e conversão dos dados que compõem o dataset",n/a,Portuguese,Text Corpora,Paper,Link
,PortugueseGLUE,Este dataset contêm tradução para o português do benchmark GLUE e conjunto de dados Scitail usando o modelo OPUS-MT e Google Cloud Translation.,n/a,Portuguese,GLUE,Paper,Link
,TweetSentBR,"This dataset contains sentiment polarity classification, this dataset contains 800k tweets in Portuguese divided into positive, negative, and neutral classes",n/a,Portuguese,Text Classification,Paper,Link
B2W,B2W-Reviews01,"This dataset contains reviews from ecommerce products. About 130k customer reviews, extracted from Americanas.com, between Jan and May 2018. Including annotated data from customers profile, like ender, age, and geograph location.",130000,Portuguese,Text Classification,Paper,Link
MercadoLibre,Mercadolibre Data Challenge 2019,"This dataset are used in MercadoLibre data challenge, and contains multi-language products classification from MercadoLibre.com",n/a,"Portuguese, Spanish",Text Classification,Paper,Link
,CorpusTCC,"This dataset contains scientific texts from brazilian community, about computer science field.",n/a,Portuguese,Summarization,Paper,Link
NILC-USP,MilkQA,"This dataset are a dense collection of questions and answers, for answers selection task. Contains Q&A from Agro field, extracted from Embrapa Gado de Leite between 2013 and 2012.",n/a,Portuguese,Question Answering,Paper,Link